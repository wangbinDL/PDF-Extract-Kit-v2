# Environmental Estimation With Distributed Finite Element Agents  

Matthew L. Elwin  

Randy A. Freeman  

Kevin M. Lynch  

or all of the measurements. By communicating with local neighbors, however, the collection of all agents’ estimates matches the result obtained by a central computer using VIM.  

Abstract — We develop an environmental estimation method that allows large groups of agents to infer the value of an environmental ﬁeld using measurements. Agents maintain estimates for subregions of the domain and communicate with local neighbors, so the method’s communication and memory requirements do not increase with the number of agents, the size of the environment representation, or the agents’ density in the environment. Despite the distributed representation, the union of individual estimates matches an estimate generated by a central computer with access to all measurements employing the variational inverse method, a ﬁnite element-based interpolation procedure. We also introduce a distributed query system, allowing users to determine an estimate anywhere in the domain without accessing all measurements or the full environment representation.  

We use the alternating direction method of multipliers (ADMM) to distribute computations and solve the FEM problem [8]. Other methods, which generally require more communication than ADMM include [9] and [10].  

We couple our distributed VIM (DVIM) method with a query system, allowing a user to determine the value of the ﬁeld anywhere. The user can obtain a low-resolution view of the overall ﬁeld and then focus on higher resolution estimates in areas of particular interest. Using this system, the full set of measurements and a complete environment representation are never communicated to or stored by a single computer. When communication with a remote base station (e.g., via satellite) costs signiﬁcantly more than inter- agent communication (e.g., via XBee), minimizing such communication becomes especially important.  

# I. INTRODUCTION  

A common problem in sciences such as oceanography, earth science, and atmospheric science is to infer the value of a scalar ﬁeld (e.g., temperature, chemical concentration, radiation intensity) anywhere in an environment based on irregularly spaced measurements. Typically, the relatively sparse data are processed by a central computer; such meth- ods have been studied extensively (see e.g., [1]). The advent of cheap sensors, however, presents an opportunity to collect more data than ever before. As the number of sensors used to measure an environment increases, so do computation and communication burdens. These problems become especially acute in remote environments such as the ocean, the polar icecaps, outer space, and underground. In such environments, communication with a distant base station may be signiﬁ- cantly harder than communication between agents, making it impractical to send all measurements to a central computer.  

In Section II we compare DVIM to existing methods in the literature. In Section III we formulate the problem, intro- ducing VIM and FEM. Section IV describes our distributed solution, DVIM, and the query system. Section V provides simulation results. We outline our conclusions and future work in Section VI.  

# II. M ETHOD  C OMPARISON  

We compare DVIM to existing techniques whose com- putation, communication, and memory requirements do not increase with respect to the number of agents. Many criteria can be used to evaluate methods, but in this work we are particularly concerned with three desirable properties: scalability with respect to the environment representation, scalability with respect to agent density, and incorporation of all relevant measurements.  

To address the problem of handling many measurements with limited memory and bandwidth, we develop a dis- tributed version of the variational inverse method (VIM) for data interpolation [2]–[6]. This technique, used in oceanog- raphy, generates an estimate by minimizing a cost functional using the Finite Element Method (FEM) [7]. Rather than having a central computer collect data from sensors, we deploy the sensors on agents that communicate with each other and perform computations. Each agent determines an estimate for a small portion of the environment. No single agent, therefore, stores a full environmental representation  

Scalability with respect to the environment representa- tion means that every agent’s memory and communication requirements do not increase with the resolution of the environment model. Methods that scale with the environment representation allow adding more agents with ﬁxed capabil- ities to obtain higher resolution estimates.  

Scalability with respect to agent density means that every agent’s memory and communication requirements do not increase as the density of the agents in the environment (and therefore measurement density) increases. High measurement density contributes to more accurate estimates, so methods with this property provide opportunities for improved accu- racy by adding more agents with ﬁxed capabilities.  

Methods that incorporate all relevant measurements gen- erate an estimate at a given point using measurements from  

<td><table  border="1"><thead><tr><td></td><td><b>VIM</b></td><td><b>KF [12]</b></td><td><b>KK [13]</b></td><td><b>NP [14]</b></td><td><b>NN [11]</b></td><td><b>ID  [11]</b></td><td><b>FE [15]</b></td></tr></thead><tbody><tr><td>SER</td><td>X</td><td></td><td></td><td></td><td>x</td><td>x</td><td>x</td></tr><tr><td>SAD</td><td>X</td><td>X</td><td></td><td>X</td><td>x</td><td></td><td>x</td></tr><tr><td>IRM</td><td>X</td><td>x</td><td>x</td><td>x</td><td></td><td>x</td><td>x</td></tr></tbody></table></td>  

# TABLE I  

C OMPARISON OF THE VARIOUS METHODS . SER -  SCALE WITH ENVIRONMENT REPRESENTATION , SAD -  SCALE WITH AGENT DENSITY , IRM -  INCORPORATE RELEVANT MEASUREMENTS .  

locations correlated with that point. Many methods assume that the correlation between points decreases with distance, and that points further than a characteristic correlation length apart are uncorrelated. Although VIM (and therefore DVIM) does not rely on this assumption (an advantage; it can handle cases when nearby points should be uncorrelated, see [3]), it incorporates all measurements when forming estimates and does have a notion of spatial correlation [4]. Methods that disregard measurements that should inﬂuence an estimate at a given point do not use information efﬁciently.  

Two features of all the methods we examine that DVIM currently lacks are control laws for mobile agents and error estimation abilities (we leave these for future work). To ensure a fair comparison, we assume, for all papers, station- ary agents and known error ﬁelds for all agents. Although estimating an error ﬁeld provides beneﬁts, it also imposes additional communication and memory requirements on the agents. Additionally, in [11], movement increases communi- cation and memory requirements because the environmental representation is coupled to the agents’ trajectories.  

We now describe several methods and evaluate whether they scale with the environment representation, scale with agent density, and incorporate all relevant measurements. Table I shows this comparison, with more details provided below. Note that a central computer collecting measurements from all agents and applying VIM does not scale with the environment representation or with agent density.  

# A. Distributed Kalman Filter [12]  

This method uses a distributed Kalman ﬁlter based on average consensus estimators to incorporate measurements and estimate basis function coefﬁcients that represent the en- vironment. The method could also work using the distributed data fusion techniques of [16]. Communication and memory is proportional to the number of basis functions, which is ﬁxed beforehand; therefore, this method does not scale with the environment representation. Every agent, however, has a full environment estimate. This method scales with agent density because it works over any connected communication graph. The relationship between measurements at different points is incorporated in the basis function choice, so mea- surements contribute to the ﬁeld estimate everywhere.  

# B. Kriged Kalman Filter [13]  

This method represents the environment as the mean and covariance functions of a spatial Gaussian process. The representation consists of basis function coefﬁcients (as in [12]) and as a combination of measurements within a given radius    $R$   of each agent. If the coefﬁcients are unknown, communication and memory is proportional to the number of basis functions; however, by assuming known coefﬁcients, communication is independent of the global environment description and this method can scale with the environment representation. Regardless, the communication increases with density: estimation requires the agents to incorporate all measurements within a radius of    $R$  . Since the agents account for measurements within a radius of    $R$   and points further than    $R$   apart are assumed to be uncorrelated, all relevant measurements contribute to the estimate.  

# C. Non-parametric Information [14]  

This method represents the environment as a weighted set of discrete samples from a probability distribution. Accord- ingly, it can approximate any type of environment, not just scalar ﬁelds. Every agent, however, has a complete envi- ronment representation and must communicate information proportional to the size of this representation. This method, therefore, does not scale with the representation size. It does, however, scale with agent density because it works over connected graphs. Information is fused using a particle-ﬁlter like approach, so the estimate at a given location incorporates relevant measurements.  

# D. Local Interpolation: Nearest Neighbor [11]  

In this method, the environment is a piecewise constant function. When used with stationary agents, an agent’s estimate over its Voronoi region is its measurement. This method, therefore, scales with the environment representation (adding more agents adds more Voronoi cells) and with density. The nearest neighbor approach, however, uses a single measurement to estimate the ﬁeld over each Voronoi cell, so it does not use all relevant measurements. With mobile agents, as originally intended, the initial Voronoi cells are subdivided based on current and past measurements, creating a more detailed estimate. The environment represen- tation, however, is coupled to the agent’s trajectories; lossy compression is used to maintain scalability with respect to the number of measurements when the agents move.  

# E. Local Interpolation: Inverse Distance [11]  

Closely related to Nearest Neighbor Local Interpolation, this method uses piecewise continuous functions to represent the environment. Here, measurements have a diminishing effect on the estimate as distance from the measurement de- creases. Thus, unlike the Nearest Neighbor version, estimates at a given point incorporate relevant measurements. However, now the estimate must incorporate all measurements from within a given radius    $R$  , so the method no longer scales with agent density.  

# F. Distributed Finite Element Kalman Filter [15], [17]  

In this method, the environment is modeled as a partial differential equation (PDE) in space and time. Using ﬁnite elements over the spatial variables and discretizing over time, the PDE is converted into a discrete time state-space system and a Kalman Filter used to estimate the state. Each agent is responsible for only a subset of the domain; thus the method scales with respect to the environmental model. At each time-step, the state is estimated using the parallel Schwarz method and measurements are incorporated using a Kalman ﬁlter update. The method requires only information shared by adjacent regions to be exchanged and converges to a centralized solution; therefore the method scales with agent density and incorporates all relevant information.  

# G. Distributed Variational Inverse Method  

This method, as developed here, is scalable with respect to environment representation, agent density, and incorpo- rates relevant measurements when forming an estimate. The environment representation is the total number of nodes in a global FEM mesh, but each agent handles only a subset of those nodes. As the density of the agents increases, communication costs do not increase because communication is proportional to the number of Voronoi neighbors and the perimeter of every agent’s Voronoi region, which do not increase with density. Finally, in each agents’ region, the estimate matches the centralized VIM method, which incorporates all measurements into the estimate.  

Unlike [15], [17], our method does not require a model of the ﬁeld; thus it is applicable in situations when partial differential equations for the environment are not known. Additionally, we present a method to form consistent ﬁnite element meshes in a distributed manner and a query system to extract the estimate, issues that are not addressed in [15], [17].  

# III. V ARIATIONAL  I NVERSE  M ETHOD  

A. Sensor and Environment Model  

The environment is a scalar ﬁeld  $\bar{\phi}(x,y)$   over a region  $\Omega\subset{\mathbb{R}}^{2}$  . Stationary agents dispersed over    $\Omega$  , with agent    $i$  located at    $(x_{i},y_{i})\in\Omega$  , inaccurately measure the ﬁeld:  

$$
z_{i}=\bar{\phi}(x_{i},y_{i})+v_{i},
$$  

where    $z_{i}$   is agent    $i$  ’s measurement and    $v_{i}$   is an error (e.g. Gaussian noise) that depends on the particular agent. There are  $n$   measurements, which we use to generate an estimate  $\phi(x,y)$   of the ﬁeld  $\bar{\phi}(x,y)$   everywhere in    $\Omega$  .  

#  $B$  . Variational Inverse Method  

We use the Variational Inverse Method (VIM), an interpo- lation method closely related to spline interpolation to esti- mate the environment [2]–[6]. Overall, the method generates an estimate  $\phi(x,y)$   of the ﬁeld  $\bar{\phi}(x,y)$   by minimizing a cost functional using the Finite Element Method (FEM) [7].  

# C. Variational Principle  

The cost functional to minimize is  

$$
J[\phi]=\int_{\Omega}S[\phi]d\Omega+D[\phi]
$$  

where    $\phi(x,y)$   is the ﬁeld estimate,    $S[\phi]$   penalizes non- smoothness of the estimate and  $D[\phi]$   penalizes the deviation  

  
Fig. 1. Finite element mesh with seven quadratic elements. Measurements are denoted with  $\mathbf{X}$  . The dots represent the nodes.  

of the estimate from measurements. In this paper we use  

$$
{\begin{array}{l}{S[\phi]=\left({\cfrac{\partial^{2}\phi}{\partial x^{2}}}\right)^{2}+\left({\cfrac{\partial^{2}\phi}{\partial y^{2}}}\right)^{2}+2\left({\cfrac{\partial^{2}\phi}{\partial x\partial y}}\right)^{2}}\\ {\qquad+\,\alpha_{1}\left(\left({\cfrac{\partial\phi}{\partial x}}\right)^{2}+\left({\cfrac{\partial\phi}{\partial y}}\right)^{2}\right)+\alpha_{0}(\phi-\phi_{0})^{2}}\end{array}}
$$  

and  

$$
D[\phi]=\sum_{k=1}^{n}\mu_{k}\left(\phi(x_{k},y_{k})-z_{k}\right)^{2}
$$  

where  $\phi_{0}(x,y)$   is a known background ﬁeld,    $\alpha_{1}$   is a weight on the curvature,  $\alpha_{0}$   weights the error of the ﬁeld, and    $\mu_{k}$  weights each measurement.  

The known background ﬁeld    $\phi_{0}(x,y)$   is the assumed value of the ﬁeld in the absence of measurements; it allows us to incorporate prior assumptions about the ﬁeld into the estimate. Alternatively,    $\phi_{0}(x,y)$   can be taken to be the average of the data, a linear ﬁt to the data, or zero. When  $\phi_{0}(x,y)=0$   it makes sense to let  $\alpha_{0}=0$   to avoid penalizing the magnitude of the resulting estimated ﬁeld.  

#  $D$  . Finite Element Method  

We use FEM to minimize the cost functional (2); details can be found in [7]. FEM consists of the following ﬁve steps:  

1) Divide the domain into sub-domains called elements. 2) Find interpolating equations for each element based on values at points within each element, called nodes. 3) Assemble elements into a global system of equations. 4) Solve the equations. 5) Post-process the result to obtain information from the solution, for example, producing a visualization.  

1) Dividing the domain:  The domain is partitioned into a mesh of    $m$   non-overlapping sub-domains called elements (see Figure 2). Each element’s domain is  $\Omega_{e}$   and the elements approximately cover    $\Omega$  . Elements only intersect at vertices or along full edges. Many element geometries are available. For simplicity, we use a mesh of triangular elements, as gen- erated by the Computational Geometry Algorithms Library (CGAL) [18].  

2) Interpolating equations:  The estimate within each ele- ment,    $\phi_{e}(x,y)$  , is approximated by a polynomial containing all terms up to order    $d$  , which can be fully described by    $r$  coefﬁcients. This polynomial’s coefﬁcients are related to the value of  $\phi_{e}(x,y)$   at points within the element called nodes:  

$$
\phi_{e}(x,y)=w^{T}(x,y)q_{e},
$$  

ere    $w(x,y)$   is the vector of    $r$   shape functions and    $q_{e}\in$   $\mathbb{R}^{r}$    is the vector of nodal values. Many shape functions are available. We use quadratic polynomials over triangular domains, so    $r~=~2$   (see Figure 2). The polynomial is  $b_{0}\,{+}\,b_{1}x\,{+}\,b_{2}y\,{+}\,b_{3}x^{2}\,{+}\,b_{4}x y\,{+}\,b_{5}y^{2}$  , where  $b_{i}$   is a coefﬁcient. There are six coefﬁcients and therefore six nodes (one at each corner and one bisecting each edge).  

Each shape function (an element of    $w(x,y))$   corresponds to a node: it is one at that node and zero at all other nodes. For example, with    $r=2$  , evaluating  $w^{T}(x,y)$   at element    $e$  ’s second node yields  $\left[\begin{array}{l l l l l l}{0}&{1}&{0}&{0}&{0}&{0}\end{array}\right]$   , which selects the second element of  q  $q_{e}$   in equation (5).  

3) Assembly:  Each nodal vector    $q_{e}$   is local to element    $e$  ; however, nodes on adjacent element edges overlap. Gener- ally, therefore, there are  $p<r m$   nodes in the ﬁnite element mesh. The global node vector    $q\ \in\ \mathbb{R}^{p}$    is related to the local node vectors by the Boolean stochastic gather matrices  $L_{e}\in\mathbb{R}^{r\times p}$    such that:  

$$
q_{e}=L_{e}q.
$$  

By deﬁning    $\phi_{e}(x,y)=0$   when    $(x,y)\notin\Omega_{e}$  , an approxi- mation of the whole ﬁeld is  

$$
\phi(x,y)\approx\sum_{i=1}^{m}\phi_{e}(x,y).
$$  

Note that by using the Dirac delta    $\delta$  , all terms of the cost functional (2) can be written inside the integral as  

$$
J[\phi]=\int_{\Omega}\left(S[\phi]+D^{\prime}[\phi]\right)d\Omega,
$$  

where  

$$
D^{\prime}[\phi]=\sum_{k=1}^{n}\mu_{k}(\phi-z_{k})^{2}\delta(x-x_{k})\delta(y-y_{k}).
$$  

Substituting equation (7) into the cost functional (9) and performing some manipulations yields  

$$
J[\phi]\approx\sum_{e=1}^{m}(q_{e}^{T}K_{e}q_{e}-2q_{e}^{T}g_{e})+\sum_{k=1}^{n}\mu_{k}(z_{k}-\phi_{0}(x_{k},y_{k}))^{2},
$$  

with  

$$
\begin{array}{r l}{K_{e}=\displaystyle\int_{\Omega_{e}}\left[\left(\frac{\partial^{2}w}{\partial x^{2}}\right)\left(\frac{\partial^{2}w}{\partial x^{2}}\right)^{T}+\left(\frac{\partial^{2}w}{\partial y^{2}}\right)\left(\frac{\partial^{2}w}{\partial y^{2}}\right)^{T}\right.}&{}\\ {\quad+\left.2\left(\frac{\partial^{2}w}{\partial x\partial y}\right)\left(\frac{\partial^{2}w}{\partial x\partial y}\right)^{T}+\alpha_{1}\left(\frac{\partial w}{\partial x}\right)\left(\frac{\partial w}{\partial x}\right)^{T}\right.}&{}\\ {\quad+\left.\alpha_{1}\left(\frac{\partial w}{\partial y}\right)\left(\frac{\partial w}{\partial y}\right)^{T}+\alpha_{0}w w^{T}\right]d\Omega_{e}}&{}\\ {\quad+\displaystyle\sum_{(x,y,y)\in\Omega_{e}}\mu_{k}w(x_{k},y_{k})w^{T}(x_{k},y_{k})\qquad\qquad(11)}\end{array}
$$  

and  

$$
g_{e}=\sum_{(x_{k},y_{k})\in\Omega_{e}}\mu_{k}w(x_{k},y_{k})z_{k}.
$$  

Substituting equation (6) into equation (10) and minimiz- ing with respect to    $q$   yields the global FEM equation  

$$
K q=g,
$$  

where  

$$
K=\sum_{e=1}^{m}L_{e}^{T}K_{e}L_{e}
$$  

and  

$$
g=\sum_{e=1}^{m}L_{e}^{T}g_{e}.
$$  

4) Solving:  Solving equation (13) for    $q$   provides the ﬁeld estimate. For brevity we have omitted boundary conditions; implicitly assuming that derivatives of the estimate across the boundary are zero. However, our technique extends to cases when either the derivatives on the boundary or the value of some boundary nodes are speciﬁed.  

5) Post-processing:  Given the global node vector    $q$  , the ﬁeld can be estimated at any location using equations (7) and (6), allowing us to, for example, plot the estimate.  

# IV. DISTRIBUTED METHOD  

Our method, distributed VIM (DVIM), enables agents to estimate ﬁelds using local communication without any agent needing all measurements or a full environment representa- tion. The scaling properties of the algorithm, derived from its distributed nature, allow for more detailed environment rep- resentations as the group size increases without a concomi- tant increase in the memory or communication requirements of individuals. Memory and communication requirements also do not increase with increasing agent density. Individual agents’ estimates are consistent with VIM solved on a central computer with full access to all measurements.  

To retrieve estimates from the group requires a query system. When at least one agent can communicate with a base station, users can query ﬁeld estimates at any location. Users may request estimates over a low-resolution grid and then focus on speciﬁc areas, allowing for less communication between the group and the base station.  

Agents must perform the following four tasks:  

1) Determine a region of dominance (ROD), the area over which they generate estimates. In this paper an agent’s ROD is the agent’s the Voronoi region. 2) Form a mesh within their (ROD) such that the union of all agents’ RODs forms a valid FEM mesh. We modify an existing meshing algorithm for this step. 3) Solve the FEM problem. We use the alternating direc- tion method of multipliers (ADMM) algorithm. 4) Respond to requests directed at determining the es- timate somewhere within the domain. We develop a direction-based depth ﬁrst search query system.  

These tasks can be repeated indeﬁnitely to provide estimates of time varying ﬁelds.  

Determining the ROD (task 1) and forming the mesh (task 2) roughly correspond to the global FEM steps of dividing the domain (step 1) and ﬁnding the interpolating function (step 2). Task 3, where the agents solve the FEM problem, covers both the assembling (step 3) and solving (step 4) steps of global FEM. One advantage of using ADMM over other solving methods is that assembly happens locally on the agents; individual entries in  $K$   and    $g$   are never explicitly constructed by any agent. Task 4, querying, is the distributed version of post-processing (step 5).  

We now discuss each step of distributed VIM individually. For clarity, we assume that every agent takes one measure- ment with one sensor, so there are  $n$   agents,    $n$   measurements, and  $n$   sensors; however, this method easily allows agents to take multiple measurements with multiple sensors.  

1) Region of Dominance:  Each agent establishes a region of dominance (ROD)    $\Omega_{i}\,\subset\,\Omega$  , partitioning    $\Omega$  i o    $n$   non- overlapping regions that approximately cover  Ω . If two agents have intersecting RODs, they are  neighbors  and must be able to communicate.  

We use bounded Voronoi regions for establishing RODs, deﬁned as the set of all points    $(x,y)$   such that  $\|(x,y)-(x_{i},y_{i})\|<\|(x,y)-(x_{j},y_{j})\|$  and    $(x,y)\in\Omega$  for all  $i\neq j$  . Two agents are Voronoi neighbors if their Voronoi regions share an edge. If the agents know their absolute position and can communicate with their Voronoi neighbors, the algorithm of [19] allows distributed computation of the region. As argued in [19], this communication requirement results in scalable networks because, on average, each agent has, at most, six neighbors.  

2) Consistent Mesh:  Once the agents know their ROD, they must form a mesh consistent with their neighbors’ meshes. Although agents compute their meshes indepen- dently, agents with shared edges must place any mesh vertices on those edges at the same location. Assuming a polygonal ROD, every agent ﬁrst divides its edges into the minimum number of segments such that no segment exceeds some pre-determined maximum length    $\ell_{\mathrm{max}}$  . The vertices subdividing these edges will be vertices in the resulting mesh, and, due to symmetry, they will be at the same absolute location for adjacent agents.  

Next, each agent runs the meshing algorithm of [18], which generates a mesh with no triangle edge lengths ex- ceeding    $\ell_{\mathrm{max}}$  . If the algorithm adds vertices on the ROD edges, they must be removed. Vertex removal is well-deﬁned on the structure returned by the meshing algorithm.  

3) Distributed FEM:  After determining its ROD and meshing it, every agent    $i$   has    $m_{i}$   elements and    $p_{i}$   nodes. Let  $E_{i}$   denote the set of elements contained in agent    $i$  ’s ROD. Since elements are not shared between agents,    $E_{i}$   and    $E_{j}$  are disjoint for    $i\neq j$  . Thus, each agent has a regional node vector    $\bar{q}_{i}\,\in\,\mathbb{R}^{p_{i}}$   ∈   (analogous to    $\grave{q_{\scriptscriptstyle\!,\!\scriptscriptstyle\perp}}$  ), which is related to the node vectors for each element in its region by  

$$
q_{f}={\bar{L}}_{f}{\bar{q}}_{i},{\mathrm{~for~all~}}f\in E_{i},
$$  

where the stochastic Boolean matrix  $\bar{L}_{f}$   is analogous to    $L_{e}$  . Likewise, we form the regional matrices  $\bar{K}_{i}$   and    ${\bar{g}}_{i}$   anal- ogously to equations (14) and (15) according to  

$$
\bar{K}_{i}=\sum_{f\in E_{i}}\bar{L}_{f}^{T}K_{f}\bar{L}_{f}
$$  

and  

$$
{\bar{g}}_{i}=\sum_{f\in E_{i}}{\bar{L}}_{f}^{T}g_{f}.
$$  

The map between agent    $i$  ’s node vector and the global node vector is given by    $g(i,j)$   such that    ${\left[{{\bar{q}}_{i}}\right]_{j}}$   corresponds to  $[q]_{g(i,j)}$  , where    $[\cdot]_{j}$   indicates the    $j$  -th entry in a vector.  

Using equations (10), (6), (17), and (18), we derive the following constrained minimization problem:  

$$
\begin{array}{r l}{\mathrm{argmin}}&{{}\displaystyle\sum_{i=1}^{n}\left(\bar{q}_{i}^{T}\bar{K}_{i}\bar{q}_{i}-2\bar{q}_{i}^{T}\bar{g}_{i}\right)}\end{array}
$$  

with respect to  $\bar{q}_{i}$  ,  for  $i=1$   to  n  

Note that the measurements enter the minimization prob- lem (19) via the term    ${\bar{g}}_{i}$  . The distributed ADMM method of [8] allows every agent to solve this problem for its own  $\bar{q}_{i}$  . Due to the constraints on  $\bar{q}_{i}$  , the union of all agents solutions is equivalent to solving the global FEM problem on the union of all agents’ meshes.  

Let  $N_{i,j}$  be the set of all pairs  $(k,l)$  such that  $[\bar{q}_{k}]_{l}=[q]_{g(i,j)}$  . In other words,    $N_{i,j}$   is the set of all agent- index, local-node-index pairs corresponding to the globalnode index    $g(i,j)$  . The cardinality of    $N_{i,j}$  ,    $|N_{i,j}|$   is then the number of local nodes corresponding to a given global node. To implement ADMM, every agent  $i$   iterates over three steps. We show a version speciﬁc to our problem derived from [8]:  

$$
\begin{array}{c}{\displaystyle[\xi_{i}(t)]_{j}=\frac{1}{\vert N_{i,j}\vert}\sum_{(k,l)\in N_{i,j}}[\hat{q}_{k}(t)]_{l},}\\ {\displaystyle\hat{q}_{i}(t+1)=(\bar{K}_{i}+\rho I)^{-1}(\bar{g}_{i}-\nu_{i}(t)+\rho\xi_{i}(t)),}\\ {\nu_{i}(t+1)=\nu_{i}(t)+\rho\left(\hat{q}_{i}(t)-\xi_{i}(t)\right),}\end{array}
$$  

where  $\rho>0$   is an optimization parameter,    $t$   is the discrete- time index,    $\xi_{i}(t)~\in~\mathbb{R}^{p_{i}}$  ,    $\hat{q}_{i}(t)~\in~\mathbb{R}^{p_{i}}$   ∈   and    $\nu_{i}(k)~\in~\mathbb{R}^{p_{i}}$  are the state variables of the ADMM algorithm. The agents execute this algorithm repeatedly until a stopping condition is met; we use a ﬁxed number of iterations, but adopting the stopping criteria of [8] is left for future work. When the algorithm completes, the agents take new measurements and the process begins again, allowing estimation of changing ﬁelds.  

The ﬁrst step, equation (20), averages all of the local node values that correspond to a given entry in    $q$  , and is the only step that requires communication. Only nodal values on the boundary of an ROD must be transmitted; nodes on the interior of the ROD directly correspond to a unique entry in    $q$   and need not be communicated.  

Equation (21) is the primal update step, and  $\hat{q}_{i}(t)$   is agent  $i$  ’s local estimate of  $\bar{q}_{i}$  . The third step, equation (22) updates the dual variable    $\nu_{i}(k)$  . Neither the primal nor the dual update steps require communication. When the algorithm converges, the entries of    $\xi_{i}(t)$   and    $\hat{q}_{i}(t)$   converge to the corresponding entries in    $q$  , so every agent has a piece of the global FEM solution.  

The agents cannot compute equation (20) directly because they do not know    $g(i,j)$  . However, if agents    $i$   and  $j$   are not neighbors they do not share nodes and do not contribute to each others’    $\xi_{i}(t)$   update; thus agents only must commu- nicate with their ROD neighbors. Additionally, as long as neighboring agents know the correspondence between the nodes shared with their neighbors, they can compute (20) by averaging all values corresponding to the same node without knowing the global index    $g(i,j)$  . Therefore, agents transmit nodal values on their ROD boundary and some book-keeping information that allows the agents to determine which shared nodes correspond to the same global node.  

For example, in the worst case, agents can transmit nodal positions along with nodal values. They can then determine which of their neighbors’ nodes correspond to the same global node. In a more efﬁcient method, agents group their nodal values by ROD edge, labeling each group with the neighboring agent’s identiﬁer and sorting the nodes with in each group in counter-clockwise order relative to the transmitting agent. This technique allows agents to associate the entries in their nodal vector with the nodal values of their neighbors.  

Overall, an agent’s memory use increases with the number of nodes in its local mesh. For a ﬁxed number of global nodes, adding more agents decreases the number of nodes per agent (and hence its memory usage); alternatively, adding more agents with ﬁxed capabilities allows the number of global nodes to increase. Thus the agents’ memory usage scales with respect to environment representation.  

Communication per agent does not directly depend on the environmental representation size, but rather it increases with the number of nodes on ROD boundaries. For Voronoi regions, as the number of agents increases, the number of Voronoi edges (and hence communication neighbors) per agent remains (on average) constant [20]. As agent density increases, Voronoi region perimeter decreases, and, for ﬁxed  $\ell_{\mathrm{max}}$  , the number of shared nodes per agent approaches the number of Voronoi vertices for that agent’s region. Hence, the communication requirements per agent do not increase with increasing agent density. The solutions obtained by individual agents, however, always match the global solution determined over an equivalent global mesh.  

# A. Queries  

Although agents estimate the ﬁeld within their own RODs, they do not know the ﬁeld outside their ROD. A query system allows a user at a base station to retrieve the estimate at any location by asking any agent. A user can make sparse queries over the whole ﬁeld to obtain a broad estimate and then focus on areas of particular interest. If there are many agents and measurements, making these queries requires signiﬁcantly less data than sending all measurements back to a central computer, a savings that is compounded if communication with a base station is more costly than agent- to-agent communication (e.g., satellite vs. XBee).  

Queries consist of the location of the desired estimate. Upon receiving a query, a naive approach employs depth ﬁrst search: if the point is in the agent’s region it returns the estimate, otherwise it forwards the request to one of its neighbors. That neighbor repeats the process, but will only return the request to the initiator after it has exhausted all other neighbors. Since the graph is connected and the agents completely cover the region, an agent with the desired information will eventually be found. This search can usually be made more efﬁcient by always picking the next neighbor based on which is closest to the vector from the current agent to the destination. By alternating between running ADMM iterations until convergence and several rounds of querying, time-varying ﬁelds can be monitored if they change slowly. Algorithm 1 displays the query process from a single agent’s perspective.  

#  

<td><table  border="1"><thead><tr><td><b>repeat</b></td></tr></thead><tbody><tr><td>1: neighbors ← {(id, ROD edge), .]</td></tr><tr><td>2: (from, location) ← receive-query-for-locationO</td></tr><tr><td>3: if location in ROD then</td></tr><tr><td>4:   send-estimate-to(from, my-estimate-at(location))</td></tr><tr><td> 5: else</td></tr><tr><td>6: </td></tr><tr><td>7:</td></tr><tr><td>to < edge-closest-to(location, neighbors)</td></tr><tr><td>8: </td></tr><tr><td>response ← send-query(to)</td></tr><tr><td>10: until has-estimate(response) or is-empty(neighbors)</td></tr><tr><td>if is-empty(neighbors) then</td></tr><tr><td> send-not-found-to(from) else</td></tr></tbody></table></td>  

# V. SIMULATIONS  

We simulate 18 agents in an environment given by  $\bar{\phi}(x,y)=x^{4}y\!+\!x^{3}y\!+\!\bar{y^{2}}\!+\!x^{2}\!+\!2$   over a unit square centered at  (0 . 5 ,  0 . 5) . The agents have zero mean Gaussian sensor noise with variance of  0 . 1 . Agents use  $\ell_{\mathrm{max}}=0.25$  ,    $\alpha_{0}=0$  ,  $\alpha_{1}=0.3$  ,  $\rho=1$  ,    $\mu_{i}=1$   and    $\phi_{0}(x,y)=0$  . Figure 2 depicts the agents’ Voronoi regions, their meshes, their estimates, and a location that the base station queried after running the ADMM algorithm for 8000 time-steps. Figure 3 depicts the actual ﬁeld and Figure 4 shows the error, averaged over 100 Monte-Carlo runs. The distributed solution and standard VIM solution over the same mesh yield the same results. The query was initially directed at agent zero, and the arrows show the chain of queries needed to determine the value at (0 . 7 ,  0 . 6) . Due to noise and because no interpolation method is perfect, the actual and estimated values differ.  

# VI. CONCLUSION  

We have introduced distributed VIM (DVIM), which uses a distributed FEM solver based on ADMM to generate envi- ronment estimates. Although the estimates generated match a centralized solution, no single agent has a full environment representation. The scaling properties of the algorithm mean  

  
Fig. 2. The agents estimates, the mesh, their Voronoi region, and the path of a query. Agent (and measurement) locations are marked with blue dots, with the numbers providing the agent’s identiﬁer.  

  
Fig. 3. The actual ﬁeld.  

that adding more agents more densely in the environment does not increase the communication or memory burdens of individual agents. More densely distributed agents, however, provide more detailed and accurate environment estimates. A query system allows a user to determine the environment estimate at any given location. Future work includes devel- oping control laws for mobile DVIM agents, estimating an error ﬁeld from VIM (using the techniques of [4], [6], [21]), and explicitly incorporating time-varying ﬁeld models into the estimation.  

# R EFERENCES  

[1] D. M. Glover, W. J. Jenkins, and S. C. Doney,  Modeling Methods for Marine Science . Cambridge: Cambridge University Press, 2011.

 [2] P. Brasseur, J. Beckers, J. Brankart, and R. Schoenauen, “Seasonal temperature and salinity ﬁelds in the mediterranean sea: Climatological analyses of a historical data set,”  Deep Sea Research Part I: Oceanographic Research Papers , vol. 43, no. 2, pp. 159 – 192, 1996.

 [3] C. Troupin, M. Ouberdous, D. Sirjacobs, A. Alvera-Azc´ arate, A. Barth, M.-E. Toussaint, S. Watelet, and J.-M. Beckers, Diva User Guide , Geo Hydrodynamics and Environment Research, MARE (GHER), University of Liege, 2015. [Online]. Available: http://modb.oce.ulg.ac.be

 [4] C. Troupin, A. Barth, D. Sirjacobs, M. Ouberdous, J.-M. Brankart, P. Brasseur, M. Rixen, A. Alvera-Azcrate, M. Belounis, A. Capet, F. Lenartz, M.-E. Toussaint, and J.-M. Beckers, “Generation of analysis and consistent error ﬁelds using the data interpolating variational analysis (diva),”  Ocean Modelling , vol. 5253, no. 0, pp. 90 – 101, 2012.  

  
Fig. 4. Absolute value of the error between the estimate and the actual ﬁeld averaged over 100 Monte-Carlo runs.  

[5] A. Barth, A. A. Azc´ arate, P. Joassin, J.-M. Becers, and C. Troupin, Introduction to Optimal Interpolation and Variational Analysis , Geo- Hydrodynamics and Environment Research (GHER), 2008.

 [6] J. Brankart and P. Brasseur, “The general circulation in the mediterranean sea: a climatological approach,”  Journal of Marine Systems , vol. 18, no. 13, pp. 41 – 70, 1998.

 [7] J. Fish and T. Belytschko,  A First Course in Finite Elements . John Wiley & Sons, 2007.

 [8] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical learning via the alternating direction method of multipliers,”  Foundations and Trends ⃝ in Machine Learn- ing , vol. 3, no. 1, pp. 1–122, 2011.

 [9] W. Bangerth, C. Burstedde, T. Heister, and M. Kronbichler, “Algo- rithms and data structures for massively parallel generic adaptive ﬁnite element codes,”  ACM Trans. Math. Softw. , vol. 38, pp. 14/1–28, 2011.

 [10] D. P. Bertsekas and J. N. Tsitsiklis,  Parallel and Distributed Compu- tation: Numerical Methods . Upper Saddle River, NJ, USA: Prentice- Hall, Inc., 1989.

[11] S. Martinez, “Distributed interpolation schemes for ﬁeld estimation by mobile sensor networks,”  Control Systems Technology, IEEE Transac- tions on , vol. 18, no. 2, pp. 491–500, March 2010.

 [12] K. Lynch, I. Schwartz, P. Yang, and R. Freeman, “Decentralized environmental modeling by mobile sensor networks,”  Robotics, IEEE Transactions on , vol. 24, no. 3, pp. 710–724, June 2008.

 [13] J. Cortes, “Distributed kriged kalman ﬁlter for spatial estimation,” Automatic Control, IEEE Transactions on , vol. 54, no. 12, pp. 2816– 2827, Dec 2009.

 [14] B. J. Julian, M. Angermann, M. Schwager, and D. Rus, “Distributed robotic sensor networks: An information-theoretic approach,”  Interna- tional Journal of Robotics Research , vol. 31, no. 10, pp. 1134–1154, September 2012.

 [15] G. Battistelli, L. Chisci, N. Forti, S. Selleri, and G. Pelosi, “Decentralized consensus ﬁnite-element kalman ﬁlter for ﬁeld estimation,”  CoRR , vol. abs/1604.02392, 2016.

 [16] S. Grime and H. Durrant-Whyte, “Data fusion in decentralized sensor networks,”  Control Engineering Practice , vol. 2, no. 5, pp. 849 – 863, 1994.

 [17] G. Battistelli, L. Chisci, N. Forti, G. Pelosi, and S. Selleri, “Distributed ﬁnite element kalman ﬁlter,” in  Control Conference (ECC), 2015 European , July 2015, pp. 3695–3700.

 [18] L. Rineau, “2D conforming triangulations and meshes,” in  CGAL User and Reference Manual , 4.7 ed. CGAL Editorial Board, 2015.

 [19] J. Cortes, S. Martinez, T. Karatas, and F. Bullo, “Coverage control for mobile sensing networks,” in  Robotics and Automation, 2002. Proceedings. ICRA ’02. IEEE International Conference on , vol. 2, 2002, pp. 1327–1332.

 [20] A. Okabe, B. Boots, K. Sugihara, S. N. Chiu, and D. G. Kendall, Spatial Tessellations: Concepts and Applications of Voronoi Diagrams , 2nd ed. John Wiley and Sons, 2000.

 [21] P. C. McIntosh, “Oceanographic data interpolation: Objective analysis and splines,”  Journal of Geophysical Research: Oceans , vol. 95, no. C8, pp. 13 529–13 541, 1990.  