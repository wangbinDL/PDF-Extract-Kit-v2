# Minimum stationary values of sparse random directed graphs  

Xing Shi Cai \*   and Guillem Perarnau \*\*  

\* Mathematics Department, Uppsala University, Sweden. Email:  xingshi.cai@tutanota.com . \*\* Departament de Matem\` atiques (MAT), Universitat Polit\` ecnica de Catalunya (UPC), Barcelona, Spain. Email:  guillem.perarnau@upc.edu .  

# November 2, 2021  

# Abstract  

We consider the stationary distribution of the simple random walk on the directed conﬁgu- ration model with bounded degrees. Provided that the minimum out-degree is at least 2, with high probability (whp) there is a unique stationary distribution (unicity regime). We show that the minimum positive stationary value is whp    $n^{-(1+C+o(1))}$    for some constant    $C\geq0$   determined by the degree distribution, answering a question raised by Bordenave, Caputo and Salez [ 5 ]. In particular,    $C$   is the competing combination of two factors: (1) the contribution of atypically “thin” in-neighbourhoods, controlled by subcritical branching processes; and (2) the contribu- tion of atypically “light” trajectories, controlled by large deviation rate functions. Additionally, we give estimates for the expected lower tail of the empirical stationary distribution. As a by- product of our proof, we obtain that the maximal hitting and the cover time are both    $n^{1+C+o(1)}$  whp. Our results are in sharp contrast to those of Caputo and Quattropani [ 11 ] who showed that under the additional condition of minimum in-degree at least 2 (ergodicity regime), stationary values only have logarithmic ﬂuctuations around    $n^{-1}$  .  

# 1 Introduction  

# 1.1 The directed conﬁguration model  

The directed conﬁguration model was introduced by Cooper and Frieze in [ 14 ]. Let   $[n]:=\{1,.\,.\,.\,,n\}$  be a set of    vertices. Let    $\vec{\bf d}_{n}\;=\;((d_{1}^{-},d_{1}^{+}),.\,.\,.\,,(d_{n}^{-},d_{n}^{+}))$  )) be a bi-degree sequence with    $m\ :=$   $\begin{array}{r}{\sum_{i\in[n]}d_{i}^{+}=\sum_{i\in[n]}d_{i}^{-}}\end{array}$   P . Let    $\delta^{\pm}$   and   $\Delta^{\pm}$   be the minimum and maximum in/out-degree, respectively. ∈ ∈ The directed conﬁguration model, which we denote by    $\vec{\mathbb{G}}_{n}\;=\;\vec{\mathbb{G}}_{n}(\vec{\mathbf{d}}_{n})$  ), is the random directed multigraph on   $[n]$   generated by giving    $d_{i}^{-}$  heads  (in-half-edges) and    $d_{i}^{+}$  tails  (out-half-edges) to vertex    $i$  , and then pairing the heads and the tails uniformly at random.  

The directed conﬁguration model is of practical importance as many complex real-world net- works are directed. For instance, it has been used to study neural networks [ 2 ], Google’s PageRank algorithm [ 13 ], and social networks [ 19 ].  

The original paper by Cooper and Frieze [ 14 ] studies the birth of a linear size strongly connected component ( scc ) in    $\vec{\mathbb{G}}_{n}$  . Their result was recently improved by Graf [ 17 ] and by the two authors [ 8 ].  

Lately, there has been some progress on the distances in the directed conﬁguration model for bi-degree sequences with ﬁnite covariances. Typical distances in    $\vec{\mathbb{G}}_{n}$   were studied by van der Hoorn and Olvera-Cravioto [ 26 ]. Caputo and Quattropani [ 11 ] showed that the diameter of    $\vec{\mathbb{G}}_{n}$   is asymptotically equal to the typical distance, provided that    $\delta^{\pm}\geq2$   and   $\Delta^{\pm}=O(1)$  . In our previous work [ 9 ], we showed that the diameter has diﬀerent behaviour if no constraints on the minimum degree are imposed.  

One motivation to the study of distances in    $\vec{\mathbb{G}}_{n}$   is its close connection to certain properties of random walks, in particular, to their stationary distribution, denoted by    $\pi$  . While    $\pi$   is trivially determined by the degree sequence in undirected graphs, in the directed case    $\pi$   is a complicated random measure that depends on the geometry of the random digraph. Cooper and Frieze [ 15 ] initiated the study of    $\pi$   in random digraphs, determining it on the strong connectivity regime of the directed Erd˝ os-R´ enyi random graph. They also established a relation between the minimum stationary value and stopping times such as the hitting and the cover time. Extremal stationary values for the    $r$  -out random digraph were studied by Addario-Berry, Balle, and the second author [ 1 ].  

Regarding the directed conﬁguration model, Bordenave, Caputo and Salez [ 5 ,  6 ] studied the mixing time of a random walk on    $\vec{\mathbb{G}}_{n}$   and showed that it exhibits cutoﬀ. Additionally, they proved that for a vertex    $i\in[n]$  ,    $\pi(i)$   is essentially determined by the local in-neighbourhood of    $i$   and well- approximated by a deterministic law (see  Remark 1.8 ). These results provide a precise description of typical stationary probabilities but fall short to capture the exceptional values of    $\pi$  .  

In [ 5 ], the authors raised the question of studying the extremal values of the stationary distri- bution in    $\vec{\mathbb{G}}_{n}$  . Let    $\pi_{\mathrm{min}}$   and    $\pi_{\mathrm{max}}$   be the smallest and largest positive values of    $\pi$  , respectively. From now on and throughout this paper, we will assume that all degrees are bounded; i.e.,   $\Delta^{\pm}=O(1)$  . In this context, the condition    $\delta^{+}\geq2$   is essentially necessary to avoid (possibly many) trivial stationary measures (see  Remark 1.4 ). Under the additional condition    $\delta^{-}\geq2$  , Caputo and Quattropani [ 11 ] showed that the random walk is ergodic with high probability (whp), so we call it the  ergodicity regime , and that there exists    $C\geq1$   such that, whp  

$$
\begin{array}{l}{C^{-1}\displaystyle\frac{\log^{1-\gamma_{0}}n}{n}\le\pi_{\mathrm{min}}\le C\displaystyle\frac{\log^{1-\gamma_{1}}n}{n}\:,}\\ {C^{-1}\displaystyle\frac{\log^{1-\kappa_{1}}n}{n}\le\pi_{\mathrm{max}}\le C\displaystyle\frac{\log^{1-\kappa_{0}}n}{n}\:,}\end{array}
$$  

whe  $\gamma_{0}\geq\gamma_{1}\geq1$   are deﬁned in terms of    $\delta^{-}$  and   $\Delta^{+}$  , and    $\kappa_{0}\leq\kappa_{1}\leq1$   are deﬁned in terms of    $\delta^{+}$  and ∆  $\Delta^{-}$  − . While these constants are diﬀerent in general,    and    if there are a linearly  $\gamma_{0}=\gamma_{1}$   $\kappa_{0}=\kappa_{1}$  many vertices with degrees   $(\delta^{-},\Delta^{+})$   and   $(\delta^{+},\Delta^{-})$  , respectively. Finally, using the bound on    $\pi_{\mathrm{min}}$  , the authors showed that in the ergodicity regime the cover time satisﬁes whp  

$$
C^{-1}n\log^{\gamma_{1}}n\leq\tau_{\mathrm{cov}}\leq C n\log^{\gamma_{0}}n\;.
$$  

The main purpose of this paper is to study the extremal values of the stationary distribution outside the ergodicity regime. If    $\delta^{+}\geq2$   but no condition on the minimum in-degree is imposed, the random walk might fail to be ergodic, but the stationary measure is whp unique; we call it the  unicity regime . While ( 1.1 ) and ( 1.2 ) indicate that the extremal values exhibit logarithmic ﬂuctuations in the ergodicity regime, our main result shows in that unicity regime    $\pi_{\mathrm{min}}$   has polynomial deviations with respect to the typical stationary values. As an easy consequence of our proof, we determine the maximal hitting and the cover time up to subpolynomial multiplicative terms.  

# 1.2 Notations and results  

Before stating our results, we need to deﬁne some parameters of the bi-degree sequence    $\vec{\bf d}_{n}$  . Al- though    $n$   does not appear in many of the notations, the reader should keep in mind that all the parameters deﬁned here depend on    $n$  .  

Let    $n_{k,\ell}:=|\{i:(d_{i}^{-},d_{i}^{+})=(k,\ell)\}|$  }|  be the number of   $(k,\ell)$   in    $\vec{\bf d}_{n}$  . Let   $\begin{array}{r}{\Delta^{\pm}=\operatorname*{max}_{i\in[n]}\{d_{i}^{\pm}\},\delta^{\pm}=}\end{array}$     $\mathrm{min}_{i\in[n]}\{d_{i}^{\pm}\}$    } . Let    $D=(D^{-},D^{+})$  s (number of heads and tails) of a uniform random vertex. In other words,  $\mathbb{P}\left\{D=(k,\ell)\right\}=n_{k,\ell}/n$  .  

The bivariate generating function of  D  is deﬁned by  

$$
f(z,w):=\sum_{k,\ell\geq0}\mathbb{P}\left\{D=(k,\ell)\right\}z^{k}w^{\ell}\;.
$$  

Let    $\lambda:=m/n$  . Consider a branching process with oﬀspring distribution that has generating function  $\frac{1}{\lambda}\frac{\partial f}{\partial w}(z,1)$   1). Let    $s^{-}$  be its survival probability and let  ν  $\begin{array}{r}{\nu\,:=\,\frac{1}{\lambda}\frac{\partial f}{\partial w}(1,1)}\end{array}$   1) be its expected number of oﬀspring, which we call the  expansion rate . Deﬁne the  subcritical in-expansion rate  by  

$$
\hat{\nu}^{-}:=\frac{1}{\lambda}\frac{\partial^{2}f}{\partial z\partial w}(1-s^{-},1)\in[0,1)\;.
$$  

Deﬁne    $D_{\mathrm{out}}=(D_{\mathrm{out}}^{-},D_{\mathrm{out}}^{+})$  ), the  out-size-biased  distribution of    $D$  , by  

$$
\mathbb{P}\left\{D_{\mathrm{out}}=(k,\ell)\right\}:=\frac{\ell}{\lambda}\mathbb{P}\left\{D=(k,\ell)\right\}\;,\qquad\mathrm{for}\ k\geq0,\ell\geq0\;.
$$  

In words,    $D_{\mathrm{{out}}}$   is the degree distribution of a vertex incident to a uniform random tail. We deﬁne  $D_{\mathrm{in}}$   analogously.  

Let   $\tilde{D}_{\mathrm{out}}=(\tilde{D}_{\mathrm{out}}^{-},\tilde{D}_{\mathrm{out}}^{+})$  ) be the random vector with distribution  

$$
\mathbb{P}\left\{\tilde{D}_{\mathrm{out}}=(k,\ell)\right\}:=\frac{k\left(1-s^{-}\right){}^{k-1}}{\hat{\nu}^{-}}\mathbb{P}\left\{D_{\mathrm{out}}=(k,\ell)\right\}\ ,\qquad\mathrm{for}\ k\geq0,\ell\geq0\ .
$$  

Deﬁne the  subcritical in-entropy  by  

$$
\hat{H}^{-}:=\mathbb{E}\left[\log\tilde{D}_{\mathrm{out}}^{+}\right]=\frac{1}{\hat{\nu}^{-}m}\sum_{i\in[n]}d_{i}^{-}d_{i}^{+}(1-s^{-})^{d_{i}^{+}-1}\log d_{i}^{+}\;.
$$  

This parameter can be seen as an average row entropy of certain transition matrix (see [ 6 ]) and is related to the typical weight of trajectories under subcritical in-growth.  

The  large deviation rate function  (or  Cram´ er function ) of    $Z=\log\tilde{D}_{\mathrm{out}}^{+}$    is deﬁned by  

$$
I(z):=\operatorname*{sup}_{x\in\mathbb{R}}\{x z-\log\mathbb{E}[e^{x Z}]\}\;,\qquad{\mathrm{for~}}z\in\mathbb{R}\;.
$$  

Note that    $I(\hat{H}^{-})=0$  ) = 0 and    $I(z)=\infty$  if    $z>\log\Delta^{+}$    or    $z<\log\delta^{+}$  . Let  

$$
\phi(a):=\frac{1}{a}\left(|\log\hat{\nu}^{-}|+I(a\hat{H}^{-})\right)\;,
$$  

and let    $a_{0}$   be its minimising value in   $[0,\infty)$  , which is attained in   $[1,\infty)$   by the properties of    $I(z)$  .  

Conditioned on    $\vec{\mathbb{G}}_{n}$  , a simple random walk on    $\vec{\mathbb{G}}_{n}$   is a Markov process   $(Z_{t})_{t\ge0}$   with state space  $[n]$  . Given the current vertex    $Z_{t}$  , the walk chooses an out-neighbour of    $Z_{t}$   uniformly at random as  $Z_{t+1}$  , which is always possible as we assume    $\delta^{+}\geq2$  . If as    $t\to\infty$  the distribution of    $Z_{t}$   converges to the same distribution    $\pi$   regardless of the choice of    $Z_{0}$  , i.e., if there exists a probability density function    $\pi$   on   $[n]$   such that  

$$
\operatorname*{lim}_{t\to\infty}\operatorname*{sup}_{i,j\in[n]}\left|\mathbb{P}\left\{Z_{t}=j\mid Z_{0}=i\right\}-\pi(j)\right|=0\;,
$$  

we say   $(Z_{t})_{t\ge0}$   has  unique stationary distribution    $\pi$  . Let  

$$
\pi_{\mathrm{min}}=\operatorname*{min}\left\{\pi(i):i\in[n],\pi(i)>0\right\}\;,\qquad\pi_{\mathrm{max}}=\operatorname*{max}\left\{\pi(i):i\in[n]\right\}\;.
$$  

Our main result is the following:  

Theorem 1.1.  Assume that    $\delta^{+}\,\geq\,2$   and    $\Delta^{\pm}\leq M$   where    $M\,\in\,\mathbb{N}$   is a ﬁxed integer. With high probability,  

$$
\pi_{\mathrm{min}}=n^{-(1+\hat{H}^{-}/\phi(a_{0})+o(1))}\ .
$$  

$\delta^{-}\geq2$  ,    $\pi_{\mathrm{min}}$   satisﬁes ( 1.1 ). In this case,  Theorem 1.1  implies the  statement    $\pi_{\mathrm{min}}=$   $n^{-1+o(1)}$    as we have  $\hat{H}^{-}\,\leq\,\log\,M$  ≤  and    $\phi(a)=\infty$  for any    $a\,\geq\,0$  , since ˆ  $\hat{\nu}^{-}\,=\,0$  = 0. Thus, our main contribution is to show that when    $\delta^{-}\in\{0,1\}$  , the polynomial exponent of    $\pi_{\mathrm{min}}$   is not    $-1$   any more. The additional exponent  $\hat{H}^{-}/\phi(\alpha_{0})$  ) comes from the fact that whp some vertices are exceedingly diﬃcult to reach by a simple random walk. In fact, determining the minimal stationary value can be seen as a competitive combination of two factors: (1) being far from the bulk of other vertices, which is controlled by the term    $|\log{\hat{\nu}^{-}}|$  |  in ( 1.10 ) and (2) having large branching factors in the trajectories leading to a vertex, which is controlled by the term    $I(a\hat{H^{-}})$  ) in ( 1.10 ). The optimal  $\begin{array}{r}{\left(\frac{1}{a_{0}\phi(a_{0})}+\frac{1}{\log\nu}+o(1)\right)\log n}\end{array}$  ratio is given by    $a_{0}$  . In particular, the vertex that minimises the stationary value is at distance   from the bulk of other vertices. Only when    $a_{0}=1$  , this vertex coincides with the vertex that is furthest from the bulk, which is at distance  $\begin{array}{r}{\bigg(\frac{1}{|\log{\hat{\nu}^{-}}|}+\frac{1}{\log{\nu}}+o(1)\bigg)\log{n}}\end{array}$    (see [ 9 ]).  

Remark 1.2.  Formally, ( 1.13 ) should be read as  

$$
\frac{\log\pi_{\operatorname*{min}}^{-1}}{\log n}\to1+\frac{\hat{H}^{-}}{\phi(a_{0})}\quad\mathrm{~in~probability,~}
$$  

where we let    $\pi_{\mathrm{min}}$   take an arbitrary value in case that the stationary distribution is non-unique. From our proof, one can obtain an upper bound on the speed of convergence of order   $(\log n)^{-1/2}$  . Whp bounds for    $7/1\,\mathrm{{min}}$   that are tight up to a constant, like the ones in ( 1.1 ), are unlikely to hold in this setting due to the use of large deviation theory. In fact, we believe that the rate of convergence  $(\log n)^{-1/2}$    cannot be vastly improved, as this is the rate of convergence in Cramer’s theorem (see Theorem  2.4  in Section  2.3 ).  

Remark 1.3.  Let    $\vec{\mathbb{G}}_{n}^{\mathrm{s}}$    be    $\vec{\mathbb{G}}_{n}$   conditioned on being a simple directed graph. Then    $\vec{\mathbb{G}}_{n}^{\mathrm{s}}$    is distributed uniformly among all simple directed graphs with degree sequence    $\vec{\bf d}_{n}$  . It is well-known that the probability of    $\vec{\mathbb{G}}_{n}$  n  being simple is bounded away from zero when the maximum degrees are bounded (see, e.g., [ 4 ,  18 ]). Thus,  Theorem 1.1  also holds for    $\vec{\mathbb{G}}_{n}^{\mathrm{s}}$  .  

Remark 1.4  (Uniqueness of    $\pi$  ) .  Theorem 1.1  requires    $\delta^{+}\,\geq\,2$     $\delta^{+}\,=\,0$  , then the stationary distribution is almost surely either trivial or non-unique. If  δ  $\delta^{+}=1$    = 1 and    $\mathbb{P}\left\{D^{+}=1\right\}$   is bounded away from 0 as    $n\to\infty$  , then with constant probability there will be multiple loops in    $\check{\mathbb{G}}_{n}$   giving rise to multiple trivial stationary distributions.  Proposition 4.1  shows that under    $\delta^{+}\geq2$  ,   $(Z_{t})_{t\ge0}$   has a unique stationary distribution whp. Unicity of the equilibrium measure whp can be also shown if    $\delta^{+}=1$   and    $\mathbb{P}\left\{D^{+}=1\right\}=o(1)$  . It is likely that the conclusion of  Theorem 1.1  still holds in this situation.  

Remark 1.5  (Maximum stationary value) .  In this paper we turned our attention to    $\pi_{\mathrm{min}}$  . By averaging,    $\pi_{\mathrm{max}}\geq1/n$  . Moreover, one can check tha the proof of t ality in ( 1.2 ) (see [ 11 , Section 3.5]) does not use any condition on  δ  $\delta^{-}$  − . Therefore,  π  $\pi_{\mathrm{max}}=n^{-1+o(1)}$    holds in the setting of  Theorem 1.1 . It would be interesting to understand the behaviour of    $\pi_{\mathrm{max}}$   when the maximum in-degree goes to inﬁnity as    $n\to\infty$  .  

Remark 1.6  (Explicit polynomial exponents for    $\pi_{\mathrm{min}}$  ) .  Since for a general distribution there is no closed-form expression for    $I(z)$  ,  Theorem 1.1  provides an implicit polynomial exponent. Never- theless,    $I(z)$   can be computed explicitly for some particular bi-degree sequences, yielding explicit polynomial exponents. In  Subsection 5.2 , we give two such examples: one where    $I(z)=\infty$  for any  $z\neq{\hat{H}}^{-}$  and the other where    $I(z)$   is the large deviation rate function of the Bernoulli distribution with an aﬃne transformation. If explicit bounds are required, there is an extensive literature on concentration inequalities for the sum of independent bounded random variables, such as Bern- stein’s and Bennett’s inequalities, (see, e.g., [ 22 ]) which provide explicit lower bounds on    $I(z)$   in terms of the moments of the distribution. Alternatively, rigorous numeric bounds can be computed with interval arithmetic libraries such as [ 24 ].  

Remark 1.7.  We can choose    $D$   to make the polynomial exponent in ( 1.13 ) arbitrarily small. For instance, ﬁxing    $M\in\mathbb{N}$   and letting  

$$
\mathbb{P}\left\{D^{-}=1\right\}=\frac{M-2}{M-1}\;,\quad\mathbb{P}\left\{D^{-}=M\right\}=\frac{1}{M-1}\;,\quad\mathbb{P}\left\{D^{+}=2\right\}=1\;,
$$  

we have   $\hat{\nu}^{-}=1\!+\!O(M^{-1})$  ),  $\hat{H}^{-}=\log2$  = log 2 and    $I(a\hat{H}^{-})=\infty$   ∞ for    $a\ne1$  . So    $a_{0}=1$   and    $\phi(a_{0})=\vert\log\hat{\nu}^{-}\vert$  | . Thus,  $\hat{H}^{-}/\phi(a_{0})=\Omega(M)$  ) and we can make it as large as we want by increasing    $M$  .  

Remark 1.8.  Consider the empirical measure  $\begin{array}{r}{\psi=\frac{1}{n}\sum_{i\in[n]}\delta_{\{n\pi(i)\}}}\end{array}$  P ; that is,    $n$   times the stationary value of a uniform random vertex. In [ 5 ], it was shown that there exists a deterministic law    $\mathcal{L}$  such that    $d_{\mathcal{W}}(\psi,\mathcal{L})\to0$   in probability, where    $d_{\mathcal{W}}$   is the 1-Wasserstein metric related to optimal transport problems. Our proof of  Theorem 1.1  allows us to control the lower tail of    $\psi$  : for every  $\alpha\in[0,\hat{H}^{-}/\phi(a_{0})]$  )] and letting    $\begin{array}{r}{\beta=\frac{\alpha\phi(a_{0})}{\hat{H}^{-}}\in[0,1]}\end{array}$   1], we have  

$$
\mathbb{E}[\psi((0,n^{-\alpha}])]=\frac{1}{n}\sum_{i\in[n]}\mathbb{P}\left\{0<\pi(i)\le n^{-(1+\alpha)}\right\}=n^{-\beta+o(1)}\;.
$$  

By computing the second moment, it can be shown that    $\psi\big((0,n^{-\alpha}]\big)$   is concentrated around it expected value. See  Subsection 4.6  for a discussion of the proof of ( 1.16 ).  

Let    $G$   be a directed graph with verte  set   $[n]$   having an attractive  scc    $\mathcal{C}_{0}$   with vertex set    $\nu_{0}$  . Let   $(Z_{t})_{t\ge0}$   be a simple random walk on  G . Let    $\tau_{x}(y):=\operatorname*{inf}\left\{t\geq0:Z_{t}=y,Z_{0}=x\right\}$  . The  maximal hitting time  is deﬁned as  

$$
\tau_{\mathrm{hit}}:=\operatorname*{max}_{\boldsymbol{x}\in[n]}\mathbb{E}[\tau_{\boldsymbol{x}}(\boldsymbol{y})]\;.
$$  

Let    $\tau_{x}^{C}:=\operatorname*{inf}\{t\geq0:\mathcal{V}_{0}\subseteq\cup_{r=0}^{t}\{Z_{r}\},Z_{0}=x\}$  {  ≥  V   ⊆∪ { } } . The  cover time  is deﬁned as  

$$
\tau_{\mathrm{cov}}:=\operatorname*{max}_{x\in[n]}\mathbb{E}[\tau_{x}^{C}]\;.
$$  

As a consequence of the proof of  Theorem 1.1 , we determine the maximal hitting and the cover time, up to subpolynomial terms.  

Theorem 1.9.  Under the hypothesis of  Theorem 1.1 , whp  

$$
\tau_{\mathrm{hit}}=n^{1+\hat{H}^{-}/\phi(a_{0})+o(1)},\qquad\tau_{\mathrm{cov}}=n^{1+\hat{H}^{-}/\phi(a_{0})+o(1)}\;.
$$  

To simplify the notations, we avoid using  ⌈·⌉ and    $\lfloor\cdot\rfloor$  to make certain parameters integers. Such omissions should be clear from the context and do not aﬀect the validity of the proofs.  

The rest of paper contains four sections:  Section 2  studies the properties of marked branching processes;  Section 3  describes a graph exploration process and shows that it can be coupled with a marked branching process; using these results,  Section 4  proves  Theorem 1.1 ; ﬁnally in  Section 5 we give some applications, including the proof of  Theorem 1.9 .  

# 2 Marked branching processes  

In this section, we prove some general results for the marked branching process deﬁned below. This part of the paper may be of independent interest.  

# 2.1 Marked branching processes  

Let    $\eta=(\xi,\zeta)$   be a random vector on    $\mathbb{Z}_{\geq0}^{2}$    and let (  $\langle\eta_{i,t}=(\xi_{i,t},\zeta_{i,t}))_{i\geq1,t\geq0}$   be iid (independent and ≥ identically distributed) copies of    $\eta$  . The  branching process , also known as the  Galton-Watson tree ,  $(X_{t})_{t\geq0}$   with oﬀspring distribution    $\xi$   is deﬁned by  

$$
X_{t}={\left\{\begin{array}{l l}{1}&{\qquad{\mathrm{if~}}t=0\ ,}\\ {\sum_{i=1}^{X_{t-1}}\xi_{i,t-1}}&{\qquad{\mathrm{if~}}t\geq1\ .}\end{array}\right.}
$$  

We call    $X_{t}$   the    $t$  -th  generation  and refer to   $(X_{r})_{t\geq r\geq0}$   as the  ﬁrst    $t$   generations . For an individual  $(i,t)$  , we call    $i$   its  sibling index  and    $t$   its  generation index . If the individual   $(i,t)$   is marked (labelled) by the integer    $\zeta_{i,t}$  , then we call   $(X_{t})_{t\geq0}$   the  marked branching process  with oﬀspring distribution    $\eta$  .  

Let    $g$   be the bivariate probability generating function of    $\eta$  , i.e.,  

$$
g(z,w):=\sum_{k,\ell\geq0}\mathbb{P}\left\{\eta=(k,\ell)\right\}z^{k}w^{\ell}\;.
$$  

and let    $g(z):=g(z,1)$   be the probability generating function of    $\xi$  . Deﬁne  

$$
\begin{array}{r}{\nu:=\mathbb{E}\left[\xi\right]=g^{\prime}(1)\;.}\end{array}
$$  

Given    $i\in[X_{t}]:=\{1,\dots,X_{t}\}$   and    $r\in[0,t]$  , let    $f^{r}(i,t)\in[X_{t-r}]$   be the sibling index of   $(i,t)$  ’s ancestor    $r$   generations away. We write    $f(i,t)=f^{1}(i,t)$   and note    $f^{0}(i,t)=i$  . Let  

$$
\Gamma_{i,t}:=\prod_{r=1}^{t}\frac{1}{\zeta_{f^{t-r}(i,t),r}}\;,\;\;\;\;\;\;\;\;\Gamma_{t}:=\sum_{i=1}^{X_{t}}\Gamma_{i,t}\;.
$$  

In this section, we will mostly be interested in the sequence of random variables   $(\Gamma_{t})_{t\geq1}$  . Let    ${\mathcal{F}}_{t}$  be the    $\sigma$  -algebra generated by   $(\xi_{i,r})_{i\geq1,t>r\geq0},(\zeta_{i,r})_{i\geq1,t\geq r\geq0}$  . Note that    $X_{t}$   and   $\Gamma_{t}$   are measurable with respect to    ${\mathcal{F}}_{t}$  . Moreover,   $\Gamma_{t}>0$   if and only if    $X_{t}>0$  .  

The following lemma is similar to [ 10 , Lemma 17].  

Lemma 2.1.  If    $\mathbb{E}[\xi/\zeta]\in(0,\infty)$  , then    $\Gamma_{t}\mathbb{E}\left[\xi/\zeta\right]^{-t}$    is a martingale with respect to    $(\mathcal{F}_{t})_{t\geq0}$  .  

Proof.  We have  

$$
\begin{array}{r l}{\mathbb{E}[\Gamma_{t}\mid\mathcal{F}_{t-1}]=}&{\displaystyle\sum_{j=1}^{X_{t-1}}\mathbb{E}\left[\sum_{i:f(i,i)=j}\Gamma_{i,i}\;\Bigg|\;\mathcal{F}_{t-1}\right]}\\ &{=\displaystyle\sum_{j=1}^{X_{t-1}}\Gamma_{j,t-1}\mathbb{E}\left[\sum_{i:f(i,i)=j}1/\zeta_{i,t}\;\Bigg|\;\mathcal{F}_{t-1}\right]}\\ &{=\displaystyle\sum_{j=1}^{X_{t-1}}\Gamma_{j,t-1}\mathbb{E}\left[\xi_{j,t-1}\mid\mathcal{F}_{t-1}\right]\mathbb{E}\left[1/\zeta_{i,t}\mid\mathcal{F}_{t-1}\right]}\\ &{=\displaystyle\sum_{j=1}^{X_{t-1}}\Gamma_{j,t-1}\mathbb{E}[\xi/\zeta]=\mathbb{E}[\xi/\zeta]\Gamma_{t-1}\;,}\end{array}
$$  

where we use Wald’s equation and that    $\left\{\xi_{j,t-1}\right\}\cup\left\{\zeta_{i,t}\right\}_{i:\,f(i)=j}$   is a mutually independent collection of random variables, conditional on    $\mathcal{F}_{t-1}$  .  $\boxed{\begin{array}{r l}\end{array}}$  

Remark 2.2.  Recall the deﬁnition of the distribution    $D_{\mathrm{out}}$   in ( 1.6 ) and let    $\eta\,{\stackrel{\mathcal{L}}{=}}\,D_{\mathrm{out}}$  . Since    $\ell\geq$   $\delta^{+}\geq2$  , we have  

$$
\mathbb{E}[\xi/\zeta]=\mathbb{E}[D_{\mathrm{out}}^{-}/D_{\mathrm{out}}^{+}]=\sum_{k,\ell\geq1}\frac{k}{\ell}\cdot\frac{\ell n_{k,\ell}}{m}=\frac{1}{m}\sum_{k,\ell\geq1}k n_{k,\ell}=1\;.
$$  

In other words, if we take    $\eta=D_{\mathrm{out}}$  ,   $\Gamma_{t}$   is a martingale by  Lemma 2.1 . We have formulated the lemma in a way that will allow us to deal with small perturbations of    $D_{\mathrm{{out}}}$  .  

# 2.2 Conditioned branching processes  

Before introducing the results, some more deﬁnitions are needed.  

# 2.2.1 Conditioned on extinction  

Let    $s:=\mathbb{P}\left\{\cap_{t\geq0}[X_{t}>0]\right\}$   be the  survival probability  of   $(X_{t})_{t\geq0}$  . The  conjugate probability distribu- tion  of    $\xi$  , denoted by   $\hat{\xi}$  , is deﬁned as  

$$
\mathbb{P}\left\{\hat{\xi}=k\right\}:=(1-s)^{k-1}\mathbb{P}\left\{\xi=k\right\}\mathrm{~,~}
$$  

when    $s<1$  , while  

$$
\mathbb{P}\left\{\hat{\xi}=1\right\}=\mathbb{P}\left\{\xi=1\right\}\mathrm{~,~}\qquad\mathbb{P}\left\{\hat{\xi}=0\right\}=1-\mathbb{P}\left\{\xi=1\right\}\mathrm{~,~}
$$  

when    $s=1$  . Deﬁne the  subcritical expansion rate  as  

$$
\hat{\nu}:=\mathbb{E}[\hat{\xi}]=g^{\prime}(1-s)\in[0,1)\;.
$$  

If    $\eta\,{\stackrel{\mathcal{L}}{=}}\,D_{\mathrm{out}}$  , then   $\hat{\nu}=\hat{\nu}^{-}$  − where   $\hat{\nu}^{-}$  is deﬁned in ( 1.5 ).  

The following  duality  is well-known:  

Theorem 2.3  (see, e.g., Theorem 3.7 in [ 25 ]) .  Let    $(X_{t})_{t\geq0}$   be a branching process with oﬀspring distribution    $\xi$   and survival probability    $s$  . If    $s<1$  , then the branching process    $(X_{t})_{t\geq0}$   conditioned on extinction is distributed as a branching process with oﬀspring distribution    $\hat{\xi}$  .  

# 2.2.2 Conditioned on survival  

Let   $(X_{t}^{*})_{t\geq0}\ \subseteq\ (X_{t})_{t\geq0}$  e th the individuals th some surviving  . Thus,  $\mathbb{P}\left\{X_{0}^{*}=0\right\}=1-s$   { }  −  and  $\mathbb{P}\left\{X_{0}^{\ast}=1\right\}=s$   { } . Conditioning on [  $[X_{0}^{*}=1]$  = 1], i.e., survival of (  $(X_{t})_{t\geq0}$  , ≥  $(X_{t}^{*})_{t\geq0}$    is a branching process with oﬀspring distribution    $\xi^{*}$  , deﬁned by ≥  

$$
\mathbb{P}\left\{\xi^{*}=k\right\}=\frac{\sum_{m\ge k}\mathbb{P}\left\{\xi=m\right\}{\binom{m}{k}}s^{k}(1-s)^{m-k}}{s}=\frac{s^{k-1}g^{(k)}(1-s)}{k!}\;,\qquad\mathrm{for~}k\ge1\;.
$$  

A simple computation gives  

$$
\mathbb{E}\left[\xi^{*}\right]=\sum_{k=1}^{\infty}\frac{k s^{k-1}g^{(k)}(1-s)}{k!}=\sum_{k=0}^{\infty}\frac{s^{k}g^{(k+1)}(1-s)}{k!}=g^{\prime}(1)=\nu\;.
$$  

Moreover,  

$$
\mathbb{P}\left\{\xi^{*}=1\right\}=\mathbb{P}\left\{X_{1}^{*}=1\mid X_{0}^{*}=1\right\}=g^{\prime}(1-s)=\hat{\nu}\;.
$$  

Let   $(\tilde{X}_{t})_{t\geq0}\subseteq(X_{t}^{*})_{t\geq0}$   ⊆   be the subprocess of the individuals that have  exactly one  surviving element ≥ ≥ in their oﬀspring. (Note that   $(\tilde{X}_{t})_{t\geq0}$   is not necessarily a connected process.) Then conditioned on ≥  $(i,t)\in(\ddot{X}_{t})_{t\geq0}$  ,    $\xi_{i,t}$   is distributed as   $\tilde{\xi}$  , deﬁned by ≥  

$$
\mathbb{P}\left\{\Tilde{\xi}=k\right\}=\mathbb{P}\left\{X_{1}=k\mid X_{1}^{*}=1\right\}=\frac{k(1-s)^{k-1}}{\hat{\nu}}\mathbb{P}\left\{\xi=k\right\},\quad\mathrm{for~}k\geq1\mathrm{~.}
$$  

Let   $\tilde{\eta}\,=\,(\tilde{\xi},\tilde{\zeta})$  ) be the distribution of    $\eta_{i,t}$   conditioned on   $(i,t)\;\in\;(\tilde{X}_{t})_{t\geq0}$  ≥ . Deﬁne the  subcritical entropy  of    $\eta$   as  

$$
\hat{H}:=\mathbb{E}[\log\tilde{\zeta}]=\frac{1}{\hat{\nu}}\sum_{k\geq1,\ell\geq0}k(\log\ell)(1-s)^{k-1}\mathbb{P}\left\{\eta=(k,\ell)\right\}=\frac{\mathbb{E}[\hat{\zeta}\log\zeta]}{\mathbb{E}[\hat{\zeta}]}\;.
$$  

$\eta\,{\stackrel{\mathcal{L}}{=}}\,D_{\mathrm{out}}$  , then   $\hat{H}=\hat{H}^{-}$  where  ${\hat{H}}^{-}$  This parameter is central in our results. If   is deﬁned in ( 1.8 ).  

Later, we will also consider the  inhomogeneous  branching process   $(\hat{X}_{t})_{t\geq0}$   in which the root has ≥ oﬀspring distribution   $\tilde{\xi}-1$   − 1 and all other individuals have oﬀspring distribution   $\hat{\xi}$  . Note that such a process will almost surely become extinct.  

# 2.3 Large deviation theory  

We will use Cram´ er’s theorem, a classical result in large deviation theory.  

Theorem 2.4  (see, e.g., Corollary 2.2.19 in [ 16 ]) .  Let    $Z_{1},\ldots,Z_{t}$   be iid copies of a random variable  $Z$   satisfying    $\mathbb{E}[e^{\lambda Z}]<\infty$  for all    $\lambda\in\mathbb{R}$  . Deﬁne  

$$
\bar{Z}_{t}=\frac1t\sum_{i=1}^{t}Z_{i}\ .
$$  

Then, for any    $z\geq\mathbb{E}[Z]$  

where  

$$
\operatorname*{lim}_{t\to\infty}\frac{1}{t}\log\mathbb{P}\left\{\bar{Z}_{t}\geq z\right\}=-I(z)\;,
$$  

$$
I(z):=\operatorname*{sup}_{\lambda\in\mathbb{R}}\{z\lambda-\log\mathbb{E}[e^{\lambda Z}]\}\;,\qquad f o r\;z\in\mathbb{R}\;,
$$  

is the  Fenchel-Legendre transform  of the cumulant generating function of    $Z$  .  

From now on we will take    $Z$   to be the discrete, random variable   $\log{\tilde{\zeta}}$   with support a subset of  $\{\log2,\log3,.\,.\,,\log M\}$  , where    $M$   is a ﬁxed integer, and expected value  $\hat{H}$  . Thus,    $I(z)$   will refer to the large deviation rate function of   $\log{\tilde{\zeta}}$   which has the properties that on    $z\in[\hat{H},\log M)$  ),    $I(z)$   is continuous, non-decreasing, with    $I^{\prime}(z)\in[0,\infty)$   and    $I(\hat{H})=0$  ) = 0. Moreover,    $I(z)$   is non-increasing on  $(-\infty,{\hat{H}})$  ). The proof for these properties follow along the line of Lemma 2.2.5 in [ 16 ].  

# 2.4 Subcritical growth: a lower bound  

Theorem 2.5.  Let    $(X_{r})_{r\geq0}$   be a marked branching process with oﬀspring distribution    $\eta=(\xi,\zeta)$   with  $\mathbb{E}[\xi]\in(1,\infty)$  . Suppose that    $\xi\le M$   and    $2\leq\zeta\leq M$   almost surely. Then for any    $a\in[1,\log(M)/\dot{H}]$  , and    $\omega\geq t$  ,  

$$
\mathbb{P}\left\{\left[0<\Gamma_{t}<e^{-a\hat{H}t}\right]\cap\bigcap_{r=1}^{t}[0<X_{r}<\omega]\right\}\geq\exp\left\{-\left(|\log\hat{\nu}|+I(a\hat{H})+o(1)\right)t\right\}.
$$  

The important event in the previous theorem is the ﬁrst one, regarding   $\Gamma_{t}$  . The event controlling the size of the ﬁrst    $t$   generations is only added to allow coupling the branching process with the graph exploration later in the paper.  

Proof.  For the sake of simplicity, we ﬁrst prove the theorem assuming that   $1$   is in the support of  $\xi$  . The modiﬁcations needed otherwise, are detailed at the end of the proof.  

Consider the events    $E_{1}=[X_{t}^{*}=1]$     $E_{2}=[X_{t}=1]$   and    $\begin{array}{r}{E_{3}=\bigcap_{r=1}^{t}[0<X_{r}<\omega]}\end{array}$  ]. The idea of the proof is to lower bound the probability in ( 2.17 ) conditioning on these events.  

When the event    $E_{1}$   happens, we call the ﬁrst    $t$   generations of   $(X_{r}^{*})_{r\geq0}$    the  spine . We may ≥ assume without loss of generality that the spine of survival individuals corresponds to the ﬁrst individual in each generation, since reordering sibling indices does not change the value of   $\Gamma_{t}$   or  $X_{r}$  . Moreover, the number of children (in   $(X_{r})_{r\geq0}$  ) and the mark of each individual in the spine is jointly distributed as  $\tilde{\eta}=(\tilde{\xi},\tilde{\zeta})$  ).  

For    $\mathbf{x}=(x_{1},\hdots,x_{t})\in\mathcal{M}:=\{1\}\times[M]^{t-1}$  , let    $F(\mathbf{x})$   be the intersection of the event    $E_{1}$   and the event that    $\xi_{1,0}=x_{t},\ldots,\xi_{1,t-1}=x_{1}$  , i.e., the    $r$  -generation of the spine has    $x_{t-r}$   children. (We require    $x_{1}=1$  , which is in the support of    $\xi$  , so that    $F(\mathbf{x})\cap E_{2}$   is not empty.) On the event    $F(\mathbf{x})$  , the ﬁrst    $t$   generations of the Galton-Watson tree   $(X_{r})_{r\geq0}$   can be constructed equivalently as follows — First start with a one-ary tree (a path) of length    $t$   which serves as the spine. Then for the  $r$  -generation individual in the spine with    $r\,\in\,\{0,.\,.\,.\,,t\,-\,1\}$  , attach an independent copy of the branching process   $(\hat{X}_{j})_{j\geq0}$   (deﬁned in  Subsection 2.2.2 ), conditioned on its root having    $x_{t-r}-1$  ≥ children.  

On    $E_{1}$  , let us write  

$$
\Gamma_{t}=\Gamma_{1,t}+\sum_{i=2}^{X_{t}}\Gamma_{i,t}=:\Gamma_{t}^{\ast}+\Gamma_{t}^{0}\;.
$$  

As    $E_{2}$   implies   $[\Gamma_{t}^{0}=0]$  = 0], the desired probability is at least  

$$
{\mathbb{P}}\left\{\left[0<\Gamma_{t}^{*}<e^{-a\hat{H}t}\right]\cap E_{1}\cap E_{2}\cap E_{3}\right\}\geq{\mathbb{P}}\left\{E_{1}\right\}{\mathbb{P}}\left\{\Gamma_{t}^{*}<e^{-a\hat{H}t}\mid E_{1}\right\}\operatorname*{min}_{\mathbf{x}\in\mathcal{M}}{\mathbb{P}}\left\{E_{2}\cap E_{3}\right\}
$$  

where we use that given    $F(\mathbf{x})$  ,    $E_{2}\cap E_{3}$   i ndependent from   $\begin{array}{r}{\left[\Gamma_{t}^{*}<e^{-a\hat{H}t}\right]}\end{array}$  ].  

Let us ﬁrst bound the probability of  $E_{1}$  . By ( 2.11 ), one has  

$$
\mathbb{P}\left\{E_{1}\right\}=\mathbb{P}\left\{X_{0}^{*}=1\right\}\prod_{i=1}^{t}\mathbb{P}\left\{X_{i}^{*}=1\;\middle\vert\;X_{i-1}^{*}=1\right\}=\mathbb{P}\left\{X_{0}^{*}=1\right\}\mathbb{P}\left\{\xi^{*}=1\right\}^{t}=s\dot{\nu}^{t}\;.
$$  

We now bound   $\Gamma_{t}^{*}$    on    $E_{1}$  . Let    $\zeta_{r}$    be the mark of the spine individual in generation    $r$  . Then    $\zeta_{r}$  are iid copies of   $\tilde{\zeta}$  . Letting    $Z_{r}=\log\zeta_{r}$   we have  

$$
\Gamma_{t}^{*}=\prod_{r=1}^{t}(\zeta_{r})^{-1}=e^{-\sum_{r=1}^{t}Z_{r}}~.
$$  

As    $t\to\infty$  , it follows from Cram´ er’s theorem ( Theorem 2.4 ) that  

$$
\mathbb{P}\left\{\Gamma_{t}^{\ast}<e^{-a\hat{H}t}\Bigm|E_{1}\right\}=e^{-(1+o(1))I(a\hat{H})t}\mathrm{~.~}
$$  

ﬁnally obtain a bound on the probability of    $E_{2}\cap E_{3}$   conditional on    $F(\mathbf{x})$  , uniform over  ∈M . By the independence of the trees attached to the spine,  

$$
\left\{E_{2}\mid F({\bf x})\right\}=\prod_{r=0}^{t-1}\mathbb{P}\left\{\hat{X}_{t-r}=0\;\Big\vert\;\hat{X}_{1}=x_{t-r}-1\right\}=\prod_{r=2}^{t}\mathbb{P}\left\{\hat{X}_{r}=0\;\Big\vert\;\hat{X}_{1}=x_{r}-1\right\}\;,
$$  

where the last step uses that    $x_{1}=1$  . For    $r\geq2$  , using that    $x_{r}\leq M$  , we have  

$$
\mathbb{P}\left\{\hat{X}_{r}=0\;\Big|\;\hat{X}_{1}=x_{r}-1\right\}\geq\mathbb{P}\left\{\hat{X}_{2}=0\;\Big|\;\hat{X}_{1}=M\right\}=\mathbb{P}\left\{\hat{\xi}=0\right\}^{M}.
$$  

Also, by Markov inequality, there exists a constant    $r_{0}$   such that for all    $r\geq r_{0}$  

$$
\mathbb{P}\left\{\hat{X}_{r}\ge1\;\Big|\;\hat{X}_{1}=x_{r}\right\}\le\mathbb{E}\left[\hat{X}_{r}\;\Big|\;\hat{X}_{1}=M\right]=M\hat{\nu}^{r-1}\le1/2\;.
$$  

It follows that  

$$
\mathbb{P}\left\{E_{2}\mid F({\bf x})\right\}\ge\mathbb{P}\left\{\hat{\xi}=0\right\}^{r_{0}M}\prod_{r>r_{0}}(1-M\hat{\nu}^{r-1})>c_{0}\;,
$$  

for some constant    $c_{\mathrm{0}}>0$  .  

To bound the probability of    $E_{3}$  , we use the same argument as in [ 9 , Theorem 3.4]. Note that  $X_{r}\;>\;0$   is already implied by    $E_{1}$  , it suﬃces to bound the probability    $X_{r}$   is not too large. By linearity of expectation,  

$$
\mathbb{E}[X_{r}\mid F({\bf x})]=1+\sum_{j=1}^{r}(x_{j-r+t}-1)\hat{\nu}^{j-1}=O(1)\;,
$$  

and by independence of the branching subtrees  

$$
\mathrm{Var}\left(X_{r}\mid F(\mathbf{x})\right)\leq\sum_{j=1}^{r}(x_{j-r+t}-1)\frac{\mathrm{Var}(\hat{\xi})\hat{\nu}^{j-2}\left(\hat{\nu}^{j-1}-1\right)}{\hat{\nu}-1}=O(\mathrm{Var}(\hat{\xi}))=O(1)\ ,
$$  

where we use the moment formula in [ 3 , pp. 4]. Thus, we have    $\mathbb{E}[\hat{X}_{r}^{2}]=O(1)$  (1) and it follows from Chebyshev’s inequality that  

$$
\mathbb{P}\left\{E_{3}^{c}\mid F(\mathbf{x})\right\}\leq\sum_{r=0}^{t}\mathbb{P}\left\{X_{r}\geq\omega\mid F(\mathbf{x})\right\}\leq\sum_{r=1}^{t}\frac{\mathbb{E}\left[X_{r}^{2}\mid F(\mathbf{x})\right]}{\omega^{2}}=O\big(t/\omega^{2}\big)=O(t^{-1})\ .
$$  

From ( 2.26 ) and ( 2.27 ), we obtain  

$$
{\mathbb P}\left\{E_{2}\cap E_{3}\mid F({\bf x})\right\}\geq{\mathbb P}\left\{E_{2}\mid F({\bf x})\right\}-{\mathbb P}\left\{E_{3}^{c}\mid F({\bf x})\right\}\geq c_{0}/2\;.
$$  

The desired bound follows from plugging ( 2.20 ), ( 2.22 ) and ( 2.28 ) into ( 2.19 ).  

If the mini sitive support of    $\xi$   is    $k_{0}\geq2$  , then the only chan d i o let    $E_{2}=\left[X_{t}=\right.$   $k_{0}]$  . The extra  k  $k_{0}-1$   − 1 individuals in generation  t  contribute at most  $k_{0}M\Gamma_{1,t}$   to Γ . Thus the same argument still works.  

# 2.5 Subcritical growth: an upper bound  

Given a ﬁxed    $\gamma>0$  , let    $G_{t}(\gamma)$   be the event    $\cap_{i\in[X_{t}]}[\Gamma_{i,t}\geq\gamma]$  . In the rest of  Subsection 2.5 , we will prove the following theorem:  

Theorem 2.6.  Let    $(X_{r})_{r\geq0}$   be a marked branching process with oﬀspring distribution    $\eta=(\xi,\zeta)$   with  $\mathbb{E}[\xi]\in(1,\infty)$  . Suppose that    $\xi\leq M$   and    $2\leq\zeta\leq M$   almost surely. Let    $\omega\rightarrow\infty$  ,    $t\in(\log^{2}\omega,\omega^{1/2})$  and    $a\geq1$  . Then, we have  

$$
\begin{array}{r}{\mathbb{P}\left\{(G_{t}(e^{-a\hat{H}t}))^{c}\cap[0<X_{t}<\omega]\right\}\leq\exp\left\{-\left(|\log\hat{\nu}|+I(a\hat{H})+o(1)\right)t\right\}\;.}\end{array}
$$  

# 2.5.1 An inhomogeneous branching process  

$(X_{r}^{(t)})_{t\geq r\geq0}\subseteq(X_{r})_{r\geq0}$  Fix    $t$   and let    ⊆   be the ﬁnite subprocess c ining individuals in the ﬁrst    $t$  ≥ ≥ ≥ generations that have progeny in generation    $t$  . Similar to    $X_{r}^{*}$    ,  $X_{r}^{(t)}$  is non-decreasing in    $r$   Condi- tioned on the event   $[X_{t}>0]$  ,   $(X_{r}^{(t)})_{t\geq r\geq0}$    can be seen as an inhomogeneous branching process. The ≥ ≥ oﬀspring distribution of the individuals in generation    $r=t-a$   in this process is    $\xi^{(a)}$  , deﬁned by  

$$
\mathbb{P}\left\{\xi^{(a)}=k\right\}=\frac{1}{s_{a}}\sum_{m\ge k}\mathbb{P}\left\{\xi=m\right\}\binom{m}{k}s_{a-1}^{k}(1-s_{a-1})^{m-k}=\frac{\left(s_{a-1}\right)^{k}}{s_{a}k!}h^{(k)}(1-s_{a-1})\ ,
$$  

where    $s_{a}:=\mathbb{P}\left\{X_{a}>0\right\}$  .  

Note the similarity between    $\xi^{(a)}$    and    $\xi^{*}$  which is deﬁned in ( 2.9 ). We have    $s_{a}\,=\,s+O(\hat{\nu}^{a})$  (see [ 9 , Eq. (3.6)]). Using the Taylor expansion of    $h^{(k)}$    around   $1-s$  , we get  

$$
\mathrm{\Sigma}^{(a)}=k\biggr\}=\frac{s^{k-1}}{k!}h^{(k)}(1-s)+O(\hat{\nu}^{a})=\mathbb{P}\left\{\xi^{*}=k\right\}+O(\hat{\nu}^{a})\ ,\qquad\mathrm{for~}0\leq a\leq t,\ k\geq1
$$  

In particular, by ( 2.11 )  

$$
{\mathbb P}\left\{\xi^{(a)}=1\right\}={\mathbb P}\left\{\xi^{*}=1\right\}+O(\hat{\nu}^{a})=\hat{\nu}+O(\hat{\nu}^{a})\;.
$$  

When   $0\leq\xi\leq M$   almost surely, it follows from ( 2.31 ) that  

$$
\mathbb{E}[\xi^{(a)}]=\mathbb{E}[\xi^{*}]+O(\hat{\nu}^{a})=\nu+O(\hat{\nu}^{a})\;.
$$  

# 2.5.2 Control the surviving process  

Denote by    $\mathbb{P}_{t}\left\{\cdot\right\}:=\mathbb{P}\left\{\cdot\mid X_{t}>0\right\}$   the probability conditioned to survival at time    $t$  .  

The argument for the following lemma is similar that of Theorem 3.4 in our previous work [ 9 ]. We give a proof for completeness.  

Lemma 2.7.  Let    $t$   and    $\omega$   be as in  Theorem 2.6 . Set  $\begin{array}{r}{t_{0}:=\left(\frac{1}{|\log\hat{\nu}|}+\frac{1}{\log\nu}\right)\log\omega=o(t)}\end{array}$    . There exists a constant    $C_{0}\geq1$   such that for any    $h\leq t-t_{0}$  ,    $i\leq h$  ,  

$$
\mathbb{P}_{t}\left\{X_{i}^{(t-h+i)}<\omega\right\}\le C_{0}\hat{\nu}^{i-t_{0}}\;.
$$  

Proof.  Conditioned on survival at time    $t-r$  ,   $(X_{j}^{(t-r)})_{t-r\geq j\geq0}$   is a branching process where the individuals at generation    $j$   have oﬀspring distribution    $\xi^{(t-r-j)}$    deﬁned in ( 2.30 ). Recall that    $\mathbb{E}[\xi^{*}]=$   $\nu>1$  . By ( 2.33 ) and since    $r+j\le h\le t-t_{0}$  , by the choice of    $t_{0}$  , we have  

$$
{\mathbb P}\left\{\xi^{(t-r-j)}=k\right\}={\mathbb P}\left\{\xi^{*}=k\right\}+O(\hat{\nu}^{t_{0}})={\mathbb P}\left\{\xi^{*}=k\right\}+O(\omega^{-1})\;,\qquad\mathrm{for~}k\geq1\;,
$$  

and similarly  

$$
\mathbb{E}\left[\xi^{(t-r-j)}\right]=\nu(1+O(\omega^{-1}))\ .
$$  

Choose    $\varepsilon>0$   so   $((1-\varepsilon)\nu)^{t_{0}}\geq\omega^{-1}$  ; this is possible as   $\hat{\nu}<1$   1. Let    $\xi^{-}$  be a ﬁxed distribution such that each    $\xi^{(t-r-j)}$    stochastically dominates    $\xi^{-}$  and    $\nu^{-}:=\mathbb{E}[\xi^{-}]\geq\nu(1-\varepsilon)$  . Let   $(X_{j}^{-})_{j\geq0}$   be a branching process with oﬀspring distribution    $\xi^{-}$  . The processes   $(X_{j}^{(t-r)})_{h-r\geq j\geq0}$   and   $(X_{j}^{-})_{j\geq0}$   can be coupled so    $X_{j}^{(t-r)}\geq X_{j}^{-}$    almost surely for every    $j\le h-r$  .  

By a theorem due to Kesten and Stigum [ 9 , Theorem 3.1], conditioned on survival,   $(\nu^{-})^{-j}X_{j}^{-}$  converges almost surely to a non-degenerated random variable    $W$   which is absolutely continuous on   $(0,\infty)$  . Writing    $i=h-r$  , let    $a_{i}:=\mathbb{P}_{t-h+i}\{X_{i}^{(t-h+i)}<\omega\}$  } . It follows that  

$$
\begin{array}{r l}&{1-a_{t_{0}}=\mathbb{P}_{t-h+t_{0}}\{X_{t_{0}}^{(t-h+t_{0})}\geq\omega\}}\\ &{\qquad\geq\mathbb{P}\left\{X_{t_{0}}^{-}\geq\omega\right\}\geq\mathbb{P}\left\{X_{t_{0}}^{-}\geq(\nu^{-})^{t_{0}}\right\}\geq\mathbb{P}\left\{1\leq(\nu^{-})^{-t_{0}}X_{t_{0}}^{-}\leq2\right\}\to\mathbb{P}\left\{1\leq W\leq X_{t_{0}}^{-}\right\}.}\end{array}
$$  

As    $t_{0}\rightarrow\infty$  ,   $1-a_{t_{0}}$   is bounded away from 0. So we have    $a_{t0}\leq1-c_{0}$  , for some    $c_{0}>0$  .  

By splitting depending on whether the oﬀspring of the ﬁrst generation is one or larger, we have the simple recursive inequality for    $i>t_{0}$  :  

$$
\begin{array}{r l}&{a_{i}\leq\mathbb{P}\left\{\xi^{(t-h+i)}=1\right\}a_{i-1}+\left(1-\mathbb{P}\left\{\xi^{(t-h+i)}=1\right\}\right)a_{i-1}^{2}}\\ &{\quad=\hat{\nu}a_{i-1}+(1-\hat{\nu})a_{i-1}^{2}+O(\hat{\nu}_{\xi}^{t-h+i}a_{i-1})\;.}\end{array}
$$  

This recursion has exactly the same form of as [ 23 , Eq. (2.4)] and can be solved the same way to show that there exists a constant    $C_{0}$   such that for    $i\leq h$  

$$
a_{i}=\mathbb{P}_{t}\left\{X_{i}^{(t-h+i)}<\omega\right\}\le C_{0}\hat{\nu}^{i-t_{0}}\;.
$$  

# 2.5.3 Ramiﬁcations in the spine  

Conditioned on survival at time    $t$  , let    $x$   be an individual of generation    $h$   of   $(X_{r}^{(t)})_{t\geq r\geq0}$  ≥ ≥ . Let  $y_{0},y_{1},\dotsc,y_{h}=x$   be the path connecting the root    $y_{0}$   to    $x$  , which we refer to as the spine associated to    $x$  . An index    $r\in\{0,\ldots,h-1\}$   is a  ramiﬁcation 1   of the spine, if    $y_{r}$   has oﬀspring at least 2 in  $(X_{r}^{(t)})_{t\geq r\geq0}$  ≥ ≥ . Let    $R(x)$   to be the number ramiﬁcations of the spine associated to    $x$  . The following result reﬁnes  Lemma 2.7  by considering the number of ramiﬁcations.  

Lemma 2.8.  Let    $t_{0}$   be as in  Lemma 2.7 . There exists a constant    $C_{1}>0$   such that, for    $h\leq t-t_{0}$  and every individual    $x$   of generation    $h$   of    $(X_{r}^{(t)})_{t\geq r\geq0}$  , we have  

$$
\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega,R(x)\ge\ell\right\}\le\hat{\nu}^{h+(t_{0}-C_{1})(\ell-2t_{0})}\;,\qquad f o r\;\;2t_{0}<\ell\le h\;.
$$  

Proof.  By permuting the sibling index of individuals, we may assume that    $x$   is the ﬁrst individual of  $X_{h}^{(t)}$  . One can decompose the set of individuals in each generation of   $(X_{r}^{(t)})_{t\geq r\geq0}$    according to their ≥ ≥ the last ancestor with    $x$  , i.e., the last of their ancestors that belongs to the spine    $y_{0},\ldots,y_{h}\,=\,x$  . Conditioning on   $[X_{t}\,>\,0]$  , the number of children of    $y_{0},\ldots,y_{h-1}$   in   $(X_{r}^{(t)})_{t\geq r\geq0}$    is distributed as ≥ ≥ independent random variables    $\xi_{0},\ldots,\xi_{h-1}$   where    $\xi_{r}\,{\overset{\underset{\mathrm{~\tiny~cal~L~}}{}}{=}}\,\xi^{(t-r)}$  .  

Therefore, we can generate   $(X_{r}^{(t)})_{h\geq r\geq0}$    equivalently as follows: (i) construct the spine; (ii) for ≥ ≥ every    $r\in\left[h-1\right]$   attach    $\xi_{r}-1$   independent copies of   $(X_{j}^{(t-(r+1))})_{h-r-1\geq j\geq0}$   conditioned on   $[X_{t}>0]$  , which we denote by   $(W_{j}^{r,2})_{h-(r+1)\geq j\geq0},.\,.\,.\,,(W_{j}^{r,\xi_{r}})_{h-(r+1)\geq j\geq0}$  , to    $y_{r}$  .  

Looking at the generation    $h$  , this decomposition gives the following recursive inequality:  

$$
X_{h}^{(t)}=1+\sum_{r=0}^{h-1}\sum_{k\geq2}^{\xi_{r}}W_{h-(r+1)}^{r,k}\succeq\sum_{i=1}^{h}Z_{i}\succeq\sum_{i=2t_{0}+1}^{h}Z_{i}\;,
$$  

where    $\succeq$  denotes  stochastically domination  and    $Z_{1},\ldots,Z_{h}$   are independent random variables with distribution  

$$
Z_{i}\stackrel{\mathcal{L}}{=}\left\{\!\!\begin{array}{l l}{0}&{\mathrm{with~probability~}b_{i}\;,}\\ {\left(X_{i-1}^{(t-h+i-1)}\:\left|\:X_{t-h+i-1}>0\right.\right)}&{\mathrm{with~probability~}1-b_{i}\;,}\end{array}\right.
$$  

where    $b_{i}:=\mathbb{P}\left\{\xi^{(t-h+i)}=1\right\}$  .  

Let    $R_{0}(x)$   be the number of ramiﬁcations of the spine associated to    $x$   with index at most  $h-2t_{0}-1$  . Let    $p_{\ell}:=\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega,R_{0}(x)=\ell\right\}$  n o . Recalling that    $a_{i}:=\mathbb{P}_{t-h+i}\{X_{i}^{(t-h+i)}\,<\,\omega\}$  } , we have  

$$
p_{\ell}\leq\sum_{i_{1}<i_{2}\cdots<i_{\ell}}\prod_{j=1}^{\ell}a_{i_{j}-1}(1-b_{i_{j}})\prod_{\stackrel{2t_{0}<j\leq T}{j\not\in\{i_{1},\ldots,i_{\ell}\}}}b_{j}\ ,
$$  

where the sum is over all choices of    $\ell$  ordered and distinct indices from    $\{2t_{0}+1,T\}$  , which indicate where the ramiﬁcations occur. Since    $t-h\geq t_{0}$   and by ( 2.32 ), we have that    $b_{i}=\hat{\nu}+O(\hat{\nu}^{t-h+i})=$   $\hat{\nu}+O(\hat{\nu}^{3t_{0}})$  ) for all   $2t_{0}\le i\le h$  . Thus, ( 2.42 ) implies that  

$$
\begin{array}{r l}&{p_{\ell}\leq\big(1-\hat{\nu}+O\big(\hat{\nu}^{3t_{0}}\big)\big)^{\ell}\big(\hat{\nu}+O\big(\hat{\nu}^{3t_{0}}\big)\big)^{h-2t_{0}-\ell}\displaystyle\sum_{i_{1}<i_{2}\cdots<i_{\ell}}\prod_{j=1}^{\ell}a_{i_{j}-1}}\\ &{\quad=\big(1+o(1)\big)\big(1-\hat{\nu}\big)^{\ell}\hat{\nu}^{h-2t_{0}-\ell}\displaystyle\sum_{i_{1}<i_{2}\cdots<i_{\ell}}\prod_{j=1}^{\ell}a_{i_{j}-1}\;,}\end{array}
$$  

where the last step uses that   $\hat{\nu}^{t_{0}}\leq\omega^{-1}$    ≤ , that    $h\leq t\leq\sqrt{\omega}$   and    $\omega\to\infty$  . Moreover,  

$$
\sum_{i_{1}<\cdots<i_{\ell}}\prod_{j=1}^{\ell}a_{i_{j}-1}\le\left(\sum_{i=2t_{0}+1}^{h}a_{i-1}\right)^{\ell}\le\left(\sum_{i=2t_{0}+1}^{\infty}C_{0}\hat{\nu}^{i-t_{0}-1}\right)^{\ell}\le\left(C_{0}\frac{\hat{\nu}^{t_{0}}}{1-\hat{\nu}}\right)^{\ell}\;.
$$  

Putting this back to ( 2.43 ) we have  

$$
p_{\ell}=(1+o(1))\hat{\nu}^{h-2t_{0}}(C_{0}\hat{\nu}^{t_{0}-1})^{\ell}\;.
$$  

Since there are at  $2t_{0}-1$   ramiﬁcations in the last   $2t_{0}-1$   indices of the spine (excluding    $x$  ), it follows that for  $\ell>2t_{0}$  ,  

$$
\begin{array}{r l}&{\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega,R(x)\ge\ell\right\}\le\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega,R_{0}(x)\ge\ell-2t_{0}\right\}}\\ &{\qquad\qquad\qquad\qquad=\displaystyle\sum_{j\ge\ell-2t_{0}}p_{j}}\\ &{\qquad\qquad\qquad\qquad=O(1)\hat{\nu}^{h-2t_{0}}\left(C_{0}\hat{\nu}^{t_{0}-1}\right)^{\ell-2t_{0}}}\\ &{\qquad\qquad\qquad\le\hat{\nu}^{h+(t_{0}-C_{1})(\ell-2t_{0})}\;,}\end{array}
$$  

for some constant    $C_{1}>0$  .  

# 2.5.4 Finishing the proof of  Theorem 2.6  

Let    $h=t-t_{0}$   and deﬁne  

$$
\ell(t):=2t_{0}+\frac{t+t_{0}}{t_{0}-C_{1}}>2t_{0}\;.
$$  

Then, by  Lemma 2.8  

$$
\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega,R(x)\ge\ell(t)\right\}\le\hat{\nu}^{2t}\;.
$$  

Clearly, the event   $[X_{t}<\omega]$   implies   $[X_{h}^{(t)}\,<\,\omega]$  ], for every    $h\leq t$  . Let    $x_{1},\allowbreak\cdot\cdot\cdot,x_{X_{h}^{(t)}}$  denote the individuals in generation    $h$   of   $(X_{r}^{(t)})_{t\geq r\geq0}$  . Let    $E_{1}$    be the event  ∩  $\cap_{i=1}^{X_{h}^{(t)}}[R(x_{i})<\ell(t)]$  )]. By a union ≥ ≥ bound over the choice of    $i$   and using ( 2.48 )  

$$
\mathbb{P}_{t}\left\{X_{t}<\omega,E_{1}^{c}\right\}\leq\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega,E_{1}^{c}\right\}\leq\sum_{j=1}^{\omega-1}\sum_{i=1}^{j}\mathbb{P}_{t}\left\{X_{h}^{(t)}=j,R(x_{i})\geq\ell(t)\right\}\leq\omega\dot{\nu}^{2t}\;.
$$  

Let    $E_{2}:=[0<X_{t}<\omega]\cap E_{1}$  . Since    $X_{h}^{(t)}$  is non-decreasing, by  Lemma 2.7  with    $i=h=t-t_{0}$  , we have  

$$
\begin{array}{r}{\mathbb{P}_{t}\left\{E_{2}\right\}\leq\mathbb{P}_{t}\left\{X_{t}<\omega\right\}\leq\mathbb{P}_{t}\left\{X_{t-t_{0}}^{(t)}<\omega\right\}\leq C_{0}\hat{\nu}^{t-2t_{0}}=O(\omega^{C_{2}})\hat{\nu}^{t}\;,}\end{array}
$$  

where    $\begin{array}{r}{C_{2}:=2+\frac{2|\log\hat{\nu}|}{\log\nu}}\end{array}$  . Let    $\gamma:=e^{-a\hat{H}t}$  . It follows from ( 2.49 ) and ( 2.50 ) that  

$$
\begin{array}{r l}&{\mathbb{P}_{t}\left\{G_{t}^{c}(\gamma),X_{t}<\omega\right\}\leq\mathbb{P}_{t}\left\{G_{t}^{c}(\gamma),X_{t}<\omega,E_{1}\right\}+\mathbb{P}_{t}\left\{X_{t}<\omega,E_{1}^{c}\right\}}\\ &{\qquad\qquad\leq\mathbb{P}\left\{G_{t}^{c}(\gamma)\mid E_{2}\right\}\mathbb{P}_{t}\left\{E_{2}\right\}+\omega\hat{\nu}^{2t}}\\ &{\qquad\qquad\leq O(\omega^{C_{2}})\hat{\nu}^{t}\displaystyle\sum_{j=1}^{\omega-1}\sum_{i=1}^{j}\mathbb{P}\left\{X_{t}=j,\Gamma_{i,t}\leq\gamma\mid E_{2}\right\}+\omega\hat{\nu}^{2t}}\\ &{\qquad\qquad\leq O(\omega^{C_{2}+1})\hat{\nu}^{t}\mathbb{P}\left\{\Gamma_{1,t}<\gamma\mid E_{2}\right\}+\omega\hat{\nu}^{2t}\;,}\end{array}
$$  

where the two last lines use a union bound and the symmetry of all individuals in generation    $t$  .  

Thus, it suﬃce to upper bound the probability of   $\left[\Gamma_{1,t}<\gamma\right]$   conditioned on    $E_{2}$  . Let    $\mathcal{A}_{t,\omega}$   be the set of rooted trees  T  with width less than    $\omega$  , height exactly    $t$   and such that the spine associated to each leaf has at most    $\ell(t)$   ramiﬁcations. The trees in    $\mathcal{A}_{t,\omega}$   are the candidates for   $\mathrm{GW}_{t}$   — the tree induced by the ﬁrst    $t$   generations of the process   $(X_{r}^{(t)})_{t\geq r\geq0}$  , conditioned on    $E_{2}$  . We will obtain ≥ ≥ an upper bound for the probability of   $\left[\Gamma_{1,t}<\gamma\right]$   conditioned on    $E_{2}\cap|\mathrm{GW}_{t}\cong T|$  ], uniform for all  $T\in\mathcal{A}_{t,\omega}$  , which will also be an upper bound of    $\mathbb{P}\left\{\Gamma_{1,t}<\gamma\mid E_{2}\right\}$  .  

Let    $y_{0},\ldots,y_{t}$   be the individuals of the spine associated to    $x_{1}$  . If    $r\,\in\,\{0,\cdot\,\cdot\,,h-1\}$   is not a ramiﬁcation of  T , we ﬁrst sample   $\tilde{\xi}^{(t-r)}$  , the number of children of    $y_{r}$   in   $(X_{r})_{r\geq0}$  . Then conditioned on   $\tilde{\xi}^{(t-r)}$  , we sample the ma of   . If    $r\in\{0,.\,.\,.\,,h-1\}$   is a ramiﬁcation of    $T$  , or if    $r\in\{h,.\,.\,.\,,t\}$  ,  $y_{r}$  we simply give    $y_{r}$   the mark  M . This procedure gives a stochastic lower bound of   $\Gamma_{1,t}$  .  

Similar to   $\tilde{\xi}$   deﬁned in ( 2.12 ), the distribution of   $\tilde{\xi}^{(a)}$    is given by  

$$
\tilde{\xi}^{(a)}=k\bigg\{P\left\{X_{1}=k\;\middle\vert\;X_{1}^{(a)}=1\right\}=\frac{k s_{a-1}(1-s_{a-1})^{k-1}\mathbb{P}\left\{\xi=k\right\}}{s_{a}\mathbb{P}\left\{\xi^{(a)}=1\right\}}=\mathbb{P}\left\{\tilde{\xi}=k\right\}+O(\xi^{a}).
$$  

for    $k\geq1$  . Therefore, we can couple   $\tilde{\xi}^{(t)},\dots,\tilde{\xi}^{(t-(h+1))}$    with   $\tilde{\xi}_{1},\dots,\tilde{\xi}_{h}$  , which are iid copies of   $\tilde{\xi}$  , such that    $\mathbb{P}\left\{\tilde{\xi}^{(t-r)}\neq\tilde{\xi}_{r+1}\right\}<\omega^{-1}$  n o . Thus, the number of positions where the two sequences diﬀer is stochastically bounded from above by a binomial random variable with parameters   $(h,\omega^{-1})$  . Let    $E_{3}$   be the event that   $(\tilde{\xi}^{(t)},.\,.\,.\,,\tilde{\xi}^{(t-(h+1))})$  ) and   $(\tilde{\xi}_{1},\allowbreak\cdot\,\allowbreak\cdot\,\tilde{\xi}_{h})$  ) diﬀer at at least    $m(t):=t(\log t)^{-1/2}$  positions. It follows that  

$$
\begin{array}{r}{\mathbb{P}\left\{E_{3}^{c}\right\}\leq(h/\omega)^{m(t)}\leq t^{-m(t)}=e^{-t\sqrt{\log t}}\;,}\end{array}
$$  

where we used that    $h<t\leq\sqrt{\omega}$  . Thus, we obtain  

$$
{\mathbb P}\left\{\Gamma_{1,t}<\gamma\ |\ E_{2}\cap[G W_{t}\cong T]\right\}\leq{\mathbb P}\left\{\Gamma_{1,t}<\gamma\ |\ E_{2}\cap E_{3}\cap[G W_{t}\cong T]\right\}+e^{-t\sqrt{\log t}}\;.
$$  

Let   $(\widetilde{\zeta}_{r})_{r\geq0}$   be iid copies of   $\tilde{\zeta}$   as deﬁned in  Subsection 2.2.2 . Conditioning on  $E_{2}\cap E_{3}\cap[G W_{h}\cong T]$  ], ≥ we have  

$$
\Gamma_{1,t}\succeq M^{-(\ell(t)+t_{0}+m(t))}\prod_{r=0}^{t-1}(\tilde{\zeta}_{r})^{-1}\;.
$$  

where  $\succeq$  denotes stochastical domination. It follows from Cram´ er’s theorem ( Theorem 2.4 ) that  

$$
\begin{array}{r l}&{\mathbb{P}\left\{\Gamma_{1,t}<e^{-a\hat{H}t}\;\Big|\;E_{2}\cap E_{3}\cap[G W_{t}\cong T]\right\}}\\ &{\leq\mathbb{P}\left\{\displaystyle\frac{\sum_{r=0}^{t-1}\log\tilde{\zeta}_{r}}{t}>a\hat{H}-\displaystyle\frac{(\ell(t)+t_{0}+m(t))\log M}{t}\right\}}\\ &{\leq\exp\left\{-\left(I\left(a\hat{H}-\displaystyle\frac{(\ell(t)+t_{0}+m(t))\log M}{t}\right)+o(1)\right)t\right\}}\\ &{=e^{-(I(a\hat{H})+o(1))t}\;,}\end{array}
$$  

where in the last step we use that    $\ell(t),m(t),t_{0}=o(t)$  ,   $\hat{H}\leq a\hat{H}<\log M$   ≤ ,    $I(z)$   is continuous on  $[\hat{H},\infty)$  ), and    $I^{\prime}(z)<\infty$  for all    $z>{\hat{H}}$  . Putting this into ( 2.54 ), we have  

$$
{\mathbb P}\left\{\Gamma_{1,t}<e^{-a\hat{H}t}\;\Big|\;E_{2}\cap[G W_{t}\cong T]\right\}\leq e^{-\left(I(a\hat{H})+o(1)\right)t}\;.
$$  

Theorem 2.6  follows by putting the above into ( 2.51 ).  

# 2.5.5 A corollary  

The following corollary is convenient for our later use.  

Corollary 2.9.  Similarly as in  ( 1.10 ) , consider  

$$
\phi(a):=\frac{1}{a}\left(|\log\hat{\nu}|+I(a\hat{H})\right)\;.
$$  

and    $a_{0}$   its minimum in    $[0,\infty)$   which satisﬁes    $a_{0}\geq1$  . For any    $p\rightarrow0$  , let    $\omega\,\in\,(|\log p|^{3},e^{|\log p|^{1/3}})$  and    $\gamma_{p}=e^{-\hat{H}|\log p|/\phi(a_{0})}$  . We have  

$$
\begin{array}{r}{\mathbb{P}\left\{G_{t_{\omega}}^{c}\left(\gamma_{p}\right)\cap\left[t_{\omega}<\infty\right]\right\}\leq p^{1+o\left(1\right)}\;.}\end{array}
$$  

In particular, if    $p=n^{-\beta}$    for    $\beta>0$  , we have  $\gamma_{p}=n^{-\beta\hat{H}/\phi\left(a_{0}\right)}$  .  

Proof.  For    $t\leq\log^{2}\omega$  , we deterministic ally have  

$$
M^{-t}>M^{-\log^{2}\omega}>\gamma_{p}\;,
$$  

which implies  

$$
{\mathbb P}\left\{{\cal G}_{t}^{c}(\gamma_{p})\right\}={\mathbb P}\left\{\cup_{i=1}^{X_{t}}\left[\Gamma_{i,t}\leq\gamma_{p}\right]\right\}\leq{\mathbb P}\left\{\cup_{i=1}^{X_{t}}\left[M^{-t}\leq\gamma_{p}\right]\right\}=0\;.
$$  

Let    $t_{*}=|\log p|/|\!\log\hat{\nu}|<\sqrt{\omega}$  |   √ . Choose    $t\in\left(\log^{2}\omega,t_{*}\right]$   and set    $\begin{array}{r}{a:=\frac{|\log p|}{t\phi(a_{0})}}\end{array}$  . As    $\phi(a_{0})\leq\phi(1)=|\log\hat{\nu}|$  | , we have    $a\geq1$  . It follows from  Theorem 2.6  that  

$$
\begin{array}{l}{\mathbb{P}\left\{G_{t}(\gamma_{p})^{c}\cap[0<X_{t}<\omega]\right\}=\mathbb{P}\left\{G_{t}\left(e^{-a\hat{H}t}\right)^{c}\cap[0<X_{t}<\omega]\right\}}\\ {\qquad\qquad\qquad\leq\exp\Biggl\{-(1+o(1))\frac{\left\vert\log p\right\vert\phi\left(a\right)}{\phi\left(a_{0}\right)}\Biggr\}\leq p^{1+o(1)}\;,}\end{array}
$$  

where the last step uses the fact that    $a_{0}$   minimises    $\phi(a)$   for all    $a\geq0$  . Thus,  

$$
\begin{array}{l}{\displaystyle\mathbb{P}\left\{G_{t_{\omega}}^{c}(\gamma_{p})\cap[t_{\omega}<\infty]\right\}=\displaystyle\sum_{t\ge0}\mathbb{P}\left\{G_{t_{\omega}}(\gamma_{p})^{c},t_{\omega}=t+1\right\}}\\ {\displaystyle\qquad\le\displaystyle\sum_{t=\log^{2}\omega}^{t_{*}}\mathbb{P}\left\{G_{t}(\gamma_{p})^{c},0<X_{t}<\omega\right\}+\mathbb{P}\left\{t_{\omega}\ge t_{*}\right\}}\\ {\displaystyle\qquad\le t_{*}p^{1+o(1)}+C_{0}\hat{\nu}^{t_{*}-2t_{0}}}\\ {\displaystyle\qquad\le p^{1+o(1)}\;,}\end{array}
$$  

where we used  Lemma 2.7  to bound    $\mathbb{P}\left\{t_{\omega}>t_{*}\right\}$  , as in ( 2.50 ).  

# 2.6 A truncated Martingale  

der a truncated version of   $\Gamma_{t}$  ned in ( 2.4 ).  x    $t_{0}\in\ensuremath{\mathbb{N}}$   and  $\gamma\,>\,0$  . Re t  $(f^{t-t_{0}}(i,t),t_{0})$  ) is the ancestor of the node (  $(i,t)$  ) in generation  t  $t_{0}$  . For every  t  $t\,\geq\,t_{0}$   ≥  and  i  $i\in[X_{t}]$   ∈ ], deﬁne  

$$
\hat{\Gamma}_{i,t}:=\gamma\left(\prod_{r=1}^{t-t_{0}}\zeta_{f^{t-r}(i,t),r}\right)^{-1}\;,\qquad\hat{\Gamma}_{t}:=\sum_{i\in[X_{t}]}\hat{\Gamma}_{i,t}\;.
$$  

Given that   $\Gamma_{i,t_{0}}\,\geq\,\gamma$   for   $i\in[X_{t_{0}}]$  ve   $\hat{\Gamma}_{i,t}\leq\Gamma_{i,t}$   ≤  for    $i\in[X_{t}]$   and   $\hat{\Gamma}_{t}\leq\Gamma_{t}$   ≤ he same argument of  Lemma 2.1 , (  $(\hat{\Gamma}_{t}\mathbb{E}[\xi/\zeta]^{-(t-t_{0})})_{t\geq t_{0}}$   is also a martingale with respect to (  $(\mathcal{F}_{r})_{r\geq t_{0}}$  F . ≥ ≥  

Proposition 2.10.  Let    $(X_{r})_{r\geq0}$   be a marked branching process with oﬀspring distribution    $\eta=(\xi,\zeta)$  with    $\mathbb{E}[\xi]\,\in\,(1,\infty)$  . Let    $M\,\in\,\mathbb{N}$   and    $\omega\rightarrow\infty$  . Suppose that    $|\mathbb{E}[\xi/\zeta]-1|\,\le\,M\omega^{-1/3}$  ,    $\xi\,\leq\,M$   and  $\zeta\geq1$  . There exists a constant    $c_{0}>0$   such that for any    $t_{0}\in\mathbb{N}$   and    $t\in[t_{0},\omega^{1/4}]$   we have  

$$
\mathbb{P}\left\{\hat{\Gamma}_{t}\geq\omega\gamma/2~\Big|~[X_{t_{0}}\geq\omega]\cap G_{t_{0}}(\gamma)\right\}\geq1-e^{-c_{0}\omega^{1/3}}\;.
$$  

Proof.  Consider the collection of events  

$$
E_{r}=[\hat{\Gamma}_{r}/\hat{\Gamma}_{t_{0}}\in(1/2,3/2)]\qquad r\in(t_{0},t]\;.
$$  

We will lower bound the probability of    $E_{r}$   using Azuma’s inequality, see, e.g., [ 21 , Chapter 11].  

Note that, given    $\mathcal{F}_{r}$  ,    $m:=\,X_{r}$   and   $\hat{\Gamma}_{r}$   are measurable. Let    $T_{1},T_{1}^{\prime},\cdot\,\cdot\,\,,T_{m},T_{m}^{\prime}$    be iid marked Galton-Watson trees with oﬀspring   , where    $T_{i}$   is rooted at node    $i$   of generation   . Given    $\mathcal{F}_{r}$  ,  $\eta$   $r$   $\hat{\Gamma}_{r+1}\;=\;g(T_{1},.\,.\,.\,,T_{m})$  ), where    $g$   is a function depending on    $\mathcal{F}_{r}$  . Note that for every choice of  $T_{1},\dots,T_{m}$   and    $T_{i}^{\prime}$  ,  

$$
|g(T_{1},\ldots,T_{i},\ldots T_{m})-g(T_{1},\ldots,T_{i}^{\prime},\ldots,T_{m})|\leq M\hat{\Gamma}_{i,r}\leq M\gamma\;,
$$  

since the tree    $T_{i}$   contributes to   $\hat{\Gamma}_{r+1}$   with at most  

$$
\sum_{j\geq0}\mathbf{1}_{f(j,r+1)=i}\hat{\Gamma}_{j,r+1}=\hat{\Gamma}_{i,r}\sum_{j\geq0}\frac{\mathbf{1}_{f(j,r+1)=i}}{\zeta_{j,r+1}}\leq\xi_{i,r}\hat{\Gamma}_{i,r}\leq M\hat{\Gamma}_{i,r}\ .
$$  

Since   $(\hat{\Gamma}_{t}\mathbb{E}[\xi/\zeta]^{-(t-t_{0})})_{t\geq t_{0}}$   is a martingale, we have ≥  

$$
\begin{array}{r}{\left|\hat{\Gamma}_{r}-\mathbb{E}\left[\hat{\Gamma}_{r+1}\left|\mathcal{F}_{r}\right]\right|=\left|\hat{\Gamma}_{r}-\mathbb{E}\left[\xi/\zeta\right]\hat{\Gamma}_{r}\right|\leq M\omega^{-1/3}\hat{\Gamma}_{r}=:s\;.}\end{array}
$$  

Thus, it follows from ( 2.68 ) and Azuma’s inequality [ 21 , pp. 92] that  

$$
\begin{array}{r l}&{\mathbb{P}\left\{\left|\hat{\Gamma}_{r+1}-\hat{\Gamma_{r}}\right|>2s\;\middle|\;\mathcal{F}_{r}\right\}}\\ &{\leq\mathbb{P}\left\{\left|\hat{\Gamma}_{r+1}-\mathbb{E}\left[\hat{\Gamma}_{r+1}\;\middle|\;\mathcal{F}_{r}\right]\right|>s\;\middle|\;\mathcal{F}_{r}\right\}+\mathbb{P}\left\{\left|\hat{\Gamma}_{r}-\mathbb{E}\left[\hat{\Gamma}_{r+1}\;\middle|\;\mathcal{F}_{r}\right]\right|>s\;\middle|\;\mathcal{F}_{r}\right\}}\\ &{\leq2\exp\!\left\{-\frac{2s^{2}}{\sum_{i=1}^{m}(M\hat{\Gamma}_{i,r})^{2}}\right\}.}\end{array}
$$  

Note that  $\begin{array}{r}{\sum_{i=1}^{m}(M\hat{\Gamma}_{i,r})^{2}\leq\gamma M^{2}\sum_{i=1}^{m}\hat{\Gamma}_{i,r}=\gamma M^{2}\hat{\Gamma}_{r}}\end{array}$  . Thus on the event    $E_{r}$  , we have  

$$
\mathbb{P}\left\{\left|\hat{\Gamma}_{r+1}-\hat{\Gamma}_{r}\right|>2s\;\right\vert\mathcal{F}_{r}\right\}\leq2\exp\left\{-\frac{2\omega^{-2/3}\hat{\Gamma}_{r}^{2}}{\gamma\hat{\Gamma}_{r}}\right\}\leq2e^{-\omega^{1/3}}\;,
$$  

where we used   $\hat{\Gamma}_{r}\geq\omega\gamma/2$  2 on    $E_{r}$   in the las  

Let    $F_{t_{0}}\,=\,[X_{t_{0}}\,\geq\,\omega]\,\cap\,G_{t_{0}}(\gamma)$   and let  $F_{r}=[|\hat{\Gamma}_{r+1}/\hat{\Gamma}_{r}-1|\,\leq\,M\omega^{-1/3}]$  |  − | ≤ ] for    $r>t_{0}$  . By ( 2.70 ), there exists  c  $c_{0}>0$   0 such that  

$$
\mathbb{P}\left\{\cup_{r=t_{0}+1}^{t}F_{r}^{c}\;\middle\vert\;F_{t_{0}}\right\}\leq\sum_{r=t_{0}}^{t-1}\mathbb{P}\left\{F_{r+1}^{c}\;|\;\cap_{t_{0}\leq s\leq r}F_{s}\right\}\leq2t e^{-\omega^{1/3}}\leq e^{-c_{0}\omega^{1/3}},
$$  

where we used    $\cap_{s\leq r}F_{s}\subseteq E_{r}$  . So, with the desired probability  

$$
\hat{\Gamma}_{t}\geq(1-2M\omega^{-1/3})^{t}\omega\gamma\geq\frac{\omega\gamma}{2}\;,
$$  

as    $t\leq\omega^{1/4}$  .  

# 3 Exploring the graph  

# 3.1 The exploration process  

In this section, we introduce a marked version of the Breadth First (BFS) Search graph exploration process deﬁned in [ 9 ].  

For a set of vertices    ${\mathcal{T}}\subseteq[n]$  , let    $\mathcal{E}^{\pm}(\mathcal{T})$   be the tails/heads incident to    $\mathcal{T}$  . Let    $\mathcal{E}^{\pm}$  be the set of all tails/heads. For a set of half-edges  X , l  $\mathcal{V}(\mathcal{X})$   be the vertices incident to    $\mathcal{X}$  . For    $e^{\pm}\in\mathcal{E}^{\pm}$  , we use    $v(e^{\pm})$   to denote the vertex incident to  e  $e^{\pm}$  .  

We start from an arbitrary head    $e_{0}^{-}\in\mathcal{E}^{-}$    ∈E . In this process, we create random pairings of half- edges one by one and keep each half-edge in exactly one of the three states —  active, paired , or undiscovered . Let    $\mathcal{A}_{i}^{\pm}$  ,    $\mathcal{P}_{i}^{\pm}$    and    $\mathcal{U}_{i}^{\pm}$    denote the set of tails/heads in the four states respectively after the    $i$  -th pairing of half-edges. Initially, let  

$$
\mathcal{A}_{0}^{-}=\{e_{0}^{-}\},\;\mathcal{A}_{0}^{+}=\mathcal{E}^{-}(v(e_{0}^{-})),\;\mathcal{P}_{0}^{\pm}=\emptyset,\;\mathcal{U}_{0}^{\pm}=\mathcal{E}^{\pm}\setminus(\mathcal{A}_{0}^{\pm}\cup\mathcal{P}_{0}^{\pm})\;.
$$  

Then set    $i=1$   and proceed as follows:  

(i) Let    $e_{i}^{-}$  be one of the heads which became active earliest in  $\mathcal{A}_{i-1}^{-}$  

−

 (ii) Pair    $e_{i}^{-}$  with a tail    $e_{i}^{+}$  chosen uniformly at random from    $\mathcal{E}^{+}\setminus\mathcal{P}_{i-1}^{+}$  . Let    $v_{i}\,=\,v(e_{i}^{-})$  ) and −  $\mathcal{P}_{i}^{\pm}=\mathcal{P}_{i-1}^{\pm}\cup\{e_{i}^{\pm}\}$   P   ∪{   } . −

 (iii) If    $e_{i}^{+}\in\mathcal{A}_{i-1}^{+}$    , then    $\mathcal{A}_{i}^{\pm}=\mathcal{A}_{i-1}^{\pm}\setminus\{e_{i}^{\pm}\}$      } ; and if    $e_{i}^{+}\in\mathcal{U}_{i-1}^{+}$    , then    $\mathcal{A}_{i}^{\pm}=(\mathcal{A}_{i-1}^{\pm}\cup\mathcal{E}^{\pm}(v_{i}))\setminus\{e_{i}^{\pm}\}$      } . − − − −

 (iv) If    $\mathcal{A}_{i}^{-}{=}\,\emptyset$   ∅ , terminate; otherwise, let    $\mathcal{U}_{i}^{\pm}\!=\!\mathcal{E}^{\pm}\setminus(\mathcal{A}_{i}^{\pm}\cup\mathcal{P}_{i}^{\pm})$  E  \ A ∪P ),    $i=i+1$   and go to (i).  

If we are in the ﬁrst case of step (iii), we say that a  collision  has happened. If there is no collision in the process up to certain time, the exposed in-neighbourhood of    $e_{0}^{-}$    is a tree.  

In parallel to the BFS process, we construct a tree with nodes corresponding to heads in the in-neighbourhood of    $e_{0}^{-}$  . Write    $f=e_{0}^{-}$    and let    $T_{f}^{-}(0)$  (0) be a tree with one root node corresponding to    $f$  . Given    $T_{f}^{-}(i-1)$     $T_{f}^{-}(i)$  ) is constructed as follows: if    $e_{i}^{-}\in\mathcal{U}_{i-1}^{-}$    , then construct    $T_{f}^{-}(i)$  ) from −  $T_{f}^{-}(i-1)$  1) by adding    $\left|\mathcal{E}^{-}(v_{i})\right|$   child nodes to the node representing    $e_{i}^{-}$  , each of which representing a head in    $\mathcal{E}^{-}(v_{i})$  ; otherwise, let    $T_{f}^{-}(i)=T_{f}^{-}(i-1)$  1). See  Figure 1  for an example the exploration process and its associated tree.  

The nodes in    $T_{f}^{-}(i)$  ) correspond to the heads in    $\mathcal{P}_{i}^{-}\cup\mathcal{A}_{i}^{-}$    ∪A . So we can assign a label  paired  or active  to each node of    $T_{f}^{-}(i)$  ). We also give the node corresponding to    $e_{i}^{-}$    a  mark  which equals the out-degree of    $v_{i}$  .  

For half-edges    $e_{1}$   and    $e_{2}$  , we deﬁne the  distance  from    $e_{1}$   to    $e_{2}$  , denoted by   $\mathrm{dist}(e_{1},e_{2})$  , to be the length of the shortest path from    $v(e_{1})$   to    $v(e_{2})$   which starts with the edge containing    $e_{1}$   if  $e_{1}$   is a tail, and which ends with the edge containing    $e_{2}$   if    $e_{2}$   is head. For example, in  Figure 1 ,  $\mathrm{dist}(e_{2}^{-},e_{1}^{-})=\mathrm{dist}(e_{2}^{+},e_{1}^{+})=1$   $\mathrm{dist}(e_{3}^{+},e_{1}^{-})=2$   $\mathrm{dist}(e_{4}^{-},e_{1}^{+})=0$  

If    $i_{t}$   is the last step where a head at distance    $t$   from    $f$   is paired, then    $T_{f}^{-}(i_{t})$  ) satisﬁes: (i) the height is    $t$  ; and (ii) the set of actives nodes is the    $t$  -th level. We call a rooted tree    $T$   incomplete  if it satisﬁes (i)-(ii). We let    $p(T)$   be the number of  paired  nodes in    $T$  .  

  
Figure 1: An ongoing exploration process and its incomplete tree  

# 3.2 Coupling the exploration and branching processes  

We will couple a marked incomplete tree constructed from the exploration process with a marked branching process with oﬀspring distribution close to    $\eta\,=\,(\xi,\zeta)\,{\stackrel{\mathcal{L}}{=}}\,D_{\mathrm{out}}$  . Thus we assume that  $\xi\le M$  ,   $2\leq\zeta\leq M$   and    $\mathbb{E}\left[\xi/\zeta\right]=1$   (by  Remark 2.2 ). We will need two slightly perturbed versions of    $\eta$  . Let    $\beta\in\left(0,1/4\right)$  . Consider the probability distribution    $\eta^{\downarrow}=\eta^{\downarrow}(\beta)=(\xi^{\downarrow}(\beta),\zeta^{\downarrow}(\beta))$   deﬁned by  

$$
\mathbb{P}\left\{\eta^{\downarrow}=(k,\ell)\right\}:=\left\{{c^{\downarrow}\mathbb{P}\left\{\eta=(k,\ell)\right\}}\atop{0}\right.\ {\mathrm{~otherwise~}}
$$  

where    $c^{\downarrow}$  is a normalising constant. It is easy to check that our assumptions on    $\eta$   implies that  $c^{\downarrow}=1+O(n^{-1+\beta})$  .  

Similarly, the probability distribution    $\eta^{\uparrow}=\eta^{\uparrow}(\beta)$   is deﬁned by  

$$
\mathbb{P}\left\{\eta^{\uparrow}=(k,\ell)\right\}:=\left\{c^{\uparrow}\mathbb{P}\left\{\eta=(k,\ell)\right\}\quad\qquad k\ge1\right.
$$  

where    $c^{\uparrow}=1-O(n^{-1+\beta})$   is a normalising constant.  

One can show that    $\eta^{\downarrow}$  satisﬁes    $|\mathbb{E}[\xi^{\downarrow}/\zeta^{\downarrow}]-1|=O(n^{-1+\beta})$   and similarly for    $\eta^{\uparrow}$  . If    $\omega$   is poly- logarithmic, it will be possible to apply  Proposition 2.10  to them.  

Let   $\mathrm{GW}_{\eta}$   be a marked Galton-Watson tree with oﬀspring distribution    $\eta$  . For a marked in- complete rooted tree    $T$  , we use the notation   $\mathrm{GW}_{\eta}\,\cong\,T$   to denote that    $T$   is isomorphic to a root subtree of   $\mathrm{GW}_{\eta}$  , i.e., the degree of paired nodes of    $T$   agrees with   $\mathrm{GW}_{\eta}$  , and all marks in

  $\mathit{(I^{\prime}}$   and   $\mathrm{GW}_{\eta}$   agree. For a set of incomplete trees    $\mathcal{T}$  , let   $[\mathrm{GW}_{\eta}\,\in\,\mathcal{T}]\,:=\,\cup_{T\in\mathcal{T}}[\mathrm{GW}_{\eta}\,\cong\,T]$  ] and

  $[T_{f}^{-}\in{\mathcal{T}}]:=\cup_{T\in{\mathcal{T}}}[T_{f}^{-}(p(T))=T]$    ]. The following is a simple adaption of [ 9 , Lemma 5.3]; we omit the proof.  

Lemma 3.1.  Let    $\beta\in\left(0,1/4\right)$   and let    $\mathcal{T}$   be a set of incomplete trees such that    $p(T)<n^{\frac{\beta}{10}}$   for all  $T\in{\mathcal{T}}$  . We have  

$$
\begin{array}{r}{(1+o(1))\operatorname{\mathbbP}\left\{\operatorname{GW}_{\eta^{\bot}(\beta)}\in\mathcal{T}\right\}\leq\operatorname{\mathbbP}\left\{T_{f}^{-}\in\mathcal{T}\right\}\leq(1+o(1))\operatorname{\mathbbP}\left\{\operatorname{GW}_{\eta^{\uparrow}(\beta)}\in\mathcal{T}\right\}\,.}\end{array}
$$  

# 4 Stationary distributions  

We will proceed to prove  Theorem 1.1  as in [ 11 ], using the results obtained in previous sections.  

# 4.1 The largest strongly connected component  

For    $\delta^{+}\geq2$  , whp there is a linear size  scc  in    $\vec{\mathbb{G}}_{n}$   [ 8 ,  14 ]. We will ﬁrst show that whp this component is attractive (and so unique), which implies that the simple random walk on    $\vec{\mathbb{G}}_{n}$   has a unique stationary distribution whp.  

For a tail/head    $e^{\pm}\in\mathcal{E}^{\pm}$  , let    $\mathcal{N}_{k}^{\pm}(e^{\pm})$  ) and    $\mathcal{N}_{\leq k}^{\pm}(e^{\pm})$  ) be the sets of tails/heads at distance    $k$   and ≤ at most    $k$   from/to    $e^{\pm}$  , respectively; that is,  

$$
\begin{array}{r l r l}&{\mathfrak{s}^{+}):=\left\{f^{+}\in\mathcal{E}^{+}:\mathrm{dist}(e^{+},f^{+})=k\right\}\ ,\quad}&&{\mathcal{N}_{k}^{-}(e^{-}):=\left\{f^{-}\in\mathcal{E}^{-}:\mathrm{dist}(f^{-},e^{-})=k\right\}\ ,}\\ &{\mathfrak{s}^{+}):=\left\{f^{+}\in\mathcal{E}^{+}:\mathrm{dist}(e^{+},f^{+})\leq k\right\}\ ,\quad}&&{\mathcal{N}_{\leq k}^{-}(e^{-}):=\left\{f^{-}\in\mathcal{E}^{-}:\mathrm{dist}(f^{-},e^{-})\leq k\right\}\ .}\end{array}
$$  

Similarly, for a vertex    $x\in[n]$  , let    $\mathcal{N}_{k}^{\pm}(x)$  ) and    $\mathcal{N}_{\leq k}^{\pm}(x)$  ) be the sets of vertices at distance    $k$   and at ≤ most    $k$   from/to    $x$  , respectively.  

Throughout this section, let    $\omega:=\log^{6}n$  . For every tail/head    $e^{\pm}\in\mathcal{E}^{\pm}$  , consider the stopping time  

$$
t_{\omega}^{\pm}(e^{\pm}):=\operatorname*{inf}\left\lbrace t\geq0:\,\big|\mathcal{N}_{t}^{\pm}(e^{\pm})\big|\geq\omega\right\rbrace\,;
$$  

that is, the ﬁrst time when there are at least    $\omega$   half-edges in the in/out-edge neighbourhood of    $e^{\pm}$  .  

Proposition 4.1.  Let    $M\in\mathbb{N}$   a suppose that    $\delta^{+}\geq2$   and    $\Delta^{\pm}\leq M$  . Let    $\mathcal{C}_{0}$   denote the largest strongly connected component in    $\vec{\mathbb{G}}_{n}$  . Let  

$$
\mathcal{E}_{0}^{-}:=\{f\in\mathcal{E}^{-}:t_{\omega}^{-}(f)<\infty\}\;.
$$  

Let    $E_{0}$   be the event that    $\mathcal{C}_{0}$   is attractive (i.e., there is a directed path from every vertex to    $\mathcal{C}_{0}$  ) and has vertex set    $\mathcal{V}_{0}=\mathcal{V}(\mathcal{E}_{0}^{-})$  . Then    $\mathbb{P}\left\{E_{0}\right\}=1-o(1)$  . Thus, whp the simple random walk on    $\vec{\mathbb{G}}_{n}$   has a unique stationary distribution supported on    $\mathcal{V}_{0}$  .  

Proof.  Let    $h:=1+\log_{2}(2\omega)=O(\log\log n)$  . Then, it follows from [ 11 , Lemma 2.2] that  

$$
\begin{array}{r l}&{\mathbb{P}\left\{\cup_{e\in\mathcal{E}^{+}}[t_{\omega}^{+}(e)>h]\right\}\leq\mathbb{P}\left\{\cup_{y\in[n]}\left[|\mathcal{N}_{h-1}^{+}(y)|\leq\omega\right]\right\}}\\ &{\qquad\qquad\qquad\leq\mathbb{P}\left\{\cup_{y\in[n]}\left[|\mathcal{N}_{h-1}^{+}(y)|\leq\frac12(\delta^{+})^{h-1}\right]\right\}=o(n^{-1})\;,}\end{array}
$$  

where we used    $|\mathcal{N}_{h}^{+}(e)|\geq|\mathcal{N}_{h-1}^{+}(y)|$  | , where    $y$   is the vertex incident to the head paired with    $e$  . So, −  $t_{\omega}^{+}(e)\leq h$   for all  e  $e\in\mathcal{E}^{+}$   whp.  

By [ 9 , Lemma 6.2]  $^2$  , whp every    $f\in\mathcal{E}^{-}$  either has    $t_{\omega}^{-}(f)=\infty$   ∞ or    $t_{\omega}^{-}(f)=O(\log n)$  ). Conditioning on    $t_{\omega}^{+}(e)=O(\log\log n)$  ) and    $t_{\omega}^{-}(f)=O(\log n)$  ), [ 9 , Proposition 7.2] implies that there is a path from  $e$   to    $f$   with probability   $1-o(n^{-2})$  .  by a union bound over all choices of    $e$   and    $f$  , we have that whp there is path from every  e  $e\in\mathcal{E}^{+}$   ∈E   to every    $f\in\mathcal{E}_{0}^{-}$  ; and in particular, from every    $x\in[n]$  to every    $y\,\in\,\nu_{0}$  . In other words, whp    $\mathcal{C}_{0}$   contains all vertices in    $\nu_{0}$   and is attractive. Using [ 9 , Proposition 6.1] with    $t=n/\omega$  , it is easy to check that whp for every    $x\in[n]\setminus\mathcal{V}_{0}$  , there exists    $y_{x}$  such that    $y_{x}$   cannot be reached from    $x$  . It follows that whp    $\mathcal{C}_{0}$   has vertex set    $\mathcal{V}_{0}$  .  

# 4.2 Random walk on heads  

It will be more convenient to perform a random walk on heads instead of vertices. Consider the random process   $(Z_{t}^{\mathbf{e}})_{t\geq0}$    with state space    $\mathcal{E}^{-}$  and, conditioning on    $\vec{\mathbb{G}}_{n}$   and    $Z_{t}^{\mathbf{e}}\,=\,f$  , let    $Z_{t+1}^{\mathbf{e}}$    be ≥ chosen uniformly at random among all heads paired with tails of    $v(f)$  , the endpoint of    $f$  . It follows from  Proposition 4.1  that   $(Z_{t}^{\mathbf{e}})_{t\geq0}$    has a unique stationary distribution supported on    $\mathcal{E}_{0}^{-}$  , denoted ≥ by    $\pi^{\mathbf{e}}$  .  

Recall that    $\pi_{\mathrm{min}}:=\operatorname*{min}\{\pi(y):\,y\in[n],\pi(y)>0\}$  . Let    $\pi_{\mathrm{min}}^{\mathbf{e}}:=\operatorname*{min}\{\pi^{\mathbf{e}}(f):\,f\in\mathcal{E}^{-},\pi^{\mathbf{e}}(f)>0\}$  {  ∈E } . Deﬁne  

$$
\pi_{0}:=\operatorname*{min}\{\pi(y):\,y\in\mathcal{V}_{0}\}\;,\qquad\pi_{0}^{\mathbf{e}}:=\operatorname*{min}\{\pi^{\mathbf{e}}(f):\,f\in\mathcal{E}_{0}^{-}\}\;.
$$  

By  Proposition 4.1 , whp    $\pi_{0}=\pi_{\mathrm{min}}$   and    $\pi_{0}^{\mathbf{e}}=\pi_{\operatorname*{min}}^{\mathbf{e}}$  . The following lemma shows that whp    $\pi_{0}^{\mathbf{e}}$    diﬀers from    $\pi_{0}$   by a bounded factor. Thus, it suﬃces to prove  Theorem 1.1  for    $\pi_{0}^{\mathbf{e}}$  .  

Lemma 4.2.  Whp    $\pi_{0}^{\mathbf{e}}\le\pi_{0}\le M\pi_{0}^{\mathbf{e}}$  ≤ .  

Proof.  Let    $\nu_{0}$   and    $\mathcal{E}_{0}^{-}$    and    $E_{0}$   be as in  Proposition 4.1 . Since    $E_{0}$   happens whp, it suﬃces to prove the lemma conditioning on    $\vec{\mathbb{G}}_{n}\in E_{0}$   ∈ . So we may assume there is a unique stationary distribution  $\pi$   and   $\begin{array}{r}{\operatorname*{lim}_{t\rightarrow\infty}\mathbb{P}\left\{Z_{t}=y\mid Z_{0}=x\right\}=\pi(y)}\end{array}$  , uniformly for all    $x\in[n]$  .  

Let    $x,y\in[n]$   and    $e\in\mathcal{E}^{-}(x)$  . Note that   $(v(Z_{t}^{\mathbf{e}}))_{t\geq0}$    (i.e., the endpoints of the heads in   $(Z_{t}^{\mathbf{e}})_{t\geq0}$  ) ≥ ≥ is itself a simple random walk on   $[n]$   with the same law as   $(Z_{t})_{t\ge0}$  . Therefore,  

$$
\mathbb{P}\left\{Z_{t}=y~\vert~Z_{0}=x\right\}=\sum_{f\in\mathcal{E}^{-}\left(y\right)}\mathbb{P}\left\{Z_{t}^{\mathbf{e}}=f~\vert~Z_{0}^{\mathbf{e}}=e\right\}~.
$$  

If    $y\in\mathcal{V}_{0}$  , then letting    $t\to\infty$  on both sides we have  

$$
\pi(y)=\sum_{f\in{\mathcal{E}}^{-}(y)}\pi^{\mathbf{e}}(f)\geq\pi_{0}^{\mathbf{e}}~,~\quad
$$  

where the last inequality holds  ition of    $\nu_{0}$  . Since    $y\in\mathcal{V}_{0}$   is arbitrary, it follows that    $\pi_{0}\geq\pi_{0}^{\mathbf{e}}$  .  

For the o ection, l  f  $f_{0}\in\mathcal{E}_{0}^{-}$   ∈E   with    $\pi^{\bf e}(f_{0})=\pi_{0}^{\bf e}$  . Let  z  be the vertex that has a tail paired with    $f_{0}$  . Let  e  $e\in\mathcal{E}^{-}$   ∈E and let  x  be its endpoint. We have  

$$
\mathbb{P}\left\{Z_{t}^{\mathbf{e}}=f_{0}\mid Z_{0}^{\mathbf{e}}=e\right\}=\frac{1}{d_{z}^{+}}\mathbb{P}\left\{Z_{t-1}=z\mid Z_{0}=x\right\}\;.
$$  

Again, letting    $t\to\infty$  we have  

$$
\pi_{0}^{\mathbf{e}}=\pi^{\mathbf{e}}(f)=\frac{1}{d_{z}^{+}}\pi(z)\geq\frac{1}{M}\pi_{0}~.
$$  

Thus    $\pi_{0}\leq M\pi_{0}^{\mathbf{e}}$  .  

# 4.3 Lower bound for    $\pi_{0}^{\mathbf{e}}$  

To prove a lower bound it will suﬃce to lower bound the probability of reaching a particular head    $f\in\mathcal{E}_{0}^{-}$    , uniformly for all starting points    $e\in\mathcal{E}^{-}$  (see  Lemma 4.4  below). We use the ideas introduced in [ 5 ,  6 ,  11 ] to capture the weight (deﬁned below) of typical trajectories departing from  $e$  . Our main contribution is to control the total weight of the trajectories landing at    $f$  .  

Deﬁne the  out-entropy    $H^{+}$    and the  entropic time    $\tau_{\mathrm{ent}}$   as  

$$
H^{+}:=\frac{1}{m}\sum_{x\in[n]}d_{x}^{-}\log d_{x}^{+},\quad\tau_{\mathrm{ent}}:=\frac{\log n}{H^{+}}\;.
$$  

We stress the similarity between    $H^{+}$    and  ${\hat{H}}^{-}$  deﬁned in ( 1.8 ); in particular,    $H^{+}=\mathbb{E}[\log D_{\mathrm{in}}^{+}]$  ]. In this case, the out-entropy is related to typical trajectories in supercritical out-growth.  

Fix    $\varepsilon>0$   small enough and let  

$$
h^{+}:=(1-\varepsilon)\tau_{\mathrm{ent}},\qquad h^{-}:=\frac{3\varepsilon}{\log\delta^{+}}\log n\;.
$$  

For    $f\in\mathcal{E}^{-}$  , let    $h(f):=t_{\omega}^{-}(f)\wedge\omega$   ∧ . Deﬁne  

$$
\tau(f):=h^{+}+h^{-}+h(f)\;.
$$  

Remark 4.3.  Bordenave, Caputo and Salez [ 5 ,  6 ] showed that the mixing time of the random walk on    $\vec{\mathbb{G}}_{n}$   coincides with the  entropic time , and exhibits cutoﬀ. Jensen’s inequality implies that

  $\tau_{\mathrm{ent}}\geq(1+o(1))\log_{\nu}n$  , which is whp the typic distance of the    $\vec{\mathbb{G}}_{n}$  . Thus, by the results in [ 9 ],

  $\tau_{\mathrm{ent}}+\operatorname*{max}_{f\in{\mathcal{E}}_{0}^{-}}h(f)$  ) is at least the diameter of  ⃗  $\vec{\mathbb{G}}_{n}$   whp.  

Throughout this section, we will use letters    $a,b$   for tails in    $\mathcal{E}^{+}$    and letters    $e,f,g$   for heads in

  $\mathcal{E}^{-}$  . Let    $P^{t}(e,f):=\mathbb{P}\left\{Z_{t}^{\mathbf{e}}=f\mid Z_{0}^{\mathbf{e}}=e\right\}$   | } . To lower bound    $\pi_{0}^{\mathbf{e}}$    it suﬃces to prove the following:  

Lemma 4.4.  With high probability, for all    $e\in\mathcal{E}^{-}$  and    $f\in\mathcal{E}_{0}^{-}$  ,  

$$
P^{\tau(f)}(e,f)\geq n^{-(1+\hat{H}^{-}/\phi(a_{0})+o(1))}\ .
$$  

Let    $f\in\mathcal{E}_{0}^{-}$    . Assuming  Lemma 4.4  and by stationarity at time    $\tau(f)$  , whp we have  

$$
\pi^{\bf e}(f)=\sum_{e\in{\mathcal E}^{-}}\pi^{\bf e}(e)P^{\tau(f)}(e,f)\geq n^{-(1+\hat{H}^{-}/\phi(a_{0})+o(1))}~,
$$  

and the lower bound in  Theorem 1.1  follows.  

From now on, we ﬁx two heads    $e,f\,\in\,\mathcal{E}^{-}$  . We sequentially expose the out-neighbourhood of    $e$   (out-phase) and the in-neighbourhood of    $f$   (in-phase) as explained below. To control the trajectories departing from    $e$  , we will use the idea of  nice paths  introduced in [ 5 ] as described in [ 11 ].  

In the out-phase we build a directed rooted tree    $T_{e}^{+}$  , partially exposing the out-neighbourhood of    $e$  . The root of    $T_{e}^{+}$    represents the head    $e$   and all its other nodes represent tails in    $\mathcal{E}^{+}$  . For a tail  $a$   represented in    $T_{e}^{+}$  , let    $\mathbf{h}(a)$   denote its height in    $T_{e}^{+}$  . Deﬁne its  weight  by  

$$
{\bf w}(a):=\prod_{i=1}^{{\bf h}(a)}\frac{1}{d_{v(a_{i})}^{+}}\;.
$$  

where    $a_{1},\cdot\cdot\cdot,a_{\mathbf{h}(a)}$   are the tails in the path from    $e$   to    $a$   in    $T_{e}^{+}$  

To construct    $T_{e}^{+}$    we use a procedure similar to  Subsection 3.1 , with the roles of heads and tails reversed and the following two modiﬁcations:  

a) There is no initial tail    $e_{0}^{+}$    . We start at step    $i=2$   with    $e_{1}^{-}=e$   and    $\mathcal{A}_{1}^{\pm}=\mathcal{E}^{\pm}(v(e))$   E  

b) For each    $i\geq2$  , at step (i) we choose    $e_{i}^{+}$    to be the tail in    $\mathcal{A}_{i-1}^{+}$    that  maximises    $\mathbf{w}(a)$   among all −  $a\in\mathcal{A}_{i-1}^{+}$    with    $\mathbf{h}(a)\leq h^{+}-1$   and    $\mathbf{w}(a)\geq n^{-1+\varepsilon^{2}}$  .  

The tree    $T_{e}^{+}$  is not constructed in a BFS manner, but maximising the weight of its nodes; by construction,    $\mathbf{h}(a)\geq\operatorname{dist}(e,a)\!+\!1$  . The process stops if there are no more eligibl Lemma 7], the authors showed that  deterministic ally  constructing    $T_{e}^{+}$    exposes at most  κ  $\kappa^{+}:=n^{1-\varepsilon^{2}/2}$   edges. Moreover, if    $g$   is the head paired with a tail    $a$   represented in    $T_{e}^{+}$  , we have    $P^{\mathbf{h}(a)}(e,g)\geq\mathbf{w}(a)$  .  

In the in-phase we build a directed rooted tree    $T_{f}^{-}$  , exposing    $\mathcal{N}_{\leq h^{-}+h(f)}^{-}(f)$  ) conditionally on    $T_{e}^{+}$  . ≤ We use the exploration process deﬁned in  Subsection 3.1  with one modiﬁcation: if    $e_{i}^{-}$    has already been paired with    $e^{+}$    in the exploration of    $T_{e}^{+}$  , we let    $e_{i}^{+}=e^{+}$  . We stop once all heads at distance  $h^{-}+h(f)$   from    $f$   have been activated and we let    $T_{f}^{-}$    be the ﬁnal incomplete tree generated up to this point. For a head    $g$   in    $T_{f}^{-}$  , let    $\mathbf{h}(g)$   be its height in    $T_{f}^{-}$  ; note that    $\mathbf{h}(g)=\mathrm{dist}(g,f)$  . We deﬁne its  weight  by  

$$
{\mathbf w}(g):=\prod_{i=1}^{{\mathbf h}(g)}\frac{1}{d_{v(g_{i})}^{+}}\;,
$$  

where    $f~=~g_{0},\cdot\cdot\cdot,g_{\bf h}(g)$   are the heads in the path from    $f$   to    $g$   in    $T_{f}^{-}$  . Similarly as before,  $P^{\mathbf{h}(g)}(g,f)\;\geq\;\mathbf{w}(g)$  . For any    $\alpha\ >\ 0$  , we can let    $\varepsilon$   be small enough such that the number of edges exposed in the in-phase is at most  

$$
\kappa^{-}:=\omega h(f)+\omega(\Delta^{-})^{h^{-}+1}=n^{3(\log\Delta^{-}/\log\delta^{+})\varepsilon+o(1)}=O(n^{\alpha})\;.
$$  

Let    $\sigma_{0}$   be a partial realisation of the conﬁguration model in which at most    $\kappa:=\kappa^{+}+\kappa^{-}$  edges are revealed during the out- and in-phases. Then    $T_{e}^{+},T_{f}^{-}$  ,    $h(f)$  ,    $\mathcal{N}_{\leq h^{-}+h(f)}^{-}(f)$  ) and    $\tau(f)$   are all ≤ measurable with respect to    $\sigma_{0}$  . Given    $\sigma_{0}$  , let    $\mathcal{A}$   be the set of unpaired tails    $a$   in  $T_{e}^{+}$    with    $\mathbf{h}(a)=h^{+}$  , and let  $\mathcal{G}$   be the set of unpaired heads    $g$   in    $T_{f}^{-}$    which satisfy    $\mathbf{h}(g)=h^{-}+h(y)$  .  

To bound the desired probability, it will be convenient to consider trajectories that are not too heavy. Following the ideas in [ 5 ,  6 ,  11 ], it suﬃces to only consider  nice paths  in    $T_{f}^{-}$  ; that is, we restrict our attention to the tails    $a\in\mathcal A$   such that    $\mathbf{w}(a)\,\leq\,n^{-1+2\epsilon}$  . For heads in  $\mathcal{G}$  , we deﬁne a truncated version of the weight. Let    $\gamma:=n^{-(1+\varepsilon)\hat{H}^{-}/\phi(a_{0})}$  . For    $g$   in    $T_{f}^{-}$    with    $\mathbf{h}(g)\geq h(f)$  , we deﬁne its  truncated weight  by  

$$
\hat{\mathbf{w}}(g):=(\mathbf{w}(\hat{g})\wedge\gamma)\prod_{i=h(f)+1}^{\mathbf{h}(g)}\frac{1}{d_{v(g_{i})}^{+}}\;.
$$  

where   $\hat{g}$   is the unique head in the path from    $f$   to    $g$   in    $T_{f}^{-}$    with  $\mathbf{h}(\hat{g})=h(f)$  ). In words, the truncated weight caps the contribution of the last    $h(f)$   steps by    $\gamma$  . By the deﬁnition of    $h^{-}$  in ( 4.11 ), for    $g\in\mathcal{G}$  we have  

$$
\hat{\mathbf{w}}(g)\leq(\delta^{+})^{-h^{-}}\gamma\leq\gamma n^{-3\varepsilon}\;.
$$  

Let    $\sigma$   be a complete pairing of half-edges compatible with    $\sigma_{0}$  . Let   $[\sigma(a)=g]$   be the event that  $a$   and    $g$   are paired in    $\sigma$  . Conditioning on    $\sigma_{0}$  , we can lower bound the desired probability as follows  

$$
P^{\tau(f)}(e,f)\geq\hat{P}^{\tau(f)}(e,f):=\sum_{a\in\mathcal{A}}\sum_{g\in\mathcal{G}}\mathbf{w}(a)\hat{\mathbf{w}}(g)\mathbf{1}_{\sigma(a)=g}\mathbf{1}_{\mathbf{w}(a)\leq n^{-1+2\varepsilon}}\ .
$$  

Let  

$$
A_{e,f}(\sigma_{0}):=\sum_{a\in\mathcal{A}}\mathbf{w}(a)\mathbf{1}_{\mathbf{w}(a)\leq n^{-1+2\varepsilon}}\;,\qquad B_{e,f}(\sigma_{0}):=\sum_{g\in\mathcal{G}}\hat{\mathbf{w}}(g)\;,
$$  

which are measurable with respect to    $\sigma_{0}$  . Consider the event  

$$
\mathcal{Y}_{e,f}=\left\{\sigma_{0}:\:A_{e,f}(\sigma_{0})\geq\frac{1}{2},\:B_{e,f}(\sigma_{0})\geq\frac{\omega\gamma}{4}\;\right\}\;.
$$  

If    $\sigma_{0}\in\mathcal{Y}_{e,f}$  , then  

$$
\mathbb{E}\left[\hat{P}^{\tau(f)}(e,f)\Bigm|\sigma_{0}\right]\geq\frac{1}{m}A_{e,f}(\sigma_{0})B_{e,f}(\sigma_{0})\geq\frac{\omega\gamma}{8m}\geq\frac{2\gamma}{n}\;.
$$  

, Lemma 3.6], exploiting the truncated nature of  $\hat{P}^{t}$  We prove a concentration result similar to [ 11 :  

Lemma 4.5.  For every    $\sigma_{0}\in\mathcal{Y}_{e,f}$   and every    $a\in(0,1)$  

$$
\mathbb{P}\left\{\hat{P}^{\tau(f)}(e,f)\leq(1-a)\mathbb{E}\left[\hat{P}^{\tau(f)}(e,f)\Bigm|\sigma_{0}\right]\Bigm|\sigma_{0}\right\}\leq\exp\!\left(-\frac{a^{2}n^{\varepsilon}}{3}\right).
$$  

Proof.  One can write  $\begin{array}{r}{\hat{P}^{\tau(f)}(e,f)=\sum_{a\in\mathcal{E}^{+}}c(a,\sigma(a))}\end{array}$  )) where  

$$
\begin{array}{r}{c({a},{g})=\mathbf{w}({a})\hat{\mathbf{w}}(g)\mathbf{1}_{\mathbf{w}({a})\leq n^{2\varepsilon-1}}\mathbf{1}_{{a}\in\mathcal{A},g\in\mathcal{G}}\;.}\end{array}
$$  

By ( 4.19 ), it follows that  

$$
||c||_{\infty}:=\operatorname*{max}_{a\in\mathcal{E}^{+}}c(a,\sigma(a))\leq\gamma n^{-(1+\varepsilon)}\;.
$$  

Using the one-sided version of Chatterjee’s inequality for uniformly random pairings [ 12 , Proposi- tion 1.1], we get that  

$$
\mathbb{P}\left\{\hat{P}^{\tau(f)}(e,f)\leq(1-a)\mathbb{E}\left[\hat{P}^{\tau(f)}(e,f)\ \Big|\ \sigma_{0}\right]\ \Big|\ \sigma_{0}\right\}\leq\exp\left(-\frac{a^{2}\mathbb{E}\left[\hat{P}^{\tau(f)}(e,f)\ \Big|\ \sigma_{0}\right]}{6\|c\|_{\infty}}\right)\leq
$$  

where we used ( 4.23 ) in the last line.  

Lemma 4.6.  Deﬁne the event  

$$
\mathcal{Y}=\bigcap_{e,f\in\mathcal{E}^{-}}(\mathcal{Y}_{e,f}\cup[t_{\omega}^{-}(f)=\infty])\;.
$$  

We have    ${\mathbb{P}}\left\{y\right\}=1-o(1)$  .  

Let us prove  Lemma 4.4  assuming that  Lemma 4.6  holds. The proof of  Lemma 4.6  is postponed to  Subsection 4.4 .  

Proof of  Lemma 4.4 .  Write  

$$
E_{f}:=[t_{\omega}^{-}(f)=\infty]\;,\;\;\;\;\;\;\;F_{e,f}:=\left[\hat{P}^{\tau(f)}(e,f)\geq\frac{\gamma}{n}\right]\;.
$$  

Using ( 4.23 ) and applying  Lemma 4.5  with    $a=1/3$  ,  

$$
\cdot\left\{F_{e,f}^{c}\;\middle\vert\;\mathcal{Y}_{e,f}\right\}=\mathbb{P}\left\{\hat{P}^{\tau(f)}(e,f)<\frac{\gamma}{n}\;\middle\vert\;\mathcal{Y}_{e,f}\right\}\leq\operatorname*{max}_{\sigma_{0}\in\mathcal{Y}_{e,f}}\mathbb{P}\left\{\hat{P}^{\tau(f)}(e,f)<\frac{\gamma}{n}\;\middle\vert\;\sigma_{0}\right\}=o(n^{-1})
$$  

It follows that,  

$$
\begin{array}{r l}{\mathbb{P}\left\{\cap_{e,f\in\mathcal{E}^{-}}(F_{e,f}\cup E_{f})\right\}\geq\mathbb{P}\left\{\mathcal{V}\cap(\cap_{e,f\in\mathcal{E}^{-}}(F_{e,f}\cup E_{f}))\right\}}&{}\\ {=\mathbb{P}\left\{\mathcal{Y}\right\}-\mathbb{P}\left\{\mathcal{Y}\cap(\cap_{e,f\in\mathcal{E}^{-}}(F_{e,f}\cup E_{f}))^{c}\right\}}&{}\\ {\geq(1-o(1))-\displaystyle\sum_{e,f\in\mathcal{E}^{-}}\mathbb{P}\left\{\mathcal{V}\cap F_{e,f}^{e}\cap E_{f}^{c}\right\}}&{}\\ {\geq(1-o(1))-\displaystyle\sum_{e,f\in\mathcal{E}^{-}}\mathbb{P}\left\{F_{e,f}^{e}\cap\mathcal{Y}_{e,f}\right\}}&{}\\ {\geq(1-o(1))-\displaystyle\sum_{e,f\in\mathcal{E}^{-}}\mathbb{P}\left\{F_{e,f}^{e}\mid\mathcal{Y}_{e,f}\right\}}&{}\\ {=1-o(1)\;,}\end{array}
$$  

where we used that    $\mathcal{Y}\cap E_{f}^{c}$    implies    $\mathcal{Y}_{e,f}$  .  

So, whp, if    $t_{\omega}^{-}(f)<\infty$   ∞ , then    $F_{e,f}$    holds. In other words, we have shown that for all    $e\in\mathcal{E}^{-}$  and  $f\in\mathcal{E}_{0}^{-}$  ,  

$$
P^{\tau(f)}(e,f)\geq\hat{P}^{\tau(f)}(e,f)\geq\frac{\gamma}{n}\geq n^{-(1+(1+\epsilon)\hat{H}^{-}/\phi(a_{0}))}.
$$  

Since    $\varepsilon$   can be arbitrarily small, this proves the lemma.  

# 4.4 Proof of  Lemma 4.6  

Deﬁne the following events  

$$
\mathcal{A}_{1}=\bigcap_{e,f\in\mathcal{E}^{-}}\left[A_{e,f}\geq\frac{1}{2}\right]\;,\qquad\mathcal{A}_{2}=\bigcap_{e,f\in\mathcal{E}^{-}}\left[B_{e,f}\geq\frac{\omega\gamma}{4}\right]\cup[t_{\omega}^{-}(f)=\infty]\;,
$$  

and note that    $\mathcal{Y}\supseteq\mathcal{A}_{1}\cap\mathcal{A}_{2}$  .  

The vertex analogue of the event    $\mathcal{A}_{1}$   was studied in [ 11 , Lemma 3.7]. Note that its proof does not use any assumption on    $\delta^{-}$  , is valid for tail-trees instead of vertex-trees and accommodates the  $\kappa^{-}=O(n^{\alpha})$   half-edges revealed during the in-phase. Thus the conclusion    $\mathbb{P}\left\{\mathcal{A}_{1}\right\}=1-o(1)$   still holds in our setting.  

We are left with showing the following lemma:  

Lemma 4.7.  We have    $\mathbb{P}\left\{\mathcal{A}_{2}\right\}=1-o(1)$  .  

Proof.  In order to analyse the probability of    $\mathcal{A}_{2}$  , it will be convenient to swap the order of the phases: we ﬁrst run the in-phase unconditionally, and then the out-phase. Write    $h=h(f)+h^{-}$  . We will ﬁrst prove that  

$$
\hat{\Gamma}_{h}^{\mathbf{T}}(f):=\sum_{g\in\mathcal{N}_{h}^{-}(f)}\hat{\mathbf{w}}(g)\;,
$$  

is large using branching processes. Note that the diﬀerence between   $\hat{\Gamma}_{h}^{\bf T}(f)$  ) and    $B_{e,f}$   (deﬁned in ( 4.21 )) is that the latter does not include the weight of heads that are paired in the out-phase.  

Consider the marked branching process    $X_{t}$   with oﬀspring distribution    $\eta^{\uparrow}=\eta^{\uparrow}(\beta)$  , where    $\eta=$   $D_{\mathrm{{out}}}$   and    $\beta\,<\,1/4$   will be ﬁxed later. We deﬁne   $\hat{\nu}^{\uparrow}$  ,   $\hat{H}^{\uparrow}$  ,    $\phi^{\uparrow}(a)$  ,    $a_{0}^{\uparrow}$    for    $\eta^{\uparrow}$  analogously to   ${\hat{\boldsymbol\nu}},\ {\hat{H}}$  ,

  $\phi(a)$   and    $a_{0}$   for    $\eta$   (see ( 2.8 ), ( 2.13 ) and ( 2.58 )). It is easy to verify that   $\hat{\nu}^{\uparrow}=(1+o(1))\hat{\nu}$  ,  $\hat{H}^{\uparrow}=

$   $(1+o(1))\hat{H}$   and    $\phi^{\uparrow}(a)=(1+o(1))\phi(a)$  . By continuity of    $\phi(a)$  , we have    $\phi^{\uparrow}(a_{0}^{\uparrow})=(1+o(1))\phi(a_{0})$  ). So  $\hat{H}^{\uparrow}/\phi^{\uparrow}(a_{0}^{\uparrow})=(1+o(1))\hat{H}/\phi(a_{0})$  ). Deﬁne    $\gamma^{\uparrow}:=n^{-(1+\varepsilon/2)\hat{H}^{\uparrow}/\phi^{\uparrow}(a_{0}^{\uparrow})}>\gamma$  .  

Let    $E_{1}(f):=[\hat{\Gamma}_{h}^{\bf T}(f)<\omega\gamma/2]$  2] and    $E_{2}(f):=[t_{\omega}^{-}(f)<\omega]$  ]. Recall the deﬁnition of   $\hat{\Gamma}_{t}$   in ( 2.63 ) and let    $E_{1}:=[\hat{\Gamma}_{t_{\omega}+h^{-}}<\omega\gamma/2]$  2] and    $E_{2}:=[t_{\omega}<\omega]$  . Let    $E_{1}^{\uparrow}:=[\hat{\Gamma}_{t_{\omega}+h^{-}}<\omega\gamma^{\uparrow}/2]$  2] and note that  $E_{1}\subseteq E_{1}^{\uparrow}$  .  

Corollary 2.9  with    $p=n^{-(1+\varepsilon/2)}$    shows that  

$$
{\ensuremath{\mathbb P}}\left\{(G_{t_{\omega}}(\gamma^{\uparrow}))^{c}\cap E_{2}\right\}\le{\ensuremath{\mathbb P}}\left\{(G_{t_{\omega}}(\gamma^{\uparrow}))^{c}\cap[t_{\omega}<\infty]\right\}=O(n^{-(1+\varepsilon/4)})\;.
$$  

So using  Proposition 2.10  

$$
\begin{array}{r l}&{\triangleright\left\{E_{1}^{\dagger}\cap E_{2}\right\}\leq\mathbb{P}\left\{E_{1}^{\dagger}\cap E_{2}\cap G_{t_{\omega}}(\gamma^{\dagger})\right\}+\mathbb{P}\left\{(G_{t_{\omega}}(\gamma^{\dagger}))^{c}\cap E_{2}\right\}}\\ &{\qquad\qquad\leq\displaystyle\sum_{t_{0}=1}^{\omega}\mathbb{P}\left\{E_{1}^{\dagger}\;\Big|\;[t_{\omega}=t_{0}]\cap G_{t_{0}}(\gamma)\right\}\mathbb{P}\left\{t_{\omega}=t_{0}\;|\;t_{\omega}<\omega\right\}+O(n^{-(1+\varepsilon/4)})}\\ &{\qquad\qquad=e^{-c_{0}\omega^{1/3}}+O(n^{-(1+\varepsilon/4)})=O(n^{-(1+\varepsilon/4)})\;.}\end{array}
$$  

Let    $T$   be a marked incomplete tree of height    $\omega+h^{-}\geq h$  . Recall that    $p(T)$   is the number of paired  nodes in    $T$   and that    $T_{f}^{-}(i)$  ) is the marked incomplete tree constructed after the    $i$  -th pairing in the graph exploration process. Conditional on   $[T_{f}^{-}(p(T))=T]$  ], the events    $E_{1}(f)$   and    $E_{2}(f)$   are measurable. Let    $\mathcal{T}$   be the set of incomplete trees    $T$   of height    $\omega+h^{-}$  that satisfy    $E_{1}(f)\cap E_{2}(f)$  . Note that    $p(T)\leq\kappa^{-}=O(n^{\alpha})$   for a constant    $\alpha>0$   as small as needed. Thus, by  Lemma 3.1  with  $\beta=10\alpha$   and ( 4.33 ), we have  

$$
\begin{array}{r l}&{\mathbb{P}\left\{E_{1}(f)\cap E_{2}(f)\right\}=\mathbb{P}\left\{T_{f}^{-}\in\mathcal{T}\right\}}\\ &{\qquad\qquad\qquad\leq(1+o(1))\mathbb{P}\left\{\mathrm{GW}_{\eta^{\uparrow}}\in\mathcal{T}\right\}}\\ &{\qquad\qquad\qquad=(1+o(1))\mathbb{P}\left\{E_{1}\cap E_{2}\right\}}\\ &{\qquad\qquad\qquad\leq(1+o(1))\mathbb{P}\left\{E_{1}^{\uparrow}\cap E_{2}\right\}}\\ &{\qquad\qquad\qquad=O(n^{-(1+\varepsilon/4)})\ .}\end{array}
$$  

By  Lemma 2.7 , we have    $\mathbb{P}\left\{E_{2}^{c}\mid t_{\omega}<\infty\right\}=o(n^{-2})$  } ). Applying  Lemma 3.1  similarly as before shows that  $\mathbb{P}\left\{(E_{2}(f))^{c}\mid t_{\omega}^{-}(f)<\infty\right\}=o(n^{-2})$   {   |  ∞} ). Therefore, it follows from ( 4.34 ) that  

$$
\mathbb{P}\left\{[{\hat{\Gamma}}_{h}^{\mathbf{T}}(f)\geq\omega\gamma/2]\cup[t_{\omega}^{-}(f)=\infty]\right\}=1-O(n^{-(1+\varepsilon/2)})\;.
$$  

Thus, by applying the union bound ﬁrst over    $f\in\mathcal{E}^{-}$  and then over    $e\in\mathcal{E}^{-}$  , we have  

$$
\mathbb{P}\left\{\mathcal{A}_{2}^{c}\right\}=o(1)+\sum_{e,f\in\mathcal{E}^{-}}\mathbb{P}\left\{\left[B_{e,f}<\frac{\omega\gamma}{4}\right]\cap\left[\hat{\Gamma}_{h}^{\mathbf{T}}(f)\geq\frac{\omega\gamma}{2}\right]\right\}\;.
$$  

Let    $\sigma^{-}$  be a partial pairing of the at most    $\kappa^{-}\,=\,o(n)$   half-edges that expose    $\textstyle{\mathcal{N}}_{\leq h}^{-}(f)$  ). We now ≤ perform the out-phase to construct the tree    $T_{e}^{+}$  conditional on    $\sigma^{-}$  . Recall that during the out- phase at most    $\kappa^{+}=o(n)$   edges are formed. Thus,  

$$
\begin{array}{r l}&{\mathbb{E}\left[B_{e,f}\ \middle\vert\ \sigma^{-}\right]=\displaystyle\sum_{g\in\mathcal{N}_{h}^{-}(f)}\hat{\mathbf{w}}(g)\mathbb{E}\left[\mathbf{1}_{g\in\mathcal{G}}\ \middle\vert\ \sigma^{-}\right]}\\ &{\qquad\geq\left(1-\frac{\kappa^{+}}{m-\kappa^{+}-\kappa^{-}}\right)\displaystyle\sum_{g\in\mathcal{N}_{h}^{-}(f)}\hat{\mathbf{w}}(g)}\\ &{\qquad\qquad=(1+o(1))\hat{\Gamma}_{h}^{\mathbf{T}}(f)\ .}\end{array}
$$  

Using ( 4.19 ), an application of Azuma’s inequality (see [ 21 , Chapter 11]) to    $B_{e,f}$   determined by the random vector   $\left(\hat{\mathbf{w}}(g)\mathbf{1}_{g\in\mathcal{G}}\right)_{g\in\mathcal{N}_{h}^{-}(f)}$   implies that  

$$
\mathbb{P}\left\{B_{e,f}<\frac{\hat{\Gamma}_{h}^{\mathbf{T}}(f)}{2}\Biggm|\sigma^{-}\right\}\leq\exp\left\{-2\frac{(1+o(1)\hat{\Gamma}_{h}^{\mathbf{T}}(f)^{2}}{\sum_{g\in\mathcal{N}_{h}^{-}(f)}\hat{\mathbf{w}}(g)^{2}}\right\}\leq\exp\left\{-2\gamma^{-1}n^{2\varepsilon}\hat{\Gamma}_{h}^{\mathbf{T}}(f)\right\}\;.
$$  

Since the event   $\bigl[\hat{\Gamma}_{h}^{\mathbf{T}}(f)>\frac{\omega\gamma}{2}\bigr]$  ] is measurable with respect to    $\sigma^{-}$  , we have  

$$
\mathbb{P}\left\{\left[B_{e,f}<\frac{\omega\gamma}{4}\right]\cap\left[\hat{\Gamma}_{h}^{\mathbf{T}}(f)>\frac{\omega\gamma}{2}\right]\right\}\leq\exp\left\{-2\gamma^{-1}n^{2\varepsilon}\left(\frac{\omega\gamma}{2}\right)\right\}\leq\exp\left\{-\omega n^{2\varepsilon}\right\}\;.
$$  

Putting this into ( 4.36 ) ﬁnishes the proof.  

# 4.5 Upper bound for    $\pi_{0}^{\mathbf{e}}$  

Let    $\varepsilon>0$   be small enough. Let    $a_{0}$   be the minimiser of    $\phi(a)$  , deﬁned as in ( 1.10 ). Let  

$$
h_{1}:=\frac{1-\varepsilon}{a_{0}\phi(a_{0})}\log n\;,\qquad h_{2}:=\log_{\nu}\omega=O(\log\log n)\;.
$$  

In this subsection we set    $\gamma:=n^{-(1-\varepsilon)\hat{H}^{-}/\phi(a_{0})}$  . Let   $\Gamma_{t}(f)$   be a graph analogue of   $\Gamma_{t}$   (see  Section 2 ) deﬁned by  

$$
\Gamma_{t}(f):=\sum_{g\in\mathcal{N}_{t}^{-}(f)}P^{t}(g,f)\;.
$$  

Recall the deﬁnition of    $\mathbf{w}(g)$   in ( 4.16 ). Deﬁne  

$$
\Gamma_{t}^{\mathbf{T}}(f):=\sum_{g\in\mathcal{N}_{t}^{-}(f)}\mathbf{w}(g)\;.
$$  

For any subgraph    $G$   with vertex set a subset of   $[n]$  , let   $\mathrm{TX}(G):=|E(G)|-(|V(G)|-1)$   denote the excess of edges of    $G$   comparing to the case when it induces a tree.  

For    $f\in\mathcal{E}^{-}$  , consider the following events:  

$$
\begin{array}{r l}&{E_{0}(f)=[t_{\omega}^{-}(f)\leq h_{1}+h_{2}]\;,\quad\;\;E_{1}(f)=[0<\Gamma_{h_{1}}^{\mathbf{T}}(f)<\gamma]\;,}\\ &{E_{2}(f)=[\mathrm{TX}(\mathcal{N}_{\leq h_{1}}^{-}(f))=0]\;,\;\;\;F(f)=\displaystyle\bigcap_{r=1}^{h_{1}}[0<|\mathcal{N}_{r}^{-}(f)|<\omega]\;.}\end{array}
$$  

Write    $\mathcal{A}(f)=E_{0}(f)\cap E_{1}(f)\cap E_{2}(f)\cap F(f)$  .  

The proof in this section uses    $T_{f}^{-}$  , the tree constructed in  Subsection 3.1 , stopping once all heads at distance    $h_{1}+h_{2}$   to    $f$   have become active. During the construction of    $T_{f}^{-}$    we deterministic ally expose at most    $\kappa_{1}^{-}:=\omega h_{1}+\omega(\Delta^{-})^{h_{2}+1}=O(\log^{C}n)$  ) pairings for some constant    $C$  .  

Proposition 4.8.  For all    $f\in\mathcal{E}^{-}$  , we have  

$$
\mathbb{P}\left\{\mathcal{A}(f)\right\}=n^{-1+\varepsilon+o(1)}\;.
$$  

Proof r    $E_{0}=[t_{\omega}\leq h_{1}+h_{2}]$   $E_{1}=[0<\Gamma_{h_{1}}<\gamma]$   $\begin{array}{r}{F=\bigcap_{r=1}^{h_{1}}[0<X_{r}<\omega]}\end{array}$  ] to be the corresponding events on the marked branching process (  $(X_{r})_{r\geq0}$   with oﬀspring distribution    $\eta^{\downarrow}(\beta)$  ≥ with  η  $\eta\,=\,D_{\mathrm{out}}$   and an arbitrary  β  $\beta\,\in\,\bigl(0,1/4\bigr)$   ∈ 4) to be determined later. With    $\boldsymbol{a}=\boldsymbol{a}_{0}$   and    $t\,=\,h_{1}$  , Theorem 2.5  implies that  

$$
{\mathbb{P}}\left\{E_{1}\cap F\right\}\geq n^{-1+\varepsilon+o(1)}\ .
$$  

When    $F$   happens, there is at least one individual in generation    $h_{1}$  . Thus, for some constant    $c$  ,  

$$
{\mathbb P}\left\{E_{0}\ |\ E_{1}\cap F\right\}\geq{\mathbb P}\left\{X_{h_{2}+h_{1}}\geq\omega\ |\ X_{h_{1}}>0\right\}\geq{\mathbb P}\left\{X_{h_{2}}\geq\omega\right\}>c\,,
$$  

where in the last inequality we used that    $\ensuremath{\boldsymbol\nu}^{-h}\ensuremath{\boldsymbol X}_{h}$   converges to an absolutely continuous random variable, and    $h_{2}=\log_{\nu}\omega$  . Therefore,    ${\mathbb{P}}\left\{E_{0}\cap E_{1}\cap F\right\}\geq n^{-1+\varepsilon+o(1)}$  

Conditional on   $[T_{f}^{-}(p(T))=T]$  ], the event  $E_{0}(f)\cap E_{1}(f)\cap F(f)$   ∩  ∩ ) is measurable. Let    $\mathcal{T}$   be the set of incomplete trees    $\mathit{\Omega}^{\prime}\mathit{I}^{\prime}$   that satisfy    $E_{0}(f)\cap E_{1}(f)\cap F(f)$  . Using  Lemma 3.1  we have  

$$
\begin{array}{r}{\mathbb{P}\left\{E_{0}(f)\cap E_{1}(f)\cap F(f)\right\}\geq(1+o(1))\mathbb{P}\left\{\mathrm{GW}_{\eta^{\downarrow}}\in\mathcal{T}\right\}\geq n^{-1+\varepsilon+o(1)}\ ,}\end{array}
$$  

where we used that the parameters   $\hat{\nu}^{\downarrow}$  ,  $\hat{H}^{\downarrow}$  ,    $\phi^{\downarrow}(a_{0}^{\downarrow})$  ) are well-approximated by  ν ,  $\hat{H}$  ,    $\phi(a_{0})$  , as in the proof of  Lemma 4.7 .  

We next bound the probability of   $(E_{2}(f))^{c}\cap F(f)$  . Recall that   $\mathrm{TX}(\mathcal{N}_{\leq h_{1}}^{-}(f))$  )) is the number of ≤ collisions; that is, steps in the exploration process where    $e_{i}^{+}\in\mathcal{A}_{i-1}^{+}$    . At step    $i$  , the probability of − a collision is at most  $\scriptstyle{\frac{\Delta^{+}i}{m-i+1}}$  . So, provided    $\ell\leq m/2$  , the number of collisions in the ﬁrst    $\ell$  steps is − dominated by a binomial random variable with parameters   $(\ell,2\ell M/m)$  . Note that the probability of   $(E_{2}(f))^{c}\cap F(f)$   is bounded from above by the probability of at least one collision happens when running the exploration process for at most    $\ell=(\omega-1)h_{1}=O(\log^{7}n)$   steps. Thus we have  

$$
{\mathbb P}\left\{(E_{2}(f))^{c}\cap F(f)\right\}=O(\ell^{2}/m)=n^{-1+o(1)}\;.
$$  

Thus, we obtain the lower bound in ( 4.44 ):  

$$
\mathbb{P}\left\{\mathcal{A}(f)\right\}=\mathbb{P}\left\{E_{0}(f)\cap E_{1}(f)\cap E_{2}(f)\cap F(f)\right\}
$$  

$$
\begin{array}{r}{\geq\mathbb{P}\left\{E_{0}(f)\cap E_{1}(f)\cap F(f)\right\}-\mathbb{P}\left\{(E_{2}(f))^{c}\cap F(f)\right\}\geq n^{-1+\varepsilon+o(1)}\;.}\end{array}
$$  

For the upper bound in ( 4.44 ), an application of  Theorem 2.6  with oﬀspring    $\eta^{\uparrow}(\beta)$  ,    $a\,=\,a_{0}^{\uparrow}$    and  $t=h_{1}$  , and  Lemma 3.1 , as in the proof of  Lemma 4.7 , gives that  

$$
\begin{array}{r l}&{\mathbb{P}\left\{\mathcal{A}(f)\right\}\leq\mathbb{P}\left\{E_{1}(f)\cap F(f)\right\}}\\ &{\qquad\qquad\leq(1+o(1))\mathbb{P}\left\{E_{1}\cap F\right\}}\\ &{\qquad\qquad\leq\mathbb{P}\left\{(G_{t}(e^{-a\hat{H}t}))^{c}\cap[0<X_{t}<\omega]\right\}}\\ &{\qquad\qquad\leq n^{-1+\varepsilon+o(1)}\;.}\end{array}
$$  

ll show that    $\begin{array}{r}{Z:=\sum_{f\in\mathcal{E}^{-}}\mathbf{1}_{\mathcal{A}(f)}}\end{array}$    satisﬁes    $Z>0$   whp, using a second moment calculation. ∈E As  m  $m\geq n$   ≥ ,  Proposition 4.8  implies that    $\mathbb{E}[Z]\geq n^{\varepsilon/2}$  . For a partial pairing    $\sigma_{0}$  , we write    $f\in I m(\sigma_{0})$  if    $f$   has been paired by  σ  $\sigma_{0}$  . For    $f_{1},f_{2}\in\mathcal{E}^{-}$  with    $f_{1}\neq f_{2}$  , we have  

$$
\begin{array}{r l r}&{}&{\mathbb{P}\left\{\mathcal{A}(f_{1})\cap\mathcal{A}(f_{2})\right\}\le\displaystyle\sum_{\sigma_{0}\in\mathcal{A}(f_{1})}\mathbb{P}\left\{\sigma_{0}\right\}\left(\mathbf{1}_{f_{2}\in I m(\sigma_{0})}+\mathbf{1}_{f_{2}\notin I m(\sigma_{0})}\mathbb{P}\left\{\mathcal{A}(f_{2})\mid\sigma_{0}\right\}\right)}\\ &{}&{\qquad\le\mathbb{P}\left\{\mathcal{A}(f_{1})\right\}\left(O\left(\frac{\log^{C}n}{n}\right)+\displaystyle\operatorname*{max}_{\sigma_{0}\in\mathcal{A}(f_{1})\atop f_{2}\notin I m(\sigma_{0})}\mathbb{P}\left\{\mathcal{A}(f_{2})\mid\sigma_{0}\right\}\right)\;,}\end{array}
$$  

where we used that at most    $\kappa_{1}^{-}=O(\log^{C}n)$  ) edges need to be exposed to determine  $\mathcal{A}(f_{1})$  .  

We brieﬂy describe how to compute    $\mathbb{P}\left\{\mathcal{A}(f_{2})\mid\sigma_{0}\right\}$   for    $f_{2}\,\notin\,\operatorname{Im}(\sigma_{0})$  ∈ ). Start the exploration process in  Subsection 3.1  from    $f_{2}$   with one modiﬁcation: if    $e_{i}^{-}$    has already been paired in    $\sigma_{0}$  , then we choose    $e_{i}^{+}$    according to    $\sigma_{0}$   instead of uniformly at random. Since at most    $\kappa_{1}^{-}$    half-edges need to be paired, the probability of   $[\mathcal{N}_{\leq h_{1}+h_{2}}(f_{2})\cap I m(\sigma_{0})\neq\emptyset]$   is at most    $O((\kappa_{1}^{-})^{2}/n)$  ). This implies that  

$$
\mathbb{P}\left\{\mathcal{A}(f_{2})\mid\sigma_{0}\right\}\le\mathbb{P}\left\{\mathcal{A}(f_{2})\right\}+O\left(\frac{(\kappa_{1}^{-})^{2}}{n}\right)\;.
$$  

By  Proposition 4.8 , we obtain  

$$
\mathbb{P}\left\{\mathcal{A}(f_{1})\cap\mathcal{A}(f_{2})\right\}\leq\mathbb{P}\left\{\mathcal{A}(f_{1})\right\}\left(O\left(\frac{(\kappa_{1}^{-})^{2}}{n}\right)+\mathbb{P}\left\{\mathcal{A}(f_{2})\right\}\right)\leq n^{-2+3\varepsilon}\ .
$$  

So    $\mathbb{E}[Z^{2}]=(1+o(1))\mathbb{E}[Z]^{2}$  , and Chebyshev’s inequality implies that whp   $[Z>0]$  .  

$[Z>0]$     $f_{0}$   be s  ${\mathcal{A}}(f_{0})$   $E_{0}(f_{0})$  have    $f_{0}\in\mathcal{E}_{0}^{-}$    . Moreover,  $E_{1}(f_{0})\cap E_{2}(f_{0})$   ∩  $\begin{array}{r}{\sum_{g\in{\mathscr{E}}^{-}}P^{h_{1}}(g,f_{0})=\Gamma_{h_{1}}(f_{0})=\Gamma_{h_{1}}^{\mathbf{T}}(f_{0})<\gamma}\end{array}$  P . Moreover,  Remark 1.5 ∈E implies that  $\pi^{\mathbf{e}}(g)\leq n^{-1+o(1)}$   ≤   for any    $g\in{\mathcal{E}}^{-}$  . By stationarity at time    $h_{1}$  , we obtain  

$$
\pi_{0}^{\bf e}\le\pi^{\bf e}(f_{0})=\sum_{g\in{\mathcal E}^{-}}P^{h_{1}}(g,f_{0})\pi^{\bf e}(g)\le n^{-1-\frac{(1-\varepsilon)\hat{H}^{-}}{\phi(a_{0})}+o(1)}\;,
$$  

for any    $\varepsilon>0$   and the upper bound in  Theorem 1.1  follows.  

# 4.6 The proof of  Remark 1.8  

To show the upper bound in ( 1.16 ) in  Remark 1.8 , we can follow the line of arguments in  Sub- section 4.3  and  Subsection 4.4 . Here we brieﬂy sketch the changes needed for it to work. For  $\alpha\in[0,\hat{H}^{-}/\phi\left(a_{0}\right)]$  )], let    $\gamma_{\alpha}:=n^{-\alpha}$    and    $\begin{array}{r}{\beta=\frac{\alpha\phi(a_{0})}{\hat{H}^{-}}\leq1}\end{array}$  1. By  Corollary 2.9  with    $p=n^{-\beta}$  , we have  

$$
{\mathbb{P}}\left\{(G_{t_{\omega}}(\gamma_{\alpha}))^{c}\cap[t_{\omega}<\infty]\right\}\le n^{-\beta+o(1)}\;.
$$  

Let  

$$
y_{f}^{\alpha}:=\bigcap_{e\in\mathcal{E}^{-}}\left[\left[A_{e,f}\geq\frac{1}{2}\right]\cap\left[B_{e,f}\geq\frac{\omega\gamma_{\alpha}}{4}\right]\right]\;.
$$  

Then by the same argument of  Lemma 4.6  using ( 4.54 ), we have  

$$
{\mathbb P}\left\{(\mathcal{Y}_{f}^{\alpha})^{c}\cap[t_{\omega}^{-}(f)<\infty]\right\}\le n^{-\beta+o(1)}\;.
$$  

Let    $\begin{array}{r}{\mathcal{B}(f):=\left[0<\pi^{\bf e}(f)<\frac{\gamma_{\alpha}}{2n}\right]}\end{array}$  . It follows from  Proposition 4.1  and  Lemma 4.5  that  

$$
\begin{array}{r l}&{\mathbb{P}\left\{\mathcal{B}(f)\right\}=(1+o(1))\mathbb{P}\left\{\left[\pi^{\mathbf{e}}(f)<\displaystyle\frac{\gamma_{\alpha}}{2n}\right]\cap\left[t_{\omega}^{-}(f)<\infty\right]\right\}}\\ &{\qquad\qquad\leq(1+o(1))\left(\mathbb{P}\left\{\left[\pi^{\mathbf{e}}(f)<\displaystyle\frac{\gamma_{\alpha}}{2n}\right]\cap\mathcal{Y}_{f}^{\alpha}\right\}+\mathbb{P}\left\{(\mathcal{Y}_{f}^{\alpha})^{c}\cap\left[t_{\omega}^{-}(f)<\infty\right]\right\}\right)}\\ &{\qquad\qquad=(1+o(1))\mathbb{P}\left\{\cup_{e\in\mathcal{E}^{-}}\left[\hat{P}^{\tau(f)}(e,f)<\displaystyle\frac{\gamma_{\alpha}}{2n}\right]\cap\mathcal{Y}_{f}^{\alpha}\right\}+n^{-\beta+o(1)}}\\ &{\qquad\qquad=n^{-\beta+o(1)}\ ,}\end{array}
$$  

where in the last step we have used  Lemma 4.5 .  

Recall the deﬁnition of    $\mathcal{A}(f)$   below ( 4.43 ). Deﬁne    $\mathcal{A}_{\alpha}(f)$   analogously with    $h_{1}$   replaced by  $h_{1,\alpha}=\beta h_{1}$   and    $\gamma$   replaced by    $\gamma_{\alpha}$  . Then by the same argument of  Proposition 4.8 , we have  

$$
{\mathbb{P}}\left\{{\mathcal{B}}(f)\right\}\geq(1+o(1)){\mathbb{P}}\left\{{\mathcal{A}}_{\alpha}(f)\right\}\geq n^{-\beta+o(1)}\;,
$$  

and  Remark 1.8  follows.  

# 5 Applications  

# 5.1 Hitting and cover times  

We now prove  Theorem 1.9 , i.e., whp the maximal hitting and the cover time of    $\vec{\mathbb{G}}_{n}$   are both  $n^{1+\hat{H}^{-}/\phi(a_{0})+o(1)}$  . Clearly,    $\tau_{\mathrm{hit}}\leq\tau_{\mathrm{cov}}$  s to lower bound    $\tau_{\mathrm{hit}}$   and upper bound    $\tau_{\mathrm{cov}}$  .  

For the lower bound, let  $C<(1+\hat{H}^{-}/\phi(a_{0}))$  )) be a constant. Then by  Theorem 1.1 , we have  $\pi_{\mathrm{min}}\leq n^{-C}$  ion of    $\tau_{x}(y)$   in  Subsection 1.2 . The  returning time  to    $x\in[n]$  is  $\tau_{x}^{+}:=\operatorname*{inf}\{t\geq1:Z_{t}=x,Z_{0}=x\}$  {  ≥ } . By the well-known relation between the expected returning time and the stationary distribution, we have whp  

$$
\operatorname*{max}_{x\in[n]}\mathbb{E}\left[\tau_{x}^{+}\;\middle|\;\vec{\mathbb{G}}_{n}\right]=\operatorname*{max}_{x\in[n]}\frac{1}{\pi(x)}=\frac{1}{\pi_{\operatorname*{min}}}\geq n^{C}\;.
$$  

Thus, whp there exists a vertex    $x_{0}\in[n]$   such that    $\mathbb{E}\left[\tau_{x_{0}}^{+}\ \Big|\ \vec{\mathbb{G}}_{n}\right]\geq n^{C}$  , which implies that  

$$
n^{C}\leq\mathbb{E}\left[\tau_{x_{0}}^{+}\ \Big|\ \vec{\mathbb{G}}_{n}\right]=1+\frac{1}{d_{x_{0}}^{+}}\sum_{y\in\mathcal{N}_{\leq1}^{+}(x_{0})}m(x_{0},y)\mathbb{E}\left[\tau_{y}(x_{0})\ \Big|\ \vec{\mathbb{G}}_{n}\right]\ ,
$$  

where    $m(x,y)$   is the multiplicity of the directed edge   $(x,y)$   in    $\vec{\mathbb{G}}_{n}$  . Thus, whp there exists two vertices    $x_{0}$   and    $y_{0}$   such that  

$$
\mathbb{E}\left[\tau_{y_{0}}(x_{0})\ \Big|\ \vec{\mathbb{G}}_{n}\right]\geq n^{C}-1\;.
$$  

It follows that    $\tau_{\mathrm{hit}}\geq n^{C}-1$  

For the upper bound, let  $C>(1+\hat{H}^{-}/\phi(a_{0}))$  )) be a ﬁxed constant and let    $\tau=\omega^{2}=\log^{12}n$  . Recall the deﬁnition of    $\nu_{0}$   in  Proposition 4.1 .  Lemma 4.4  implies that whp for all    $x\,\in\,[n]$   and  $y\in\mathcal{V}_{0}$  , there exists    $\tau(y)\leq\tau$   (by ( 4.12 )) such th  $P^{\tau(y)}(x,y)\geq n^{-C}$  , . So the probability to hit    $y$  in at most    $\tau$   steps, which we call a  try , is at least  n  $n^{-C}$    uniformly for any starting point    $x$  . Thus, the number of tries needed to hit    $y$   is stochastically dominated by a geometric random variable with success probability    $n^{-C}$  . It follows that whp  

$$
\tau_{\mathrm{hit}}=\operatorname*{max}_{\boldsymbol x\in[n]}\mathbb E[\tau_{\boldsymbol x}(\boldsymbol y)]\leq n^{C}\boldsymbol\tau\ .
$$  

Therefore, by Matthews’ bound [ 20 , Theorem 2.6], we have  

$$
\tau_{\mathrm{cov}}\leq H_{n}\tau_{\mathrm{hit}}=n^{C+o(1)}\;,
$$  

where    $H_{n}$   is the    $n$  -th harmonic number.  

# 5.2 Explicit constants for particular degree sequences  

In this section we discuss two particular examples where the polynomial exponent can be made explicit.  

# 5.2.1  $r$  -out digraph  

For any integer    $r\geq2$  , an    $r$  -out digraph    $D_{n,r}$   is a random directed graph with    $n$   vertices in which each vertex chooses    $r$   out-neighbours uniformly at random. It is used as a model for studying uniform random Deterministic Finite Automata [ 7 ]. For    $r\geq2$  , Addario-Berry, Balle, Perarnau [ 1 ] showed that in    $D_{n,r}$  , whp,  

$$
\pi_{\mathrm{min}}=n^{-(1+\log(r)/(s r-\log r)+o(1))}\;\;,
$$  

where    $s$   is the largest solution of   $1-s=e^{-s r}$  

Although in    $D_{n,r}$   the in-degrees are random, events that hold whp in    $\vec{\mathbb{G}}_{n}$   also holds whp in  $D_{n,r}$  , assuming that    $D^{-}$  (the in-degree of a uniform random vertex in    $\vec{\mathbb{G}}_{n}$  ) converges to a Poisson distribution with mean    $r$   whereas the    $D^{+}=r\ge2$   almost surely. It is an exercise to check that whp the maximum in-degree of    $D_{n,p}$   has order  $\frac{\log n}{\log\log n}$  . A careful inspection of the proof of  Theorem 1.1 shows that the bounded maximum degree condition can be relaxed to   $\Delta^{\pm}=o(\log n)$  . Therefore, the conclusion of  Theorem 1.1  holds in this setting. As    $\mathbb{P}\left\{D^{+}=r\right\}=1$  , we have    $I(a\hat{H}^{-})=\infty$   ∞ for any    $a\ne1$  , so    $a_{0}=1$   and    $\pi_{\mathrm{min}}=n^{-(1+\left|\log\hat{\nu}^{-}\right|+o(1))}$  , which coincides with ( 5.6 ).  

# 5.2.2 A toy example  

Here we show how the explicit value for    $\pi_{\mathrm{min}}$   can be computed for a simple distribution, providing an example where    $a_{0}\neq1$  . Consider a degree distribution    $D$   deﬁned by  

$$
\mathbb{P}\left\{D=(0,2)\right\}=\mathbb{P}\left\{D=(0,3)\right\}=\mathbb{P}\left\{D=(5,0)\right\}=\mathbb{P}\left\{D=(5,2)\right\}=\frac{1}{4}\;.
$$  

s    $D^{+}$    is uniform on    $\{2,3\}$   and is independent from    $D^{-}$  , we have  $\tilde{D}_{\mathrm{out}}^{+}=\log2+\log(3/2)X$  , where X  is a Bernoulli random variable with probability    $p=3/5$  . The large deviation rate function for a Bernoulli random variable with probability    $p$   is  $\begin{array}{r}{I_{\mathrm{Be}}(z)=z\log\left(\frac{z}{p}\right)+(1-z)\log\left(\frac{1-z}{1-p}\right)}\end{array}$     for    $z\in[0,1]$  (see, e.g., [ 16 , Exercise 2.2.23]). Thus, we have  I  $I(z)=I_{\mathrm{Be}}\left((\log(3/2))^{-1}(z-\log2)\right)$   − .  

With the help of interval arithmetic libraries [ 24 ], we get  

$$
0.936426\;,\quad\hat{\nu}\doteq0.181095\;,\quad a_{0}\doteq1.06671\;,\quad\phi(a_{0})\doteq1.65129\;,\quad1+\frac{\hat{H}^{-}}{\phi(a_{0})}\doteq1.567
$$  

with errors guaranteed to be at most   $10^{-6}$    by the algorithm. As shown in  Figure 2 , the function  $\phi(a)$   attains minimum at    $a_{0}\,>\,1$  . In particular, the vertex that is  hardest  to hit has a “thin” in-neighbourhood which is of   $97.1\%$   of the length of the longest such “thin” in-neighbourhoods. In other words, it is  not  the vertex that is furthest away from others that is hardest to ﬁnd.  

  
Figure 2: Plot of the function    $\phi(a)$  

Acknowledgements. We would like to thanks Pietro Caputo and Matteo Quattropani for in- sightful discussions on the topic.  

# References  

[1] L. Addario-Berry, B. Balle, and G. Perarnau. Diameter and Stationary Distribution of Random r-Out Digraphs.  The Electronic Journal of Combinatorics , pages P3.28–P3.28, Aug. 2020. ISSN 1077-8926. doi:  10/ghd74q .  

[2] H. Amini. Bootstrap Percolation in Living Neural Networks.  J Stat Phys , 141(3):459–475, Nov. 2010. ISSN 1572-9613. doi:  10/c53hx4 . [3] K. B. Athreya and P. E. Ney.  Branching Processes . Grundlehren Der Mathematischen Wis- senschaften. Springer-Verlag, Berlin Heidelberg, 1972. doi:  10/dft4 . [4] J. Blanchet and A. Stauﬀer. Characterizing optimal sampling of binary contingency ta- bles via the conﬁguration model. Random Structures & Algorithms , 42(2):159–184, 2013. doi:  10/f4mtxh . [5] C. Bordenave, P. Caputo, and J. Salez. Random walk on sparse random digraphs.  Probab. Theory Relat. Fields , 170(3):933–960, Apr. 2018. ISSN 1432-2064. doi:  10/gc8nxk . [6] C. Bordenave, P. Caputo, and J. Salez. Cutoﬀat the “entropic time” for sparse Markov chains. Probab. Theory Relat. Fields , 173(1):261–292, Feb. 2019. ISSN 1432-2064. doi:  10/ghcrhr . [7] X. S. Cai and L. Devroye. The graph structure of a deterministic automaton chosen at random. Random Structures & Algorithms , 51(3):428–458, 2017. ISSN 1098-2418. doi:  10/gbtqgb . [8] X. S. Cai and G. Perarnau. The giant component of the directed conﬁguration model revisited. arXiv:2004.04998 [cs, math] , Apr. 2020. URL  http://arxiv.org/abs/2004.04998 . [9] X. S. Cai and G. Perarnau. The diameter of the directed conﬁguration model.  arXiv:2003.04965 [cs, math] , Mar. 2020. URL  http://arxiv.org/abs/2003.04965 .

 [10] P. Caputo and M. Quattropani. Mixing time of PageRank surfers on sparse random digraphs. arXiv:1905.04993 [math] , July 2020. URL  http://arxiv.org/abs/1905.04993 .

 [11] P. Caputo and M. Quattropani. Stationary distribution and cover time of sparse directed con- ﬁguration models.  Probab. Theory Relat. Fields , Aug. 2020. ISSN 1432-2064. doi:  10/ghd74v .

 [12] S. Chatterjee. Stein’s method for concentration inequalities.  Probab. Theory Relat. Fields , 138 (1-2):305–321, Feb. 2007. ISSN 0178-8051, 1432-2064. doi:  10/fm2x4r .

 [13] N. Chen, N. Litvak, and M. Olvera-Cravioto. Generalized PageRank on directed conﬁgu- ration networks.  Random Structures & Algorithms , 51(2):237–274, 2017. ISSN 1098-2418. doi:  10/gbrth6 .

 [14] C. Cooper and A. Frieze. The Size of the Largest Strongly Connected Component of a Random Digraph with a Given Degree Sequence.  Combinatorics, Probability and Computing , 13(3): 319–337, May 2004. doi:  10/cn8q5j .

 [15] C. Cooper and A. Frieze. Stationary distribution and cover time of random walks on random di- graphs.  J. Comb. Theory Ser. B , 102(2):329–362, Mar. 2012. ISSN 0095-8956. doi:  10/cv9wbh .

 [16] A. Dembo and O. Zeitouni. Large Deviations Techniques and Applications . Stochastic Modelling and Applied Probability. Springer-Verlag, Berlin Heidelberg, second edition, 2010. doi:  10.1007/978-3-642-03311-7 .

 [17] A. Graf.  On the Strongly Connected Components of Random Directed Graphs with given Degree Sequences . PhD thesis, University of Waterloo, 2016. URL  http://hdl.handle.net/10012/ 10681 .  

[18] S. Janson. The probability that a random multigraph is simple.  Combinatorics, Probability and Computing , 18(1-2):205–225, 2009. doi:  10/bg4m2c .

 [19] H. Li. Attack Vulnerability of Online Social Networks. In  2018 37th Chinese Control Confer- ence (CCC) , pages 1051–1056, July 2018. doi:  10/ggh2kg .

 [20] P. Matthews. Covering Problems for Brownian Motion on Spheres. Ann. Probab. , 16(1): 189–199, Jan. 1988. ISSN 0091-1798, 2168-894X. doi:  10/c8q2r8 .

 [21] M. Molloy and B. Reed. Graph Colouring and the Probabilistic Method . Algorithms and Combinatorics. Springer-Verlag, Berlin Heidelberg, 2002. doi:  10.1007/978-3-642-04016-0 .

 [22] V. Petrov. Sums of Independent Random Variables . Ergebnisse Der Mathe- matik Und Ihrer Grenzgebiete. 2. Folge. Springer-Verlag, Berlin Heidelberg, 1975. doi:  10.1007/978-3-642-65809-9 .

 [23] O. Riordan and N. Wormald. The diameter of sparse random graphs. Combin. Probab. Comput. , 19(5-6):835–926, 2010. ISSN 0963-5483. doi:  10/dgp6hh .

 [24] D. P. Sanders, L. Benet, K. Agarwal, E. Gupta, B. Richard, M. Forets, yashrajgupta, E. Han- son, B. van Dyk, C. Rackauckas, S. Miclua-Cˆ ampeanu, T. Koolen, C. Wormell, F. A. V´ azquez, J. Grawitter, J. TagBot, K. O’Bryant, K. Carlsson, M. Piibeleht, Reno, R. Deits, S. Olver, T. Holy, kalmarek, and matsueushi. JuliaIntervals/Interval Arithmetic.jl: V0.17.5. Zenodo, June 2020. URL  https://github.com/JuliaIntervals/Interval Arithmetic.jl .

 [25] R. van der Hofstad.  Random Graphs and Complex Networks , volume 1 of  Cambridge Series in Statistical and Probabilistic Mathematics . Cambridge University Press, Cambridge, England, 2016. doi:  10.1017/9781316779422 .

 [26] P. van der Hoorn and M. Olvera-Cravioto. Typical distances in the directed conﬁgura- tion model.  Ann. Appl. Probab. , 28(3):1739–1792, June 2018. ISSN 1050-5164, 2168-8737. doi:  10/ggh2ch .  