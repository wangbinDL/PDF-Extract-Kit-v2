# Partition-Insensitive Parallel ADMM Algorithm for High-dimensional Linear Models 

Xiaofei Wu<br>College of Mathematics and Statistics, Chongqing University,
Jiancheng Jiang<br>Department of Mathematics and Statistics, University of North Carolina at Charlotte,


#### Abstract

The parallel alternating direction method of multipliers (ADMM) algorithms have gained popularity in statistics and machine learning due to their efficient handling of large sample data problems. However, the parallel structure of these algorithms, based on the consensus problem, can lead to an excessive number of auxiliary variables when applied to highdimensional data, resulting in large computational burden. In this paper, we propose a partition-insensitive parallel framework based on the linearized ADMM (LADMM) algorithm and apply it to solve nonconvex penalized high-dimensional regression problems. Compared to existing parallel ADMM algorithms, our algorithm does not rely on the consensus problem, resulting in a significant reduction in the number of variables that need to be updated at each iteration. It is worth noting that the solution of our algorithm remains largely unchanged regardless of how the total sample is divided, which is known as partition-insensitivity. Furthermore, under some mild assumptions, we prove the convergence of the iterative sequence generated by our parallel algorithm. Numerical experiments on synthetic and real datasets demonstrate the feasibility and validity of the proposed algorithm. We provide a publicly available R software package to facilitate the implementation of the proposed algorithm.


Keywords: Global convergence; Nonconvex optimization; Parallel algorithm; Robust regression; LADMM

[^0]
[^0]:    ${ }^{*}$ Corresponding author. Email: zmzhang@cqu.edu.cn. The research of Zhimin Zhang was supported by the National Natural Science Foundation of China [Grant Numbers 12271066, 12171405, 11871121], and the research of Xiaofei Wu was supported by the project of science and technology research program of Chongqing Education Commission of China [Grant Numbers KJQN202302003].




---

# 1 Introduction 

High-dimensional linear regression models can be written as the form of " loss + penalty ",

$$
\arg \min _{\beta \in \mathbb{R}^{p}} \sum_{i=1}^{n} L\left(y_{i}-x_{i}^{\top} \beta\right)+P_{\lambda}(|\beta|)
$$

where $x_{i} \in \mathbb{R}^{p}$ and $y_{i} \in \mathbb{R}$ denote the $i$-th observation values of the covariates and the response, respectively. The loss function $L: \mathbb{R} \rightarrow \mathbb{R}_{+}$is a versatile function that encompasses various forms, such as least squares, asymmetric least squares (Newey and Powell (1987)), Huber loss (Huber (1964)), quantile loss (Koenker and Basset (1978)), and smooth quantile loss (Jennings et al. (1993) and Aravkin et al. (2014)). $P_{\lambda}(|\beta|)$ represents various nonconvex penalty terms including SCAD (Fan and Li (2001)), MCP (Zhang (2010a)), Capped- $\ell_{1}$ (Zhang (2010b)), Snet (Wang et al. (2010)), Mnet (Huang et al. (2016)), among others.

Over the past decade, many parallel ADMM algorithms have been proposed to solve (1) when dealing with particularly large samples ( $n$ is large) and/or distributed data storage. Examples include Boyd et al. (2010), Yu and Lin (2017), Yu et al. (2017), and Fan et al. (2021). The parallel algorithm is based on the principle of decomposing a complex optimization problem into smaller and more manageable subproblems that can be solved independently and simultaneously on local machines. Assuming there are $M$ local machines available, the data $y=\left(y_{1}, y_{2}, \ldots, y_{n}\right)^{\top}$ and $X=\left(x_{1}, x_{2}, \ldots, x_{n}\right)^{\top}$ can be divided into $M$ blocks as,

$$
y=\left(y_{1}^{\top}, y_{2}^{\top}, \ldots, y_{M}^{\top}\right)^{\top}, X=\left(X_{1}^{\top}, X_{2}^{\top}, \ldots, X_{M}^{\top}\right)^{\top}
$$

Here, $y_{m} \in \mathbb{R}^{n_{m}}, X_{m} \in \mathbb{R}^{n_{m} \times p}$, and $\sum_{m=1}^{M} n_{m}=n$. In order to adapt to this parallel structure, the existing parallel ADMM algorithms employ strategies similar to the regularized consensus problem described by Boyd et al. (2010). The regularized consensus problem is a method in machine learning and optimization for solving consensus problems in a distributed environment. It aims to achieve cooperation among distributed machines by running local optimization algorithms with regularization terms to promote model consistency and consensus.

By introducing $\left\{r_{m}=y_{m}-X_{m} \beta_{m}\right\}_{m=1}^{M}$ and the consensus constraints $\left\{\beta=\beta_{m}\right\}_{m=1}^{M}$, the




---

constrained optimization problem of (1) can be expressed as,

$$
\begin{array}{r}
\min _{\boldsymbol{\beta},\left\{\boldsymbol{r}_{m}, \boldsymbol{\beta}_{m}\right\}_{m=1}^{M}} \sum_{m=1}^{M} L\left(\boldsymbol{r}_{m}\right)+P_{\lambda}(|\boldsymbol{\beta}|) \\
\text { s.t. } \boldsymbol{X}_{m} \boldsymbol{\beta}_{m}+\boldsymbol{r}_{m}=\boldsymbol{y}_{m}, \boldsymbol{\beta}=\boldsymbol{\beta}_{m}, m=1,2, \ldots, M
\end{array}
$$

where $\boldsymbol{r}_{m}=\left(r_{m, 1}, r_{m, 2}, \ldots, r_{m, n_{m}}\right)^{\top}$ and $L\left(\boldsymbol{r}_{m}\right)=\sum_{l=1}^{n_{m}} L\left(r_{m, l}\right)$. To solve the constrained optimization problem in (3), both the central and local machines must handle subproblems of the same dimension as $\boldsymbol{\beta}$ (which is $p$ dimensions) in each iteration. However, in the case of penalized regression, it is common for $\boldsymbol{\beta}$ to have a relatively large dimension. As $\boldsymbol{r}=\left(\boldsymbol{r}_{1}^{\top}, \boldsymbol{r}_{2}^{\top}, \ldots, \boldsymbol{r}_{m}^{\top}\right)^{\top}$ and each local machine needs to solve its corresponding $\boldsymbol{\beta}_{m}$, solving parallel problems results in solving at least $M p$ subproblems more than the nonparallel case.

Although this phenomenon can be mitigated through parallel computing structures, where $M$ local machines compute $M p$ subproblems in an equivalent time to a single machine computing $p$ subproblems, the presence of these additional auxiliary variables $\left\{\boldsymbol{\beta}_{m}\right\}_{m=1}^{M}$ can negatively impact the convergence speed and accuracy of the algorithm. This drawback has also been emphasized by Lin et al. (2022), which suggested avoiding excessive auxiliary variables when designing the ADMM algorithm. Moreover, it is important to note that $\left\{\boldsymbol{\beta}_{m}\right\}_{m=1}^{M}$ is not the desired estimation value we initially aim for, but rather auxiliary variables added to avoid the need for double loops and facilitate a parallel structure. More details can be found in Yu et al. (2017). Hence, a natural question arises: Is it possible to design an ADMM algorithm without introducing these auxiliary variables, while also maintaining a parallel structure and avoiding double loops?

This paper proposes a new parallel strategy based on the LADMM to address the above problem. Compared to existing related algorithms, the proposed algorithm offers three main advantages:

- Not an approximation algorithm: Our algorithm directly tackles nonnonvex optimization problems without relying on methods like local linear approximation (LLA) in Zou and Li (2008), majorization minimization step in Yu et al. (2017), or iterative reweighting in Pan et al. (2021). For smooth quantile losses, our algorithm directly derives its proximal operator, rather than using first-order approximation methods in Mkhadri et al. (2017). Additionally, we prove that our algorithm converges globally to a critical point of the nonconvex optimization problem.




---

- Fewer iterative variables: Compared to existing parallel ADMM algorithms, our parallel LADMM algorithm requires fewer iterative variables. Existing algorithms typically handle subproblems with dimensions of at least $(2 M+1) p+2 n$, while our algorithm only requires $p+2 n$. This reduction in the number of variables is especially significant when $p$ and/or $M$ are large.
- Partition-Insensitive: As stated by Fan et al. (2021), when more local machines are employed, the parallel algorithms proposed by Yu et al. (2017) and Fan et al. (2021) tend to select more non-zero variables. In contrast, our algorithm ensures that the solution remains largely consistent across different numbers of local machines and the number of samples they process. This consistency guarantees the reliability and accuracy of the solutions, regardless of the chosen parallelization strategy.

To ensure the proposed algorithm's wide applicability, we opt for a highly flexible smooth quantile loss, denoted as $L$ in (1), and employ general nonconvex penalties such as Snet and Mnet for $P_{\lambda}(|\beta|)$ in (1). The remainder of this paper is organized as follows. In Section 2, we introduce two smooth quantile loss functions and some nonconvex penalties, and derives the closed-form solutions of their proximal operators. In Section 3, we propose the LADMM algorithm for solving nonconvex penalized regression problems and derive its parallel version. Besides, we explained the equivalence relationship between parallel and nonparallel algorithms and provide the convergence of the algorithms in this section. In Section 4, we present the performance of the proposed algorithm on some synthetic datasets, and compare it with existing methods. In Section 5, we utilize the new parallel algorithm to conduct regressions on an online publicly available dataset, leading to the discovery of several new findings. In Section 6, we summarize the findings and concludes the paper with a discussion of future research directions. The technical proofs and supplementary numerical experimentsis are included in the Appendix. R package PIPADMM for implementing the partition-insensitive parallel LADMM is available at https://github.com/xfwu1016/PIPADMM.

# 2 Preliminary 

In this section, we introduce two smooth quantile loss functions and several nonconvex penalties, and combine them to create the generalized nonconvex penalized regression models. To facilitate




---

the description of the algorithm, we provide closed-form solutions for the proximal operators of both the smooth quantile functions and nonconvex penalties.

# 2.1 Smooth Quantile Losses 

The quantile loss function, proposed by Koenker and Basset (1978), is defined as

$$
\rho_{\tau}(u)=u\left(\tau-1_{u<0}\right)
$$

where $\tau \in(0,1)$ and $1_{u<0}$ is an indicator function, which is equal to 1 if $u<0$, and 0 otherwise. This loss function has been widely studied in linear regression because it can go beyond the typical Gaussian assumptions and effectively resolve heavy-tailed random error distributions. However, the traditional quantile loss function may not be suitable for scenarios with noise and outliers, as it aims for precise fitting of a specific section of the data. Studies conducted by Aravkin et al. (2014) and Mkhadri et al. (2017) have shown the limitations of the traditional quantile loss function and suggested using the smooth quantile loss as a better alternative.

The two popular smooth quantile losses proposed by Jennings et al. (1993) and Aravkin et al. (2014) are respectively defined as

$$
L_{\tau, c}(u)= \begin{cases}\tau(u-0.5 c) & \text { if } u \geq c \\ \frac{\tau u^{2}}{2 c} & \text { if } u \in[0, c) \\ \frac{(1-\tau) u^{2}}{2 c} & \text { if } u \in[-c, 0) \\ (\tau-1)(u+0.5 c) & \text { if } u<-c\end{cases}
$$

and

$$
L_{\tau, \kappa}(u)= \begin{cases}\tau\left(u-\frac{\tau \kappa}{2}\right) & \text { if } u>\tau \kappa \\ \frac{u^{2}}{2 \kappa} & \text { if } u \in[(\tau-1) \kappa, \tau \kappa] \\ (\tau-1)\left[u-\frac{(\tau-1) \kappa}{2}\right] & \text { if } u<(\tau-1) \kappa\end{cases}
$$

where $\tau \in(0,1), c>0$ and $\kappa>0$ are three given constants. The smooth quantile loss is termed as a generalized loss because $L_{\tau, c}(u)$ and/or $L_{\tau, \kappa}(u)$ can closely resemble the quantile loss, least squares




---

loss, asymmetric least squares loss, and Huber loss by adjusting $\tau, c$ and $\kappa$. Obviously, these two smooth quantile losses are the modified versions of the quantile loss function that incorporates smooth functions to replace the non-differentiable portion near the origin. This modification allows for differentiability at the origin, making it easier to analyze theoretically and design algorithms. One manifestation is that $L_{\tau, *}(u)(*$ represents $c$ or $\kappa)$ has a Lipschitz continuous first derivative, while quantile loss does not.

Because of these advantages and flexibility, the smooth quantile losses are widely used in various fields, such as nonparametric regression (Oh et al. (2011)), functional data clustering (Kim and Oh (2020)), linear regression (Zheng (2011)), and penalized linear regression (Ouhourane et al. (2022)). Several algorithms have been proposed to handle smooth quantile regression (with or without penalty terms), including gradient descent (Zheng (2011)), orthogonal matching pursuit (Aravkin et al. (2014)) and coordinate descent (Mkhadri et al. (2017)). It is important to note that the aforementioned algorithms primarily concentrate on convex penalties. Although Mkhadri et al. (2017) extended the algorithm to nonconvex cases, its theoretical convergence is not guaranteed. To the best of our knowledge, no ADMM algorithm has been introduced specifically for solving regression models with smooth quantile regression. The first use of parallel ADMM algorithm to solve regularized smooth quantile regression is also a contribution of this paper.

# 2.2 Nonconvex Penalized Smooth Quantile Regression Models 

Next, we introduce two generalized nonconvex penalties, namely Snet (SCAD $+\ell_{2}$ ) in Wang et al. (2010), and Mnet (MCP $+\ell_{2}$ ) in Huang et al. (2016), and additionally propose a novel nonconvex penalty, called Cnet (Capped- $\ell_{1}+\ell_{2}$ ). Capped- $\ell_{1}$ is a nonconvex regularization proposed by Zhang (2010b) and has garnered attention in Guan et al. (2018) and Pan et al. (2021). It is defined as $\operatorname{Cap-L} 1_{a, \lambda_{1}}(\beta)=\lambda_{1} \sum_{j} \min \left\{\left|\beta_{j}\right|, a\right\}$. Snet, Mnet and Cnet are defined separately as,

$$
\begin{aligned}
& \operatorname{Snet}(\beta)=\operatorname{SCAD}_{a, \lambda_{1}}(\beta)+\frac{\lambda_{2}}{2}\|\beta\|_{2}^{2} \\
& \operatorname{Mnet}(\beta)=\operatorname{MCP}_{a, \lambda_{1}}(\beta)+\frac{\lambda_{2}}{2}\|\beta\|_{2}^{2} \\
& \operatorname{Cnet}(\beta)=\operatorname{Cap}-\mathrm{L} 1_{a, \lambda_{1}}(\beta)+\frac{\lambda_{2}}{2}\|\beta\|_{2}^{2}
\end{aligned}
$$

where $a$ is a constant determined by the first regularization term, and its suggested value can be found in papers that proposed these regularization terms.




---

Combining the smooth quantile loss and the above generalized nonconvex penalty together produces a generalized nonconvex penalized regression model,

$$
\arg \min _{\beta \in \mathbb{R}^{p}} \sum_{i=1}^{n} L_{\tau, *}\left(y_{i}-x_{i}^{\top} \beta\right)+P_{\lambda}(|\beta|)
$$

We abbreviate it as NPSQR. NPSQR is a special case of regularized M-estimator, whose statistical properties have been studied in Negahban et al. (2010), Li et al. (2011), Loh and Wainwright (2015), Loh (2017) and Zhou et al. (2018).

# 2.3 Proximal Operator 

In iterative algorithms, the existence of closed-form solutions for the proximal operator has a significant impact on the algorithm efficiency. To facilitate the closed-form solutions for the updates of $\beta$ and $r$ in the ADMM algorithm, we introduce the following proximal operators,

$$
\operatorname{prox}_{\mu, g}(v)=\arg \min _{u}\left\{g(u)+\frac{\mu}{2}\|u-v\|_{2}^{2}\right\}
$$

where $g(u)$ is a nonnegative real-valued function, $v$ is a constant vector, and $\mu>0$ is a constant. In this paper, the function $g(u)$ is separable or additive, meaning $g(u)=\sum_{j} g\left(u_{j}\right)$. Then, we can divide the optimization problem (7) into multiple independent univariate problems. Here, we introduce the closed-form solutions for the proximal operators of the aforementioned losses and penalties, which play a crucial role in our algorithm. The detailed derivation process of these closed-form solutions is provided in Appendix A. We are not aware of any prior work that has derived closed-form solutions for these operators (except for some special cases with Snet in Wang et al. (2010) and Mnet in Huang et al. (2016)), making our contribution novel in this regard. Furthermore, our derivation method is concise and easily extendable, enabling potential applications beyond the scope of this study.




---

- The closed-form solution for proximal operator of $L_{\tau, c}(u)$

$$
\operatorname{prox}_{\mu, L_{\tau, c}}\left(v_{j}\right)= \begin{cases}v_{j}-\frac{\tau}{\mu}, & \text { if } v_{j} \geq c+\frac{\tau}{\mu} \\ \frac{c \mu v_{j}}{c \mu+\tau}, & \text { if } 0 \leq v_{j}<c+\frac{\tau}{\mu} \\ \frac{c \mu v_{j}}{c \mu+1-\tau}, & \text { if }-c+\frac{\tau-1}{\mu} \leq v_{j}<0 \\ v_{j}-\frac{\tau-1}{\mu}, & \text { if } v_{j}<-c+\frac{\tau-1}{\mu}\end{cases}
$$

- The closed-form solution for proximal operator of $L_{\tau, \kappa}(u)$

$$
\operatorname{prox}_{\mu, L_{\tau, \kappa}}\left(v_{j}\right)= \begin{cases}v_{j}-\frac{\tau}{\mu}, & \text { if } v_{j} \geq \tau \kappa+\frac{\tau}{\mu} \\ \frac{\kappa \mu v_{j}}{\kappa \mu+1}, & \text { if }(\tau-1) \kappa+\frac{\tau-1}{\mu} \leq v_{j}<\tau \kappa+\frac{\tau}{\mu} \\ v_{j}-\frac{\tau-1}{\mu}, & \text { if } v_{j}<(\tau-1) \kappa+\frac{\tau-1}{\mu}\end{cases}
$$

Obviously, when $c=0$ and $\kappa=0$, the proximal operator of quantile loss in Yu et al. (2017) and Gu et al. (2018) is a special case in (8) or (9).
- $\quad$ The closed-form solution for proximal operator of Snet with $(a-1)\left(\eta+\lambda_{2}\right)>1$ :

$$
\operatorname{prox}_{\eta, P_{\lambda}}\left(v_{j}\right)= \begin{cases}\operatorname{sign}\left(v_{j}\right) \cdot\left[\frac{\eta\left|v_{j}\right|-\lambda_{1}}{\eta+\lambda_{2}}\right]^{+}, & \text { if }\left|v_{j}\right| \leq \frac{\lambda_{1}\left(1+\eta+\lambda_{2}\right)}{\eta} \\ \operatorname{sign}\left(v_{j}\right) \cdot\left[\frac{(a-1) \eta\left|v_{j}\right|-a \lambda_{1}}{(a-1)\left(\eta+\lambda_{2}\right)-1}\right]^{+}, & \text { if } \frac{\lambda_{1}\left(1+\eta+\lambda_{2}\right)}{\eta}<\left|v_{j}\right|<\frac{a \lambda_{1}\left(\eta+\lambda_{2}\right)}{\eta} \\ \frac{\eta v_{j}}{\eta+\lambda_{2}}, & \text { if }\left|v_{j}\right| \geq \frac{a \lambda_{1}\left(\eta+\lambda_{2}\right)}{\eta}\end{cases}
$$

Obviously, when $\lambda_{2}=0$ and $\eta=1$, the closed-form solution for proximal operator of SCAD in Fan and Li (2001) is a special case in (10). Besides, when $\eta=1$, the solution in (10) is the same as the closed-form solution mentioned in Wang et al. (2010). Note that $(a-1)\left(\eta+\lambda_{2}\right)>1$ is also valid in this paper, as $a=3.7$ and $\eta>>1$.
- $\quad$ The closed-form solution for proximal operator of Mnet:

$$
\operatorname{prox}_{\eta, P_{\lambda}}\left(v_{j}\right)= \begin{cases}\operatorname{sign}\left(v_{j}\right) \cdot\left[\frac{a \eta\left|v_{j}\right|-a \lambda_{1}}{a\left(\eta+\lambda_{2}\right)-1}\right]^{+}, & \text { if }\left|v_{j}\right|<\frac{a \lambda_{1}\left(\eta+\lambda_{2}\right)}{\eta} \\ \frac{\eta v_{j}}{\eta+\lambda_{2}}, & \text { if }\left|v_{j}\right| \geq \frac{a \lambda_{1}\left(\eta+\lambda_{2}\right)}{\eta}\end{cases}
$$

Obviously, when $\lambda_{2}=0$ and $\eta=1$, the proximal operator of MCP in Zhang (2010a) is a special




---

case in (11). In addition, when $\eta=1$, the solution in (11) is the same as the solution mentioned in Huang et al. (2016).

- The closed-form solution for proximal operator of Cnet:

$$
\operatorname{prox}_{\eta, P_{\lambda}}\left(v_{j}\right)= \begin{cases}\operatorname{sign}\left(v_{j}\right) \cdot\left[\eta\left|v_{j}\right|-\frac{\lambda_{1}}{\eta+\lambda_{2}}\right]_{+}, & \text { if }\left|v_{j}\right|<\frac{a\left(\eta+\lambda_{2}\right)}{\eta} \\ \frac{\eta v_{j}}{\eta+\lambda_{2}}, & \text { if }\left|v_{j}\right| \geq \frac{a\left(\eta+\lambda_{2}\right)}{\eta}\end{cases}
$$

In addition to these two smooth quantile losses and three nonconvex combined penalties, our PIPADMM package also offers closed-form solutions for proximal operators of least squares loss and its asymmetric variant, quantile regression loss, Huber loss, and elastic-net regularization (Zou and Hastie (2005)). Naturally, our algorithm (R package) can also be applied to solve these regression problems with elastic-net. Some of these proximal operators have already obtained the closed-form solutions from previous research in Liang et al. (2024) and its cited literature, and the closed-form solutions of all the proximal operators mentioned above can be derived using the similar methods as presented in this paper. As a result, we will not delve into detailed introductions for them.

# 3 The Parallel LADMM Algorithm 

The alternating direction method of multipliers (ADMM) is an algorithm employed to solve problems characterized by a two-block separable objective function with equality constraints. By setting $r=y-X \beta$, the given NPSQR problems in (6) can be transformed into the following form,

$$
\min _{\beta, r} L_{\tau, *}(r)+P_{\lambda}(|\beta|), \quad \text { s.t. } \quad X \beta+r=y
$$

where $L_{\tau, *}(r)=\sum_{i=1}^{n} L_{\tau, *}\left(r_{i}\right)$. To solve NPSQR using ADMM, the augmented Lagrangian of (13) needs to be formulated as,

$$
\mathcal{L}_{\mu}(\beta, r, d)=L_{\tau, *}(r)+P_{\lambda}(|\beta|)-d^{\top}(X \beta+r-y)+\frac{\mu}{2}\|X \beta+r-y\|_{2}^{2}
$$




---

Here, $d \in \mathbb{R}^{n}$ represents the dual variable, and $\mu>0$ is a tunable augmentation parameter. The ADMM algorithm alternates between the following iterative steps with given $r_{0}$ and $d_{0}$,

$$
\left\{\begin{aligned}
\beta_{k+1} & =\arg \min _{\beta}\left\{P_{\lambda}(|\beta|)+\frac{\mu}{2}\left\|X \beta+r_{k}-y-d_{k} / \mu\right\|_{2}^{2}\right\} \\
r_{k+1} & =\arg \min _{r}\left\{L_{\tau, *}(r)+\frac{\mu}{2}\left\|X \beta_{k+1}+r-y-d_{k} / \mu\right\|_{2}^{2}\right\} \\
d_{k+1} & =d_{k}-\mu\left(X \beta_{k+1}+r_{k+1}-y\right)
\end{aligned}\right.
$$

Similar iterative schemes have been used by Yu et al. (2017) and Gu et al. (2018) to solve the convex penalized quantile regression. However, the $\beta$-update in (15) is a regression problem similar to penalized least squares regression, which does not have a closed-form solution. This update is computationally expensive because it necessitates numerical optimization techniques like the coordinate descent algorithm, leading to a double-loop algorithm. Next, we will introduce the LADMM algorithm to address this challenge.

# 3.1 LADMM Algorithm 

Linearized ADMM (LADMM) algorithms have found wide application in convex sparse statistical learning models, including the Dantzig selector in Wang and Yuan (2012), sparse group and fused Lasso methods in Li et al. (2014), sparse quantile regression in Gu et al. (2018), and sparse support vector machines in Liang et al. (2024). The fundamental idea of the LADMM algorithm involves linearizing the quadratic term using a nontrivial matrix, thereby creating a proximal operator. We solve (13) using LADMM through the following iteration,

$$
\left\{\begin{aligned}
\beta_{k+1} & =\arg \min _{\beta}\left\{P_{\lambda}(|\beta|)+\frac{\mu}{2}\left\|X \beta+r_{k}-y-d_{k} / \mu\right\|_{2}^{2}+\frac{1}{2}\left\|\beta-\beta_{k}\right\|_{S}^{2}\right\} \\
r_{k+1} & =\arg \min _{r}\left\{L_{\tau, *}(r)+\frac{\mu}{2}\left\|X \beta_{k+1}+r-y-d_{k} / \mu\right\|_{2}^{2}\right\} \\
d_{k+1} & =d_{k}-\mu\left(X \beta_{k+1}+r_{k+1}-y\right)
\end{aligned}\right.
$$

where $S=\eta I_{p}-\mu X^{\top} X$ and $\left\|\beta-\beta_{k}\right\|_{S}^{2}=\left(\beta-\beta_{k}\right)^{\top} S\left(\beta-\beta_{k}\right)$. To ensure the convergence of the algorithm, we need $S$ to be a positive-definite matrix, i.e., $\eta>\operatorname{eigen}\left(\mu X^{\top} X\right)$, where $\operatorname{eigen}\left(\mu X^{\top} X\right)$ denotes the maximum eigenvalue of $\mu X^{\top} X$.




---

Pre-computation: $\eta=$ eigen $\left(\mu X^{\top} X\right)$.
Input: Observation data: $X, y$; initial primal variables: $\beta^{0}, r^{0}$; initial dual variables $d^{0}$; augmented parameters: $\mu$; penalty parameter: $\lambda_{1}, \lambda_{2}$; selected parameters: $\tau \in(0,1), c>0$ or $\kappa>0$.
Output: The last iteration solution $\beta^{K}$.
while not converged do

1. Update $\beta_{j}^{k+1}=\operatorname{prox}_{\eta, P_{\lambda}}\left(\beta_{j}^{k}-\left[\mu X^{\top}\left(X \beta^{k}+r^{k}-y-d^{k} / \mu\right)\right]_{j} / \eta\right), j=1,2, \ldots, p$,
2. Update $r_{i}^{k+1}=\operatorname{prox}_{\mu, \mathcal{L}_{\tau, *}}\left(\left[y+d^{k} / \mu-X \beta^{k+1}\right]_{i}\right), i=1,2, \ldots, n$,
3. Update $d^{k+1}=d^{k}-\mu\left(X \beta^{k+1}+r^{k+1}-y\right)$.

end while
return solution
For the $\beta$-update, omitting constant terms unrelated to $\beta$, the subproblem becomes

$$
\beta^{k+1}=\underset{\beta}{\operatorname{argmin}}\left\{P_{\lambda}(|\beta|)+\frac{\eta}{2}\left\|\beta-\beta^{k}+\frac{\mu X^{\top}\left(X \beta^{k}+r^{k}-y-d^{k} / \mu\right)}{\eta}\right\|_{2}^{2}\right\}
$$

Obviously, both $\beta^{k+1}$ and $r^{k+1}$ can be represented as proximal operators, that is,

$$
\left\{\begin{array}{l}
\beta^{k+1}=\operatorname{prox}_{\eta, P_{\lambda}}\left(\beta^{k}-\frac{\mu X^{\top}\left(X \beta^{k}+r^{k}-y-d^{k} / \mu\right)}{\eta}\right) \\
r^{k+1}=\operatorname{prox}_{\mu, \mathcal{L}_{\tau, *}}\left(y+d^{k} / \mu-X \beta^{k+1}\right)
\end{array}\right.
$$

To summarize, the iterative scheme of LADMM for (13) is described in Algorithm 1 .

# 3.2 Parallel LADMM Algorithm 

This subsection delves into the implementation of parallel LADMM algorithms designed for handling large-scale data. The main objective is to address the issue in a distributed manner, with each processor tasked with handling a subset of the training data. This distributed approach offers significant advantages when facing with a high volume of training examples that cannot be practically processed on a single machine, or when the data is inherently distributed or stored in a similar manner.

We divide $X$ and $y$ into $M$ row blocks using the following approach:

$$
y=\left(y_{1}^{\top}, y_{2}^{\top}, \ldots, y_{M}^{\top}\right)^{\top}, \quad X=\left(X_{1}^{\top}, X_{2}^{\top}, \ldots, X_{M}^{\top}\right)^{\top}
$$

where $X_{m} \in \mathbb{R}^{n_{m} \times p}$ and $y_{m} \in \mathbb{R}^{n_{m}}$, with $\sum_{m=1}^{M} n_{m}=n$. Here, $X_{m}$ and $y_{m}$ denote the $m$-th block




---


of data, which will be processed by the $m$-th local machine. Correspondingly, $r$ and $d$ can also be divided into $M$ blocks:

$$
r=\left(r_{1}^{\top}, r_{2}^{\top}, \ldots, r_{M}^{\top}\right)^{\top}, \quad d=\left(d_{1}^{\top}, d_{2}^{\top}, \ldots, d_{M}^{\top}\right)^{\top}
$$

Then, we can rewrite problem (13) into the following equivalent problem:

$$
\begin{aligned}
\min _{\beta,\left\{r_{m}\right\}_{m=1}^{M}} & \sum_{m=1}^{M} \sum_{l=1}^{n_{m}} L_{\tau, *}\left(r_{m, l}\right)+P_{\lambda}(|\beta|) \\
\text { s.t. } & X_{m} \beta+r_{m}=y_{m}, \quad m=1,2, \ldots, M
\end{aligned}
$$

where $r_{m}=\left(r_{m, 1}, r_{m, 2}, \ldots, r_{m, n_{m}}\right)^{\top}$. Similar to (14), the augmented Lagrangian for problem (21) is given by

$$
\begin{aligned}
\mathcal{L}_{\mu}\left(\beta,\left\{r_{m}\right\}_{m=1}^{M},\left\{d_{m}\right\}_{m=1}^{M}\right)= & \sum_{m=1}^{M} \sum_{l=1}^{n_{m}} L_{\tau, *}\left(r_{m, l}\right)+P_{\lambda}(|\beta|)-\sum_{m=1}^{M} d_{m}^{\top}\left(X_{m} \beta+r_{m}-y_{m}\right) \\
& +\frac{\mu}{2} \sum_{m=1}^{M}\left\|X_{m} \beta+r_{m}-y_{m}\right\|_{2}^{2}
\end{aligned}
$$

where $d_{m} \in \mathbf{R}^{n_{m}}$ is the dual variable. Then, we can solve (21) using parallel LADMM through the




---

following iteration,

$$
\left\{\begin{array}{l}
\beta^{k+1}=\underset{\beta}{\operatorname{argmin}}\left\{P_{\lambda}(|\beta|)+\frac{\mu}{2} \sum_{m=1}^{M}\left\|X_{m} \beta+r_{m}^{k}-y_{m}-d_{m}^{k} / \mu\right\|_{2}^{2}+\frac{1}{2}\left\|\beta-\beta^{k}\right\|_{S}^{2}\right\} \\
r_{m}^{k+1}=\underset{r_{m}}{\operatorname{argmin}}\left\{n \sum_{l=1}^{P_{n}} L_{\tau, *}\left(r_{m, l}\right)+\frac{\mu}{2}\left\|X_{m} \beta^{k+1}+r_{m}-y_{m}-d_{m}^{k} / \mu\right\|_{2}^{2}\right\}, m=1,2, \ldots, M \\
d_{m}^{k+1}=d_{m}^{k}-\mu\left(X_{m} \beta^{k+1}+r_{m}^{k+1}-y_{m}\right), m=1,2, \ldots, M
\end{array}\right.
$$

where $S=\eta I_{p}-\mu \sum_{m=1}^{M} X_{m}^{\top} X_{m}=\eta I_{p}-\mu X^{\top} X$. The steps that need to execute parallel processes in iteration are $r_{m}$ and $d_{m}$. However, the central machine updating $\beta$ encounters two issues due to not loading data. The first issue is that the update of $\beta$ requires loading the collected data $X, y$. The second one is that the calculation of $\eta$ also depends on the collected data.

The above two issues seem to make this parallel LADMM algorithm impossible to implement. Fortunately, continuing to derive the specific iteration steps for $\beta$, we found that both of these issues can be solved. Similar to (18), the iteration of $\beta$ and $r_{m}$ can be represented as

$$
\left\{\begin{array}{l}
\beta^{k+1}=\operatorname{prox}_{\eta, P_{\lambda}}\left(\beta^{k}-\frac{\mu \sum_{m=1}^{M} X_{m}^{\top}\left(X_{m} \beta^{k}+r_{m}^{k}-y_{m}-d_{m}^{k} / \mu\right)}{\eta}\right) \\
r_{m}^{k+1}=\operatorname{prox}_{\mu, L_{\tau, *}}\left(y_{m}+d_{m}^{k} / \mu-X_{m} \beta^{k+1}\right), m=1,2, \ldots, M
\end{array}\right.
$$

Let $\xi_{m}^{k}=X_{m}^{\top}\left(X_{m} \beta^{k}+r_{m}^{k}-y_{m}-d_{m}^{k} / \mu\right), m=1,2, \ldots, M$. From the iteration sequence, it can be seen that in the $k+1$ iteration, $\xi_{m}^{k}$ can be generated by each local machine and transmitted to the central machine. Then,

$$
\beta^{k+1}=\operatorname{prox}_{\eta, P_{\lambda}}\left(\beta^{k}-\frac{\mu}{\eta} \sum_{m=1}^{M} \xi_{m}^{k}\right)
$$

Therefore, the first issue caused by the central machine not loading data has been resolved. The second issue is the calculation of $\eta$. Each local machine only loads a portion of the data, which means we can only obtain $\eta_{m}=\operatorname{eigen}\left(\mu X_{m}^{\top} X_{m}\right)$ on each local machine. Note that $\sum_{m=1}^{M} \eta_{m} \geq \operatorname{eigen}\left(\mu X^{\top} X\right)$. Then, in the specific implementation of the parallel algorithm, we can choose $\sum_{m=1}^{M} \eta_{m}$ as the value for $\eta$. In practical operation, we only need to calculate $\eta_{m}$ once at the beginning of the iteration and then pass it on to the central machine. Therefore, both issues related to the implementation of parallel LADMM have been resolved.

To sum up, the iterative scheme of parallel LADMM for (13) can be described in Algorithm 2




---

and visualized Figure 1.


# Pre-computation: $\eta_{m}=$ eigen $\left(\mu \mathbf{X}_{m}^{\top} \mathbf{X}_{m}\right), m=1,2, \ldots, M$. 

Input: - Central machine: augmented parameter $\mu$; tuning parameters $\lambda_{1}$ and $\lambda_{2} ; \eta=\sum_{m=1}^{M} \eta_{m}$; and initial iteration variable $\boldsymbol{\beta}^{0}$.

- The $m$-th local machine: observation data $\left\{\mathbf{X}_{m}, \mathbf{y}_{m}\right\}_{m=1}^{M}$; augmented parameter $\mu$; selected parameters $\tau \in(0,1)$, and $c>0$ or $\kappa>0$; and initial iteration variables $\boldsymbol{\beta}^{0}, \mathbf{r}_{m}^{0}, \mathbf{d}_{m}^{0}$ and $\boldsymbol{\xi}_{m}^{0}=$ $\mathbf{X}_{m}^{\top}\left(\mathbf{X}_{m} \boldsymbol{\beta}^{0}+\mathbf{r}_{m}^{0}-\mathbf{y}_{m}-\mathbf{d}_{m}^{0} / \mu\right)$

Output: The last iteration solution $\boldsymbol{\beta}^{K}$.
while not converged do
Central machine: 1. Receive $\eta_{m}$ (only once) and $\boldsymbol{\xi}_{m}^{k}$ transmitted by $M$ local machines,
2. Update $\beta_{j}^{k+1}=\operatorname{prox}_{\eta, P_{\lambda}}\left(\beta_{j}^{k}-\left(\mu \sum_{m=1}^{M} \boldsymbol{\xi}_{m}^{k}\right)_{j} / \eta\right), j=1,2, \ldots, p$,
3. Send $\boldsymbol{\beta}^{k+1}$ to the local machines.

Local machines:
for $m=1,2, \ldots, M$ (in parallel)

1. Receive $\boldsymbol{\beta}^{k+1}$ transmitted by the central machine,
2. Update $r_{m, l}^{k+1}=\operatorname{prox}_{\mu, L_{\tau}, *}\left(\left[\mathbf{y}_{m}+\mathbf{d}_{m}^{k} / \mu-\mathbf{X}_{m} \boldsymbol{\beta}^{k+1}\right]_{l}\right), m=1,2, \ldots, M$,
$l=1,2, \ldots, n_{m}$,
3. Update $\mathbf{d}_{m}^{k+1}=\mathbf{d}_{m}^{k}-\mu\left(\mathbf{X}_{m} \boldsymbol{\beta}^{k+1}+\mathbf{r}_{m}^{k+1}-\mathbf{y}_{m}\right), m=1,2, \ldots, M$,
4. Calculate $\boldsymbol{\xi}_{m}^{k+1}=\mathbf{X}_{m}^{\top}\left(\mathbf{X}_{m} \boldsymbol{\beta}^{k+1}+\mathbf{r}_{m}^{k+1}-\mathbf{y}_{m}-\mathbf{d}_{m}^{k+1} / \mu\right), m=1,2, \ldots, M$,
5. Send $\boldsymbol{\xi}_{m}^{k+1}$ to the central machine.

## end while

return solution.
Next, we will theoretically describe the relationship between Algorithm 1 and Algorithm 2. In fact, when $M=1$, the two algorithms are the same, so the following discussion focuses on the case of $M \geq 2$ in Algorithm 2. For ease of distinction, let us denote $\left\{\hat{\boldsymbol{\beta}}^{k}, \hat{\mathbf{r}}^{k}, \hat{\mathbf{d}}^{k}\right\}$ as the $k$-th iteration results of Algorithm 1, and $\left\{\tilde{\boldsymbol{\beta}}^{k}, \tilde{\mathbf{r}}^{k}, \tilde{\mathbf{d}}^{k}\right\}$ as the $k$-th iteration results of Algorithm 2. Then, we can draw the following surprising conclusion.

Theorem 1 If we use the same initial iteration variables $\left(\left\{\hat{\boldsymbol{\beta}}^{0}, \hat{\mathbf{r}}^{0}, \hat{\mathbf{d}}^{0}\right\}=\left\{\tilde{\boldsymbol{\beta}}^{0}, \tilde{\mathbf{r}}^{0}, \tilde{\mathbf{d}}^{0}\right\}\right)$ and $\eta$, the iterative solutions obtained by these two algorithms are actually the same, i.e.,

$$
\left\{\hat{\boldsymbol{\beta}}^{k}, \hat{\mathbf{r}}^{k}, \hat{\mathbf{d}}^{k}\right\}=\left\{\tilde{\boldsymbol{\beta}}^{k}, \tilde{\mathbf{r}}^{k}, \tilde{\mathbf{d}}^{k}\right\}, \text { for all } k
$$

This theorem states that as long as Algorithm 1 and Algorithm 2 ( $M \geq 2$ ) adopt the same initial value and $\eta$, the iterative solution remains the same regardless of how the samples are divided. In other words, as long as $\eta$ is the same, changes in $M$ and the number of samples loaded by each machine will not affect the iterative solution of parallel LADMM. While this conclusion may not seem immediately intuitive, the proof for it is remarkably simple and is given in Appendix B.1.




---

However, as discussed above in solving the calculation problem of $\eta$ in parallel algorithms, as the number of local machines increases, the value of $\eta$ will also increase. There have been some studies on the impact of the $\eta$ size in the linearized ADMM algorithm. Their conclusion was that a large $\eta$ value causes the algorithm to converge slower, and there may be no significant changes in the convergence solution of the iteration. Interested readers can refer to the research conducted by He et al. (2020) and its references. Next, we will discuss the convergence of the algorithm.

Let us assume that $X^{\top} X>\mu I_{p}$, where $\mu>0$ is a constant. It is worth noting that this assumption is, in fact, the lower restricted eigenvalue condition that is required in many penalized linear models as mentioned in Wang et al. (2020) and the references cited therein. We now demonstrate the convergence of the LADMM algorithm, and the proof is given in Appendix B.2.

Theorem 2 Let the sequence $w^{k}=\left\{\beta^{k}, r^{k}, d^{k}\right\}$ be generated by Algorithms 1 or 2 with an arbitrary initial feasible solution $w^{0}=\left\{\beta^{0}, r^{0}, d^{0}\right\}$. Then, with the condtions $\mu>\sqrt{\frac{2 n}{\min \{c, \kappa\}}}$ and $\eta>\operatorname{eigen}\left(\mu X^{\top} X\right)$, the sequence $w^{k}$ converges to $w^{*}=\left\{\beta^{*}, r^{*}, d^{*}\right\}$, where $w^{*}$ is a critical point of $L_{\mu}$.

Remark 1 To solve the NPSQR with $L_{\tau, c}$, the condition $\mu>\sqrt{\frac{2 n \max \{\tau, 1-\tau\}}{c}}$ guarantees convergence of the algorithm. On the other hand, for the NPSQR with $L_{\tau, \kappa}$, the algorithm can converge as long as $\mu>\sqrt{\frac{2 n}{\kappa}}$. The reason behind the different requirements for $\mu$ in these two models lies in the disparity of the Lipschitz constant of the first derivative of the two smooth quantile losses.

A reasonable concern arises regarding the assumption about $\mu$ in this theorem. When $n$ is large and either $c$ or $k$ takes small values, $\mu$ may become large. However, this does not affect the convergence of the algorithm. An intuitive explanation lies in the optimization formula (14). Our loss function is not divided by $n$ like the empirical loss. Compared to the case of empirical loss, our objective function is multiplied by a factor of $n$. If we consider the normalized loss function $\sum_{m=1}^{M} \frac{\sum_{l=1}^{n_{m}} L_{\tau, *}\left(r_{m, l}\right)}{n}$ instead of the unnormalized one $\sum_{m=1}^{M} \sum_{l=1}^{n_{m}} L_{\tau, *}\left(r_{m, l}\right)$, then $\mu$ only needs to be greater than $\sqrt{2} \sqrt{\frac{n}{\min \{c, \kappa\}}}$. When $n$ is relatively large, $\mu$ may be relatively small or even close to 0 .

# 3.3 Comparison with Other Parallel ADMM Algorithms 

There is a dearth of parallel algorithms for addressing the NPSQR issue. However, recent research has introduced parallel ADMM algorithms for nonconvex penalized quantile regression (NPQR), such as Yu et al. (2017), Fan et al. (2021) and Wen et al. (2023). Ignoring some unrelated constant




---

terms, as the values of $c$ and $\kappa$ approach 0 , the function $L_{\tau, *}$ converges to the quantile loss. Consequently, our algorithm can be applied to solve the NPQR problem. To provide a more comprehensive comparison, we review several parallel ADMM algorithms that address the NPQR problem.

- QPADM in Yu et al. (2017). To have a parallel structure, auxiliary variables $\left\{\beta_{m}=\beta\right\}_{m=1}^{M}$ are added to (13). This modification transforms the constrained optimization problem into the following form,

$$
\begin{aligned}
\min _{\beta,\left\{r_{m}, \beta_{m}\right\}_{m=1}^{M}} & \sum_{m=1}^{M} \rho_{\tau}\left(r_{m}\right)+P_{\lambda}(|\beta|) \\
\text { s.t. } & X_{m} \beta_{m}+r_{m}=y_{m}, \beta_{m}=\beta, m=1,2, \ldots, M
\end{aligned}
$$

Problem (26) has the following augmented Lagrangian,

$$
\begin{aligned}
L_{\mu}\left(\beta,\left\{r_{m}, \beta_{m}, d_{m}\right\}_{m=1}^{M}\right)= & \sum_{m=1}^{M} \rho_{\tau}\left(r_{m}\right)+P_{\lambda}(|\beta|)-\sum_{m=1}^{M} d_{1 m}^{\top}\left(X_{m} \beta_{m}+r_{m}-y_{m}\right) \\
& +\frac{\mu}{2} \sum_{m=1}^{M}\left\|X_{m} \beta_{m}+r_{m}-y_{m}\right\|_{2}^{2}-\sum_{m=1}^{M} d_{2 m}^{\top}\left(\beta_{m}-\beta\right)+\frac{\mu}{2} \sum_{m=1}^{M}\left\|\beta_{m}-\beta\right\|_{2}^{2}
\end{aligned}
$$

where $d_{1 m} \in \mathbf{R}^{n_{m}}$ and $d_{2 m} \in \mathbf{R}^{p}$ are the dual variables. The parallel ADMM iterative scheme of (27) is

$$
\left\{\begin{array}{l}
\beta^{k+1}=\operatorname{argmin}_{\beta}\left\{P_{\lambda}(|\beta|)+\frac{\mu}{2} \sum_{m=1}^{M}\left\|\beta_{m}^{k}-\beta-d_{2 m}^{k} / \mu\right\|_{2}^{2}\right\} \\
r_{m}^{k+1}=\operatorname{argmin}_{r_{m}}\left\{\rho_{\tau}\left(r_{m}\right)+\frac{\mu}{2}\left\|X_{m} \beta_{m}^{k+1}+r_{m}-y_{m}-d_{1 m}^{k} / \mu\right\|_{2}^{2}\right\}, m=1,2, \ldots, M \\
\beta_{m}^{k+1}=\operatorname{argmin}_{\beta_{m}}\left\{\left\|X_{m} \beta_{m}+r_{m}^{k}-y_{m}-d_{1 m}^{k} / \mu\right\|_{2}^{2}+\left\|\beta_{m}-\beta^{k+1}-d_{2}^{k} / \mu\right\|_{2}^{2}\right\}, m=1,2, \ldots, M \\
d_{1 m}^{k+1}=d_{1 m}^{k}-\mu\left(X_{m} \beta_{m}^{k+1}+r_{m}^{k+1}-y_{m}\right), m=1,2, \ldots, M \\
d_{2 m}^{k+1}=d_{2 m}^{k}-\mu\left(\beta_{m}^{k+1}-\beta^{k+1}\right), m=1,2, \ldots, M
\end{array}\right.
$$

Note that when $M=1$, the algorithm becomes a nonparallel version of ADMM. Increasing the consensus constraint $\beta_{m}=\beta$ aids in eliminating internal loops and adapting to parallel frameworks. However, this change introduces an additional step in solving $\beta_{m}$, which involves finding the inverse of $X_{m}^{\top} X_{m}+I_{n_{m}}$. Although Yu et al. (2017) suggested using the Woodbury matrix identity to alleviate the computational burden of inverting these matrices, matrix multiplication can still be time-consuming, especially when $p$ is large. Motivated by the maximization in QICD proposed by




---

Peng and Wang (2015), QPADM transforms the nonconvex problem into weighted $\ell_{1}$ soft-shrinkage operator for $\boldsymbol{\beta}^{k+1}$-subproblem. However, this approximation may result in an excessive number of iterative steps.

- QPADMslack in Fan et al. (2021). Inspired by Guan et al. (2018), Fan et al. (2021) relaxed $r_{m}$ in (27) to $u_{m}-v_{m}$ with $u_{m} \geq 0$ and $v_{m} \geq 0(m=1,2, \ldots, M)$, and made an improvement in solving the $\boldsymbol{\beta}^{k+1}$-subproblem. These changes can reduce the iteration steps and improve the computational accuracy of QPADM in calculating NPQR. The constrained optimization problem is

$$
\begin{aligned}
& \min _{\boldsymbol{\beta},\left\{u_{m} \geq 0, v_{m} \geq 0, \beta_{m}\right\}_{m=1}^{M}} \sum_{m=1}^{M}\left(\tau u_{m}+(1-\tau) v_{m}\right)+P_{\lambda}(|\boldsymbol{\beta}|) \\
& \text { s.t. } \quad \boldsymbol{X}_{m} \beta_{m}+u_{m}-v_{m}=y_{m} \\
& \boldsymbol{\beta}_{m}=\boldsymbol{\beta}, m=1,2, \ldots, M
\end{aligned}
$$

The iteration of QPADMslack shares similar steps with that of QPADM, yet two main differences exist. Firstly, the $\boldsymbol{r}$ subproblem is replaced by the $\boldsymbol{u}$ and $\boldsymbol{v}$ subproblems. Secondly, in contrast to the approximate solution of $\boldsymbol{\beta}$ in QPADM, QPADMslack utilizes the approximate closed-form solution for SCAD and MCP, as proposed by Gong et al. (2013) and Guan et al. (2018). The term "approximate closed-form solution" means finding the minimum point by comparing the minimum value of the objective function among several candidate minimum points. By contrast, our LADMM algorithm utilizes the derived proximal operators in Section 2.3 to directly provide a closed-form solution for this nonconvex problem.

The above two algorithms are parallel algorithms specifically designed for working with sample




---

data. The QPADM algorithm necessitates iteration over $(2 M+1) p+2 n$ variables, whereas the QPADMslack algorithm requires iteration over $(2 M+1) p+3 n$ variables. When $p$ is very large, the Woodbury matrix identity that needs to be used is

$$
\left(\mathbf{X}_{m}^{\top} \mathbf{X}_{m}+\mathbf{I}_{p}\right)^{-1}=\mathbf{I}_{p}-\mathbf{X}_{m}^{\top}\left(\mathbf{X}_{m} \mathbf{X}_{m}^{\top}+\mathbf{I}_{n m}\right)^{-1} \mathbf{X}_{m}
$$

The matrix multiplication here is quite time-consuming. Referring to Algorithm 2, we observe that our LADMM algorithm only requires iterating over $p+2 n$ variables and computing the maximum eigenvalue of the matrix $\mu \mathbf{X}^{\top} \mathbf{X}$ (or $\left\{\mu \mathbf{X}_{m}^{\top} \mathbf{X}_{m}\right\}_{m=1}^{M}$ ). According to the suggestion by Liang et al. (2024), we use the power method provided by Golub and Loan (2013) to compute the maximum eigenvalue in the LADMM algorithm. This method is very efficient. We illustrate the time required to calculate the maximum eigenvalue as $p$ changes in Figure 2. The R code for computing the maximum eigenvalue is also available in our R package.

- Feature-splitting ADMM in Wen et al. (2023). Inspired by the three-block ADMM in Sun et al. (2015), Wen et al. (2023) proposed a feature-splitting algorithm for solving $\ell_{1}$ quantile regression. Furthermore, guided by the theoretical results of Fan et al. (2014), they utilized the LLA algorithm to solve NPQR. In particular, they used $\ell_{1}$ quantile regression as the initial value and iterated the calculation several times to obtain the solution, with a high probability of convergence to the true solution of NPQR. Feature-splitting ADMM (FSADMM) requires iterating over $p+$ $(3 M-1) n$ variables. Therefore, when $n$ is relatively large, FSADMM may encounter computational difficulties with many local machines.


# 4 Simulation Studies 

In this section, we aim to demonstrate the model selection, estimation accuracy, and computational efficiency of the proposed LADMM algorithm in both nonparallel and parallel environments. To accomplish this, we apply the LADMM algorithm to solve various problems, including NPQR (Wang et al. (2012)), nonconvex penalized least squares and Huber regression (Fan et al. (2018) and Pan et al. (2021)), as well as NPSQR (Mkhadri et al. (2017)). To choose the optimal values for the regularization parameters $\lambda_{1}$ and/or $\lambda_{2}$, we follow the approach proposed by Lee et al. (2014)




---

and Yu et al. (2017). We minimize the HBIC criterion, defined as

$$
\operatorname{HBIC}\left(\lambda_{1}, \lambda_{2}\right)=\log \left(\sum_{i=1}^{n} L\left(y_{i}-x_{i}^{\top} \hat{\beta}_{\lambda_{1}, \lambda_{2}}\right)\right)+\frac{\left|S_{\lambda_{1}, \lambda_{2}}\right| \log (\log n)}{n} C_{n}
$$

Here, $L$ represents a specific loss function, and $\hat{\boldsymbol{\beta}}_{\lambda_{1}, \lambda_{2}}$ corresponds to the nonconvex estimator obtained. $\left|S_{\lambda_{1}, \lambda_{2}}\right|$ denotes the number of nonzero coordinates in $\hat{\boldsymbol{\beta}}_{\lambda_{1}, \lambda_{2}}$, and the value $C_{n}=6 \log (p)$ is recommended by Peng and Wang (2015) and Fan et al. (2021). By minimizing the HBIC criterion, we can effectively select the optimal $\lambda_{1}$ and/or $\lambda_{2}$ values for our nonconvex estimators. These choices allow us to balance the trade-off between model complexity and goodness of fit.

All experiments were performed using R on a computer equipped with an AMD Ryzen 9 7950X 16-Core Processor running at 4.50 GHz and with 32 GB RAM. To facilitate the implementation and usage of the LADMM algorithm, we have developed an R package called PIPADMM. The package is available at the following GitHub repository: https://github.com/xfwu1016/PIPADMM.

# 4.1 Simulation for NPQR 

In the first simulation, we apply the LADMM algorithm to solve the NPQR problem and compare its performance with several recent algorithms, including QRADM from Yu et al. (2017), QRADMslack from Fan et al. (2021), and Feature-splitting ADMM (FSADMM) from Wen et al. (2023). While both Yu et al. (2017) and Fan et al. (2021) provided R packages for their respective algorithms, these packages are only compatible with Mac operating systems. Additionally, the QPADM package only provide estimated coefficients and lacked information such as iteration count and iteration time. To ensure fairness in the comparison, we have rewritten the R code for the QPADM and QPADMslack algorithms, based on the descriptions provided in the respective papers.

For all tested ADMM algorithms, we set the maximum iteration number to 500, with the stopping criterion defined as follows:

$$
\frac{\left\|\beta^{k}-\beta^{k-1}\right\|_{2}}{\max \left(1,\left\|\beta^{k}\right\|_{2}\right)} \leq 10^{-4}
$$

This stopping criterion ensures that the difference between consecutive iterations of the estimated coefficients does not exceed a specified threshold.




---

Regarding our simulation studies, we employed the simulated models in the simulation studies of Peng and Wang (2015), Yu et al. (2017), Fan et al. (2021) and Wen et al. (2023). Specifically, we generate data from the heteroscedastic regression model $y=x_{6}+x_{12}+x_{15}+x_{20}+$ $0.7 x_{1} \epsilon$, where $\epsilon \sim N(0,1)$. The covariates $\left(x_{1}, x_{2}, \ldots, x_{p}\right)$ are generated in two steps.

- First, we generate $\tilde{\mathbf{x}}=\left(\tilde{x}_{1}, \tilde{x}_{2}, \ldots, \tilde{x}_{p}\right)^{\top}$ from a $p$-dimensional multivariate normal distribution $N(0, \Sigma)$, where $\Sigma_{i j}=0.5^{|i-j|}$ for $1 \leq i, j \leq p$.
- Second, we set $x_{1}=\Phi\left(\tilde{x}_{1}\right)$ and $x_{j}=\tilde{x}_{j}$ for $j=2, \ldots, p$.

In nonparallel environments $(M=1)$, we simulate datasets with sizes $(n, p)=(30,000,1,000)$, $(1,000,30,000),(10,000,30,000)$, and $(30,000,30,000)$. In parallel environments $(M \geq 2)$, we simulate datasets with sizes $(n, p)=(200,000,500)$ and $(500,000,1,000)$. We run 500 independent simulations, and the average results for both nonparallel and parallel computations are presented in Table 1 and Table 2, respectively. Due to space limitations, this section will only focus on the results of the SCAD $(a=3.7)$ and $\tau=0.7$, while the other simulation results are included in Appendix C.1.



* The meanings of the notations used in this table are as follows: $\mathrm{P}_{1}(\%)$ : proportion that $x_{1}$ is selected; $\mathrm{P}_{2}(\%)$ : proportion that $x_{6}, x_{12}, x_{15}$, and $x_{20}$ are selected; AE: absolute estimation error; Ite: number of iterations; Time (s): running time. Numbers in the parentheses represent the corresponding standard deviations, and the optimal solution is represented in bold.

Table 1 provides evidence that LADMM outperforms QPADM and QPADMslack in terms of computational speed and estimation accuracy when the dimensionality of the problem $p$ is large. Furthermore, LADMM outperforms FSADMM when the sample size $n$ is large. To further illustrate the advantages of LADMM, we present the computational time in Figure 3. These numerical results indicate that LADMM demonstrates remarkable advantages in terms of computational efficiency and accuracy compared to other ADMM algorithms, particularly for scenarios where both $n$ and/or




---

$p$ are large. In Table 2, we observe that when multiple local machines are employed, LADMM shows comparable performances in terms of computational time and accuracy compared to QRADM and QRADMslack. This result is also visualized in Figure 4. Table 2 and Figure 4 underscore the advantages of utilizing LADMM in parallel computing settings. It is worth noting that our solution is not particularly influenced by $M$, which is consistent with the theoretical results of Section 3.2.


$* P 1$ and $P 2$ are not presented in Table 2 because all methods have a value of 100 for these two metrics. It is clear that Nonzero, AE, and Ite of LADMM are not significantly affected by the value of $M$.


# 4.2 Simulation for NPLS and NPHR 

In this simulation, we utilize the proposed LADMM method and the ILAMM algorithm proposed in Fan et al. (2018) and Pan et al. (2021) to solve nonconvex penalized least squares regression




---


```
+ False Positive (FP) refers to the number of variables with a coefficient of zero that are mistakenly included in the


(NPLS) and nonconvex penalized Huber regression (NPHR). We then compare the performance of
these methods. Pan et al. (2021) provided a R package for ILAMM which is available at https:
//github.com/XiaoouPan/ILAMM. Similar to Pan et al. (2021), we generate the heteroscedastic
model, $y_{i}=x_{i}^{\top} \beta+c^{-1}\left(x_{i}^{\top} \beta\right)^{2} \epsilon_{i}$ with $x_{i} \sim \mathcal{N}\left(0, I_{p}\right)$ for $i=1, \ldots, n$, where the constant $c$ is chosen
as $c=\sqrt{3 \beta^{\top} \beta}$ such that $\mathrm{E}\left(c^{-1}\left(x_{i}^{\top} \beta\right)^{2}\right)^{2}=1$. The true vector of regression coefficients $\beta$ is
$(4,3,2,-2,-2,-2,0, \ldots, 0)^{\top}$. Moreover, we consider the following two error distributions.

- Normal distribution: $\epsilon_{i} \sim \mathcal{N}\left(\mu, \sigma^{2}\right)$ with $\mu=0$ and standard deviation $\sigma=1.5$.
- Lognormal distribution: $\epsilon_{i} \sim \mathrm{LN}\left(\mu, \sigma^{2}\right)$ with $\mu=0$ and $\sigma=1.2$.


```




---

# Appendix C. 2 . 

## 4.3 Simulation for NPSQR

In this example, we utilize the proposed LADMM and algorithm in Mkhadri et al. (2017) to solve nonconvex penalized smooth quantile regression (NPSQR) and compare their performances. Mkhadri et al. (2017) provided an efficient $R$ package for calculating NPSQR, which can be found at the following https://github.com/KarimOualkacha/cdaSQR/tree/master. Following the scenario 3 in Section 4.2 of Mkhadri et al. (2017), we generate data sets with $y_{i}=x_{i}^{\top} \beta+\epsilon_{i}$, where $\epsilon \sim N\left(0, \sigma^{2} I_{p}\right)$ and $\sigma=5$. The covariates $\left(x_{1}, x_{2}, \ldots, x_{p}\right)$ are generated from $N(0, \Sigma)$, where $\Sigma_{i j}=0.5^{|i-j|}$ for $1 \leq i, j \leq p$. The true vector of regression coefficients is

$$
\beta=(\underbrace{3, \ldots, 3}_{5}, \underbrace{-1.5, \ldots,-1.5}_{5}, \underbrace{1, \ldots, 1}_{5}, \underbrace{2, \ldots, 2}_{5}, \underbrace{0, \ldots, 0}_{p-20})^{\top}
$$

The comparison results of our algorithm LADMM and cdaSQR are summarized in Table 4. These results are all about Snet, and the results for Mnet and Cnet are in the Appendix C.3. In terms of computational accuracy and speed, LADMM and cdaSQR perform similarly, but LADMM has significant advantages in terms of FP and FN .



## 5 Real Data Studies

In this section, we compare the performance of several parallel ADMM algorithms in an online publicly available dataset at http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity. This dataset provides a summary of the popularity, measured in terms of shares, as well as 60 features of 39,644 news published by Mashable over a two-year period. The features include various




---

aspects such as binary variables indicating news categories (Lifestyle, Entertainment, Business, Social Media, Technology, or World), published time (day of the week and weekend or not), average word length, number of keywords, rate of nonstop words, and more. For more information about the dataset, please refer to the study conducted by Fernandes et al. (2015).

This dataset is heterogeneous in nature, and it has been analyzed by Fan et al. (2021) using NPQR. Their aim was to analyze how various features impact the popularity of news, particularly focusing on those that had gained high levels of popularity. They identified $x_{14}$ (Entertainment) and $x_{27}$ (number of key words) as the two most influential features. However, the empirical results in Fan et al. (2021) also indicated that QPADM and QPADMslack algorithms tend to select a relatively larger number of variables, and this tendency becomes more pronounced as $M$ increases. The following empirical results indicate that LADMM does not have this increasing trend, and also has good prediction accuracy.

As in the study by Fan et al. (2021), we standardize the non-binary factors to have zero mean and unit variance. The features and response variables are denoted as $x_{1}, x_{2}, \ldots, x_{60}$ and $y$, respectively. To assess the performance of the algorithm, we randomly partition the complete dataset 100 times. Each partition consisted of randomly selecting 35,000 samples as the training set, with the remaining samples designated as the test set. To replicate the scenario of parallel computing, we randomly divide the training set into subsets of equal size. In our analysis, we consider values of $M=10$ and $M=100$. We then record the average number of selected nonzero coefficients (Nonzero), the prediction error (PE), the number of iterations (Ite), and the CPU running time (in seconds) of the algorithms. The prediction error (PE) is calculated as follows: $\mathrm{PE}=\frac{1}{n_{\text {test }}} \sum_{i=1}^{n_{\text {test }}}\left|y_{i}-\hat{y}_{i}\right|$, where $n_{\text {test }}=4644$ represents the sample size of the test set. We only include the results of SCAD $(a=3.7)$ regression in Table 5, and the results of MCP regression are included in Appendix C.4.




---


# Table 5: Analysis of the news popularity data under the SCAD penalty with $\tau=0.5$. 


## Algorithm





---

# 6 Conclusion and Discussion 

This paper introduces a parallel LADMM algorithm designed to efficiently solve nonconvex penalized smooth quantile regression problems. Our algorithm is easy to implement, and it offers great flexibility that allows for its extension to many other nonconvex penalized regression models, such as quantile regressions, least squares regressions, and Huber regressions. These extensions have also been implemented in our R package PIPADMM. At present, the parallel algorithm implementation in our R package is akin to the pseudo-parallelism of the R packages QRADMM and QPADMslack. One advantage of this pseudo-parallelism is that it is very convenient for readers to verify algorithms and reproduce code on a computer. Readers can also utilize our designed algorithm framework for processing distributed stored data in Spark. Notably, compared to existing parallel algorithms that rely on consensus-based approaches for solving regression models, our solution companion changes very little with different sample partitioning strategies and requires fewer iterations to achieve convergence. Furthermore, for smooth loss functions, we prove the global convergence of our LADMM algorithm, meaning that the iterative solutions converge to a critical point of the Lagrangian function.

The parallel algorithm design process outlined in Section 3.2 indicates that other linearized ADMM algorithms, such as those presented in Li et al. (2014), Gu et al. (2018), and Liang et al. (2024), can be readily adapted to parallel computation using our strategy. Although this paper only deals with regression problems with simple combined regularization like Elastic-net, Snet, Mnet, and Cnet, our algorithm also has the potential to handle more complex combinatorial regularization terms. Examples include sparse group lasso in Wu and Lange (2008) and Laria et al. (2019), as well as sparse fused lasso in Tibshirani et al. (2005) and Wu et al. (2024).

There is still much work to be done, such as studying the convergence of parallel ADMM algorithms for solving NPQR, which has remained an unresolved open problem in Yu et al. (2017). This paper only employs NPSQR to approximate NPQR, which appears to solve the convergence issue. However, this study demonstrated that the global convergence claimed by Theorem 2 cannot be guaranteed for arbitrarily small $c$ and $\kappa$. Furthermore, our algorithm not only efficiently handles regularized regression problems, but also easily extends to regularized classification problems with smoothing losses, such as Huber loss SVM in Wang et al. (2008) and least squares loss SVM in Huang et al. (2014).




---

# Acknowledgements 

We are grateful to Professor Karim Oualkacha from the Department of Mathematics at the Universit du Qubec  Montral for providing us with the code for the R package cdaSQR. Specifically, we are very grateful to Professor Bingsheng He for his valuable discussions with us, which greatly helped us prove the convergence of the algorithm. The research was partially supported by the National Natural Science Foundation of China [grant numbers 11871121, 12171405, 12271066] and the project of science and technology research program of Chongqing Education Commission of China [Grant Numbers KJQN202302003].

## References

Aravkin, A. Y., Kambadur, A., Lozano, A. C., and Luss, R. (2014). Sparse Quantile Huber Regression for Efficient and Robust Estimation. ArXiv, 14(1):1-1.

Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J. (2010). Distributed Optimization and Statistical Learning Via the Alternating Direction Method of Multipliers. Foundation and Trends in Machine Learning, 3(1):1-122.

Fan, J. and Li, R. (2001). Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties. Journal of the American Statistical Association, 96(456):1348-1360.

Fan, J., Liu, H., Sun, Q., and Zhang, T. (2018). I-LAMM for Sparse Learning: Simultaneous Control of Algorithmic Complexity and Statistical Error. The Annals of Statistics, 46(2):814841.

Fan, J., Xue, L., and Zou, H. (2014). Strong Oracle Optimality of Folded Concave Penalized Estimation. The Annals of Statistics, 42(3):819-849.

Fan, Y., Lin, N., and Yin, X. (2021). Penalized Quantile Regression for Distributed Big Data Using the Slack Variable Representation. Journal of Computational and Graphical Statistics, 30(3):557-565.

Fernandes, K., Vinagre, P., and Cortez, P. (2015). A Proactive Intelligent Decision Support System




---

for Predicting the Popularity of Online News. In 17th Portuguese Conference on Artificial Intelligence, pages 535-546.

Golub, G. H. and Loan, C. F. V. (2013). Matrix Computations - 4th Edition. Johns Hopkins University Press.

Gong, P., Zhang, C., Lu, Z., Huang, J., and Ye, J. (2013). A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems. International Conference on Machine Learning, 28:37-45.

Gu, Y., Fan, J., Kong, L., Ma, S., and Zou, H. (2018). ADMM for High-Dimensional Sparse Penalized Quantile Regression. Technometrics, 60(3):319-331.

Guan, L., Qiao, L., Li, D., Sun, T., Ge, K., and Lu, X. (2018). An Efficient ADMM-Based Algorithm to Nonconvex Penalized Support Vector Machines. 2018 IEEE International Conference on Data Mining Workshops, pages 1209-1216.

Guo, K., Han, D., and Wu, T. (2016). Convergence of Alternating Direction Method for Minimizing Sum of Two Nonconvex Functions with Linear Constraints. International Journal of Computer Mathematics, 94(8):1-18.

He, B., Ma, F., and Yuan, X. (2020). Optimally Linearizing the Alternating Direction Method of Multipliers for Convex Programming. Computational Optimization and Applications, 75:361388.

Huang, J., Breheny, P., Ma, S., and Zhang, C. (2016). THE Mnet Method for Variable Selection. Statistica Sinica, 3:903-923.

Huang, X., Shi, L., and Suykens, J. A. K. (2014). Asymmetric Least Squares Support Vector Machine Classifiers. Computational Statistics \& Data Analysis, 70:395-405.

Huber, P. J. (1964). Robust Estimation of a Location Parameter. Annals of Mathematical Statistics, 35:492-518.

Jennings, L. S., Wong, K. H., and Teo, K. L. (1993). An Optimal Control Problem in Biomechanics. Ifac Proceedings Volumes, 26(2):279-282.




---

Kim, J. and Oh, H.-S. (2020). Pseudo-quantile Functional Data Clustering. Journal of Multivariate Analysis, 178:104626.

Koenker, R. and Basset, G. (1978). Regressions Quantiles. Econometrica, 46.
Laria, J. C., Aguilera-Morillo, M. C., and Lillo, R. E. (2019). An Iterative Sparse-Group Lasso. Journal of Computational and Graphical Statistics, 28(3):722-731.

Lee, E. R., Noh, H., and Park, B. U. (2014). Model Selection via Bayesian Information Criterion for Quantile Regression Models. Journal of the American Statistical Association, 109(505):216-229.

Li, G., Peng, H., and Zhu, L. (2011). Nonconcave Penalized M-estimation with a Diverging Number of Parameters. Statistica Sinica, 21(1):391-419.

Li, X., Mo, L., Yuan, X., and Zhang, J. (2014). Linearized Alternating Direction Method of Multipliers for Sparse Group and Fused LASSO Models. Computational Statistics \& Data Analysis, 79:203-221.

Liang, R., Wu, X., and Zhang, Z. (2024). Linearized Alternating Direction Method of Multipliers for Elastic-net Support Vector Machines. Pattern Recognition, 148:110134.

Lin, Z., Fang, C., and Li, H. (2022). Alternating Direction Method of Multipliers for Machine Learning. Springer Singapore.

Loh, P.-L. (2017). Statistical Consistency and Asymptotic Normality for High-dimensional Robust M-estimators. The Annals of Statistics, 45(2):866-896.

Loh, P.-L. and Wainwright, M. J. (2015). Regularized M-estimators with Nonconvexity: Statistical and Algorithmic Theory for Local Optima. The Journal of Machine Learning Research.

Mkhadri, A., Ouhourane, M., and Oualkacha, K. (2017). A Coordinate Descent Algorithm for Computing Penalized Smooth Quantile Regression. Statistics and Computing, 27:865-883.

Negahban, S. N., Ravikumar, P., Wainwright, M. J., and Yu, B. (2010). A Unified Framework for High-Dimensional Analysis of M-Estimators with Decomposable Regularizers. Statistical Science, 27(4):538-557.




---

Newey, W. and Powell, J. (1987). Asymmetric Least Squares Estimation and Testing. Econometrica, 55:819-847.

Oh, H.-S., Thomas, C. M. L., and Douglas, W. N. (2011). Fast Nonparametric Quantile Regression With Arbitrary Smoothing Methods. Journal of Computational and Graphical Statistics, 20(2):510-526.

Ouhourane, M., Yang, Y., Benedet, A. L., and Oualkacha, K. (2022). Group Penalized Quantile Regression. Statistical Methods and Applications, 31(3):495-529.

Pan, X., Sun, Q., and Zhou, W. (2021). Iteratively Reweighted $\ell_{1}$-penalized Robust Regression. Electronic Journal of Statistics, 15(1):3287-3348.

Peng, B. and Wang, L. (2015). An Iterative Coordinate Descent Algorithm for High-Dimensional Nonconvex Penalized Quantile Regression. Journal of Computational \& Graphical Statistics, 24(3):676-694.

Sun, D., Toh, K.-C., and Yang, L. (2015). A Convergent 3-Block SemiProximal Alternating Direction Method of Multipliers for Conic Programming with 4-Type Constraints. SIAM Journal on Optimization, 25(2):882-915.

Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., and Knight, K. (2005). Sparsity and Smoothness Via the Fused Lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology, 67(1):91-108.

Wang, L., Peng, B., Bradic, J., Li, R., and Wu, Y. (2020). A Tuning-free Robust and Efficient Approach to High-dimensional Regression. Journal of the American Statistical Association, 115(532):1-44.

Wang, L., Wu, Y., and Li, R. (2012). Quantile Regression for Analyzing Heterogeneity in Ultra high Dimension. Journal of the American Statistical Association, 107:214-222.

Wang, L., Zhu, J., and Zou, H. (2008). Hybrid Huberized Support Vector Machines for Microarray Classification and Gene Selection. Bioinformatics, 24(3):412-419.

Wang, X., Park, T., and Carriere, K. C. (2010). Variable Selection via Combined Penalization for High-dimensional Data Analysis. Computational Statistics \& Data Analysis, 54(10):2230-2243.




---

Wang, X. and Yuan, X. (2012). The Linearized Alternating Direction Method of Multipliers for Dantzig Selector. Siam Journal on Scientific Computing, 34(5):2792-2811.

Wen, J., Yang, S., Wang, C., Jiang, Y., and Li, R. (2023). Feature-splitting Algorithms for Ultrahigh Dimensional Quantile Regression. Journal of Econometrics, page 105426.

Wu, T. and Lange, K. (2008). Coordinate Descent Algorithms for Lasso Penalized Regression.
Wu, X., Ming, H., Zhang, Z., and Cui, Z. (2024). Multi-block Alternating Direction Method of Multipliers for Ultrahigh Dimensional Quantile Fused Regression. Computational Statistics \& Data Analysis, 192:107901.

Yu, L. and Lin, N. (2017). ADMM for Penalized Quantile Regression in Big Data. International Statistical Review, 85:494-518.

Yu, L., Lin, N., and Wang, L. (2017). A Parallel Algorithm for Large-Scale Nonconvex Penalized Quantile Regression. Journal of Computational and Graphical Statistics, 26(4):935-939.

Zhang, C. (2010a). Nearly Unbiased Variable Selection Under Minimax Concave Penalty. Annals of Statistics, 38(2):894-942.

Zhang, T. (2010b). Analysis of Multi-stage Convex Relaxation for Sparse Regularization. Journal of Machine Learning Research, 11(35):1081-1107.

Zheng, S. (2011). Gradient Descent Algorithms for Quantile Regression with Smooth Approximation. International Journal of Machine Learning \& Cybernetics, 2:191-207.

Zhou, W.-X., Bose, K., Fan, J., and Liu, H. (2018). A New Perspective on Robust M-Estimation: Finite Sample Theory and Applications to Dependence-Adjusted Multiple Testing. The Annals of Statistics, 46(5):1904-1931.

Zou, H. and Hastie, T. (2005). Regularization and Variable Selection Via the Elastic Net. Journal of the Royal Statistical Society Series B: Statistical Methodology, 67(2):301-320.

Zou, H. and Li, R. (2008). One-step Sparse Estimates in Nonconcave Penalized Likelihood Models. The Annals of Statistics, 36(4):1509-1533.




---

# Online support materials 

In this online support materials, we first provide the derivation of the closed-form solution for the proximal operator in Section 2.3 of the main text. Next, we give the proofs of Theorem 1 and Theorem 2. Finally, we present supplementary numerical experiments.

## A Proof of the Closed-form Solution of Proximal Operator

In A.1, we derive closed-form solutions for the proximal operators of two smooth quantile regression methods similar to those in Liang et al. (2024). In A.2, we propose a simple and scalable approach for deriving closed-form solutions of the proximal operator for regularized terms.

## A. 1 Loss Fuction

Recalling that the loss function $L_{\tau, *}(u)=\sum_{i=1}^{n} L_{\tau, *}\left(u_{i}\right)$, we can divide the optimization problem

$$
\arg \min _{u}\left\{L_{\tau, *}(u)+\frac{\mu}{2}\|u-v\|_{2}^{2}\right\}
$$

into $n$ independent univariate problems, which can be expressed as $\min _{u_{i}}\left\{L_{\tau, *}\left(u_{i}\right)+\frac{\mu}{2}\left(u_{i}-v_{i}\right)^{2}\right\}, i=$ $1,2, \ldots, n$. We denote the minimum solution of (31) by $\hat{u}$.

- $L_{\tau, c}\left(u_{i}\right)$ : Let us examine the expression for $L_{\tau, c}\left(u_{i}\right)$ :

$$
\begin{cases}\tau\left(u_{i}-0.5 c\right) & \text { if } u_{i} \geq c \\ \frac{\tau u_{i}^{2}}{2 c} & \text { if } 0 \leq u_{i}<c \\ \frac{(1-\tau) u_{i}^{2}}{2 c} & \text { if }-c \leq u_{i}<0 \\ (\tau-1)\left(u_{i}+0.5 c\right) & \text { if } u_{i}<-c\end{cases}
$$

Throughout the entire domain of the above function, it is either a linear or quadratic function.




---

Therefore, it has a closed-form solution in each small domain. After some algebra, we have

$$
\hat{u}_{j}= \begin{cases}v_{j}-\frac{\tau}{\mu}, & \text { if } v_{j} \geq c+\frac{\tau}{\mu} \\ \frac{c \mu v_{j}}{c \mu+\tau}, & \text { if } 0 \leq v_{j}<c+\frac{\tau}{\mu} \\ \frac{c \mu v_{j}}{c \mu+1-\tau}, & \text { if }-c+\frac{\tau-1}{\mu} \leq v_{j}<0 \\ v_{j}-\frac{\tau-1}{\mu}, & \text { if } v_{j}<-c+\frac{\tau-1}{\mu}\end{cases}
$$

- $L_{\tau, \kappa}\left(u_{i}\right)$ : Let us examine the expression for $L_{\tau, \kappa}\left(u_{i}\right)$ :

$$
\begin{cases}\tau\left(u_{i}-\frac{\tau \kappa}{2}\right) & \text { if } u_{i}>\tau \kappa \\ \frac{u_{i}^{2}}{2 \kappa} & \text { if } u_{i} \in[(\tau-1) \kappa, \tau \kappa] \\ (\tau-1)\left[u_{i}-\frac{(\tau-1) \kappa}{2}\right] & \text { if } u_{i}<(\tau-1) \kappa\end{cases}
$$

Similar to $L_{\tau, c}\left(u_{i}\right)$, we can derive the following closed-form solution of $L_{\tau, \kappa}\left(u_{i}\right)$,

$$
\hat{u}_{j}= \begin{cases}v_{j}-\frac{\tau}{\mu}, & \text { if } v_{j} \geq \tau \kappa+\frac{\tau}{\mu} \\ \frac{c \mu v_{j}}{c \mu+\tau}, & \text { if }(\tau-1) \kappa+\frac{\tau-1}{\mu} \leq v_{j}<\tau \kappa+\frac{\tau}{\mu} \\ v_{j}-\frac{\tau-1}{\mu}, & \text { if } v_{j}<(\tau-1) \kappa+\frac{\tau-1}{\mu}\end{cases}
$$

# A. 2 Nonconvex Penalty 

Recalling that the nonconvex penalty $P_{\lambda}(|u|)=\sum_{j}^{p} P_{\lambda}\left(\left|u_{j}\right|\right)$, we can divide the optimization problem

$$
\arg \min _{u}\left\{P_{\lambda}(|u|)+\frac{\eta}{2}\|u-v\|_{2}^{2}\right\}
$$

into $p$ independent univariate problems, which can be expressed as $\min _{u_{j}}\left\{P_{\lambda}\left(\left|u_{j}\right|\right)+\frac{\mu}{2}\left(u_{j}-v_{j}\right)^{2}\right\}, j=$ $1,2, \ldots, p$. Let

$$
\hat{u}_{j}=\arg \min _{u_{j}}\left\{P_{\lambda}\left(\left|u_{j}\right|\right)+\frac{\eta}{2}\left(u_{j}-v_{j}\right)^{2}\right\}
$$




---

Because $P_{\lambda}\left(\left|u_{j}\right|\right)$ is a function of the absolute value of $u_{j}$, and the subsequent term is a quadratic function ( $v_{j}$ is a constant), we have $\operatorname{sign}\left(\hat{u}_{j}\right)=\operatorname{sign}\left(v_{j}\right)$ and $\operatorname{sign}\left(v_{j}\right) \hat{u}_{j} \geq 0$. Let

$$
\tilde{u}_{j}=\operatorname{sign}\left(v_{j}\right) \hat{u}_{j}
$$

then we can convert (36) into the following optimization formula

$$
\tilde{u}_{j}=\underset{u_{j} \geq 0}{\arg \min }\left\{P_{\lambda}\left(u_{j}\right)+\frac{\eta}{2}\left(u_{j}-\left|v_{j}\right|\right)^{2}\right\}
$$

Make the first derivative of the optimization formula above equal to 0 , resulting in

$$
\tilde{u}_{j}=\left|v_{j}\right|-\nabla P_{\lambda}\left(\tilde{u}_{j}\right) / \eta \text { and } \tilde{u}_{j} \geq 0
$$

By (37), we have

$$
\hat{u}_{j}=\operatorname{sign}\left(v_{j}\right) \tilde{u}_{j}
$$

The above two equations play a crucial role in deriving the closed-form solutions for the proximal operators of the (nonconvex) penalty term. Clearly, $\hat{u}_{j}$ in (40) is the solution of the proximal operator.

- For Snet, we have

$$
\nabla P_{\lambda}\left(\tilde{u}_{j}\right)= \begin{cases}\lambda_{1}+\lambda_{2} \tilde{u}_{j}, & \text { if } \tilde{u}_{j} \leq \lambda_{1} \\ \frac{a \lambda_{1}-\tilde{u}_{j}}{a-1}+\lambda_{2} \tilde{u}_{j}, & \text { if } \lambda_{1}<\tilde{u}_{j}<a \lambda_{1} \\ \lambda_{2} \tilde{u}_{j}, & \text { if } \tilde{u}_{j} \geq a \lambda_{1}\end{cases}
$$

By substituting (41) into the equation (39), we can discuss the solutions of the equation (39) in different regions. For $\tilde{u}_{j} \leq \lambda_{1}$, the solution of equation (39) is $\left[\frac{\eta\left|v_{j}\right|-\lambda_{1}}{\eta+\lambda_{2}}\right]_{+}$.For $\lambda_{1}<\tilde{u}_{j}<a \lambda_{1}$, the solution of equation (39) is $\left[\frac{(a-1) \eta\left|v_{j}\right|-a \lambda_{1}}{(a-1)\left(\eta+\lambda_{2}\right)-1}\right]_{+}$. For $\tilde{u}_{j} \geq a \lambda_{1}$, the solution of equation (39) is $\frac{\eta\left|v_{j}\right|}{\eta+\lambda_{2}}$.




---

Note that $\hat{u}_{j}=\operatorname{sign}\left(v_{j}\right) \tilde{u}_{j}$ and $(a-1)\left(\eta+\lambda_{2}\right)>1$, it follows that

$$
\hat{u}_{j}= \begin{cases}\operatorname{sign}\left(v_{j}\right) \cdot\left[\frac{\eta\left|v_{j}\right|-\lambda_{1}}{\eta+\lambda_{2}}\right]^{+}, & \text { if }\left|v_{j}\right| \leq \frac{\lambda_{1}\left(1+\eta+\lambda_{2}\right)}{\eta} \\ \operatorname{sign}\left(v_{j}\right) \cdot\left[\frac{(a-1) \eta\left|v_{j}\right|-a \lambda_{1}}{(a-1)\left(\eta+\lambda_{2}\right)-1}\right]^{+}, & \text { if } \frac{\lambda_{1}\left(1+\eta+\lambda_{2}\right)}{\eta}<\left|v_{j}\right|<\frac{a \lambda_{1}\left(\eta+\lambda_{2}\right)}{\eta} \\ \frac{\eta v_{j}}{\eta+\lambda_{2}}, & \text { if }\left|v_{j}\right| \geq \frac{a \lambda_{1}\left(\eta+\lambda_{2}\right)}{\eta}\end{cases}
$$

- For Mnet, we have

$$
\nabla P_{\lambda}\left(\tilde{u}_{j}\right)= \begin{cases}\frac{\lambda_{1}-\tilde{u}_{j}}{a}+\lambda_{2} \tilde{u}_{j}, & \text { if } \tilde{u}_{j} \leq a \lambda_{1} \\ \lambda_{2} \tilde{u}_{j}, & \text { if } \tilde{u}_{j}>a \lambda_{1}\end{cases}
$$

By substituting (43) into the equation (39), we can discuss the solutions of the equation (39) in different regions. For $\tilde{u}_{j} \leq a \lambda_{1}$, the solution of equation (39) is $\left[\frac{a \eta\left|v_{j}\right|-a \lambda_{1}}{a\left(\eta+\lambda_{2}\right)-1}\right]^{+}$. For $\tilde{u}_{j}>a \lambda_{1}$, the solution of equation (39) is $\frac{\eta\left|v_{j}\right|}{\eta+\lambda_{2}}$. Together with $\hat{u}_{j}=\operatorname{sign}\left(v_{j}\right) \tilde{u}_{j}$, we obtain

$$
\hat{u}_{j}= \begin{cases}\operatorname{sign}\left(v_{j}\right) \cdot\left[\frac{a \eta\left|v_{j}\right|-a \lambda_{1}}{a\left(\eta+\lambda_{2}\right)-1}\right]^{+}, & \text { if }\left|v_{j}\right|<\frac{a \lambda_{1}\left(\eta+\lambda_{2}\right)}{\eta} \\ \frac{\eta v_{j}}{\eta+\lambda_{2}}, & \text { if }\left|v_{j}\right| \geq \frac{a \lambda_{1}\left(\eta+\lambda_{2}\right)}{\eta}\end{cases}
$$

- For Cnet, we have

$$
\nabla P_{\lambda}\left(\tilde{u}_{j}\right)= \begin{cases}\lambda_{1}+\lambda_{2} \tilde{u}_{j}, & \text { if } \tilde{u}_{j} \leq a \\ \lambda_{2} \tilde{u}_{j}, & \text { if } \tilde{u}_{j}>a\end{cases}
$$

By substituting (45) into the equation (39), we can discuss the solutions of the equation (39) in different regions. For $\tilde{u}_{j} \leq a$, the solution of equation (39) is $\left[\frac{\eta\left|v_{j}\right|-\lambda_{1}}{\eta+\lambda_{2}}\right]^{+}$. For $\tilde{u}_{j}>a \lambda_{1}$, the solution of equation (39) is $\frac{\eta\left|v_{j}\right|}{\eta+\lambda_{2}}$. Together with $\hat{u}_{j}=\operatorname{sign}\left(v_{j}\right) \tilde{u}_{j}$, we have

$$
\hat{u}_{j}= \begin{cases}\operatorname{sign}\left(v_{j}\right) \cdot\left[\frac{\eta\left|v_{j}\right|-\lambda_{1}}{\eta+\lambda_{2}}\right]^{+}, & \text { if }\left|v_{j}\right|<\frac{a\left(\eta+\lambda_{2}\right)}{\eta} \\ \frac{\eta v_{j}}{\eta+\lambda_{2}}, & \text { if }\left|v_{j}\right| \geq \frac{a\left(\eta+\lambda_{2}\right)}{\eta}\end{cases}
$$




---

# B 

## Proof of Theorem 1 and 2

## B. 1 Proof of Theorem 1

Looking back at (19) and (20), we have

$$
\left(\mu X^{\top}\left(X \beta_{k}+r_{k}-y-d_{k} / \mu\right)\right)=\left[\mu \sum_{m=1}^{M} X_{m}^{\top}\left(X_{m} \beta_{k}+r_{k m}-y_{m}-d_{m}^{k} / \mu\right)\right]
$$

If the two algorithms have the same $k$-th iteration solution $\left(\beta_{k}, r_{k}, d_{k}\right)$, then we can derive the following equation from (18) and (24),

$$
\hat{\beta}_{k+1}=\tilde{\beta}_{k+1}, k=0,1,2,3, \ldots
$$

Substituting the above equation into (16) and (23), along with (20), we can observe that $\hat{r}_{k+1}=$ $\tilde{r}_{k+1}$ and $\hat{d}_{k+1}=\tilde{d}_{k+1}$. Therefore, we are able to conclude the result of Theorem 1.

## B. 2 Proof of Theorem 2

The proof of Theorem 2 mainly follows the proof of algorithm convergence presented in Guo et al. (2016). However, there are two key differences in our proof. One is the inclusion of an additional quadratic linearization term in our algorithm. The other is our method does not rely on assumptions about the boundedness of iterative solutions, which is necessary for the convergence proof in Guo et al. (2016). Our proof can be divided into the following four steps,

- First, we prove that the augmented Lagrangian function that requires alternating iteration minimization is monotonically nonincreasing.
- Second, we demonstrate that the iterative solutions of our algorithm remain bounded throughout the entire iteration process.
- Third, we demonstrate that the iterative solutions of LADMM satisfy $\lim _{k \rightarrow+\infty}\left\|w_{k}-w_{k+1}\right\|_{2}=0$.
- Finally, we prove that the augmented Lagrangian function of all limit points of our LADMM algorithm is a constant.




---

Upon completing these four steps, following the approach outlined in Theorem 3.1 of Guo et al. (2016), we can conclude that the iterative sequence produced by our algorithm is a Cauchy sequence.

Now, we start our proof. Because Theorem 1 has been proven to take the same $\eta$, the solutions of Algorithm 1 and Algorithm 2 are exactly the same. Therefore, we only need to discuss the nonparallel version of the LADMM algorithm. In terms of constrained optimization, we have

$$
\min _{\beta, r} \sum_{i=1}^{n} L_{\tau, *}\left(r_{i}\right)+P_{\lambda}(|\beta|), \quad \text { s.t. } \quad X \beta+r=y
$$

In Guo et al. (2016), they assumed that $L_{\tau, *}$ is a continuously differentiable function with Lipschitz continuous gradient $\nabla L_{\tau, *}$. Here, we employ the following lemma to ensure that $L_{\tau, c}$ and $L_{\tau, \kappa}$ possess this property.

Lemma 1 (Proposition 2 in Mkhadri et al. (2017)) The smooth quantile $L_{\tau, c}$ and $L_{\tau, \kappa}$ are differentiable and have Lipschitz continuous first derivatives, that is,

$$
\begin{aligned}
& \left\|\nabla L_{\tau, c}\left(u_{1}\right)-\nabla L_{\tau, c}\left(u_{2}\right)\right\|_{2} \leq \frac{\max \{\tau, 1-\tau\}}{c}\left\|u_{1}-u_{2}\right\|_{2}, \\
& \left\|\nabla L_{\tau, \kappa}\left(u_{1}\right)-\nabla L_{\tau, \kappa}\left(u_{2}\right)\right\|_{2} \leq \frac{1}{\kappa}\left\|u_{1}-u_{2}\right\|_{2},
\end{aligned}
$$

where $u_{1}$ and $u_{2}$ are two arbitrary real numbers.
Problem (48) has the following augmented Lagrangian,

$$
L_{\mu}(\beta, r, d)=L_{\tau, *}(r)+P_{\lambda}(|\beta|)-d^{\top}(X \beta+r-y)+\frac{\mu}{2}\|X \beta+r-y\|_{2}^{2}
$$

and the iterative steps of LADMM are as follows,

$$
\left\{\begin{array}{l}
\beta_{k+1}=\operatorname{argmin}_{\beta}\left\{P_{\lambda}(|\beta|)+\frac{\mu}{2}\left\|X \beta+r_{k}-y-d_{k} / \mu\right\|_{2}^{2}+\frac{1}{2}\left\|\beta-\beta_{k}\right\|_{S}^{2}\right\} \\
r_{k+1}=\operatorname{argmin}_{r}\left\{L_{\tau, *}(r)+\frac{\mu}{2}\left\|X \beta_{k+1}+r-y-d_{k} / \mu\right\|_{2}^{2}\right\} \\
d_{k+1}=d_{k}-\mu\left(X \beta_{k+1}+r_{k+1}-y\right)
\end{array}\right.
$$

We first demonstrate that the augmented Lagrangian function $L_{\mu}\left(\beta_{k}, r_{k}, d_{k}\right)$ decreases as the number of iterations increases.




---

Lemma 2 Let the sequence $w^{k}=\left\{\beta^{k}, r^{k}, d^{k}\right\}$ be generated by the LADMM, then

$$
L_{\mu}\left(w^{k+1}\right) \leq L_{\mu}\left(w^{k}\right)+\left(\frac{n}{\mu \min \left\{c^{2}, \kappa^{2}\right\}}-\frac{\mu}{2}\right)\left\|r^{k}-r^{k+1}\right\|_{2}^{2}
$$

Together with $\mu>\sqrt{\frac{2 n}{\min \{c, \kappa\}}},\left\{L_{\mu}\left(w^{k}\right)\right\}_{k=1}^{\infty}$ is monotonically nonincreasing.
Proof The optimality conditions for the $k+1$-iteration of LADMM is

$$
\left\{\begin{array}{l}
0 \in \partial P_{\lambda}\left(\left|\beta^{k+1}\right|\right)-X^{\top} d^{k}+\mu X^{\top}\left(X \beta^{k+1}+r^{k}-y\right)+S\left(\beta^{k+1}-\beta^{k}\right) \\
0=\nabla L_{\tau, *}\left(r^{k+1}\right)-d^{k}+\mu\left(X \beta^{k+1}+r^{k+1}-y\right) \\
d^{k+1}=d^{k}-\mu\left(X \beta^{k+1}+r^{k+1}-y\right)
\end{array}\right.
$$

Using the last equality and rearranging terms, we obtain

$$
\left\{\begin{array}{l}
0 \in \partial P_{\lambda}\left(\left|\beta^{k+1}\right|\right)-X^{\top} d^{k+1}+\mu X^{\top}\left(r^{k}-r^{k+1}\right)+S\left(\beta^{k+1}-\beta^{k}\right) \\
\nabla L_{\tau, *}\left(r^{k+1}\right)=d^{k+1}
\end{array}\right.
$$

From the definition of the augmented Lagrangian function $L_{\mu}$, it follows that

$$
\begin{aligned}
& L_{\mu}\left(\beta^{k+1}, r^{k+1}, d^{k+1}\right)=L_{\mu}\left(\beta^{k+1}, r^{k+1}, d^{k}\right)+\left\langle d^{k}-d^{k+1}, X \beta^{k+1}+r^{k+1}-y\right\rangle \\
& =L_{\mu}\left(\beta^{k+1}, r^{k+1}, d^{k}\right)+\frac{1}{\mu}\left\|d^{k}-d^{k+1}\right\|_{2}^{2}
\end{aligned}
$$

and

$$
\begin{aligned}
L_{\mu}\left(\beta^{k+1}, r^{k}, d^{k}\right) & -L_{\mu}\left(\beta^{k+1}, r^{k+1}, d^{k}\right)=\left(L_{\tau, *}\left(r^{k}\right)-L_{\tau, *}\left(r^{k+1}\right)\right)-\left\langle d^{k}, r^{k}-r^{k+1}\right\rangle \\
& +\frac{\mu}{2}\left(\left\|X \beta^{k+1}+r^{k}-y\right\|_{2}^{2}-\left\|X \beta^{k+1}+r^{k+1}-y\right\|_{2}^{2}\right)
\end{aligned}
$$

Since $L_{\tau, *}$ is a convex function and $d^{k+1}=d^{k}-\mu\left(X \beta^{k+1}+r^{k+1}-y\right)$, we have

$$
\left\{\begin{array}{l}
L_{\tau, *}\left(r^{k}\right)-L_{\tau, *}\left(r^{k+1}\right) \geq\left\langle\nabla L_{\tau, *}\left(r^{k+1}\right), r^{k}-r^{k+1}\right\rangle \\
X \beta^{k+1}+r^{k}-y=\left(d^{k}-d^{k+1}\right) / \mu+\left(r^{k}-r^{k+1}\right) \\
X \beta^{k+1}+r^{k+1}-y=\left(d^{k}-d^{k+1}\right) / \mu
\end{array}\right.
$$




---

Note that $\nabla L_{\tau, *}\left(r^{k+1}\right)=d^{k+1}$, and inserting Equation (56) into Equation (55) yields

$$
L_{\mu}\left(\beta^{k+1}, r^{k+1}, d^{k}\right) \leq L_{\mu}\left(\beta^{k+1}, r^{k}, d^{k}\right)-\frac{\mu}{2}\left\|r^{k}-r^{k+1}\right\|_{2}^{2}
$$

Combining Equations (54) and (57), we get

$$
L_{\mu}\left(\beta^{k+1}, r^{k+1}, d^{k+1}\right) \leq L_{\mu}\left(\beta^{k+1}, r^{k}, d^{k}\right)-\frac{\mu}{2}\left\|r^{k}-r^{k+1}\right\|_{2}^{2}+\frac{1}{\mu}\left\|d^{k}-d^{k+1}\right\|_{2}^{2}
$$

From Lemma 1 and recalling that $L_{\tau, *}(r)=\sum_{i=1}^{n} L_{\tau, *}\left(r_{i}\right)$, we can derive

$$
\left\|d^{k}-d^{k+1}\right\|_{2}=\left\|\nabla L_{\tau, *}\left(r^{k}\right)-\nabla L_{\tau, *}\left(r^{k+1}\right)\right\|_{2} \leq \frac{\sqrt{n}}{\min \{c, \kappa\}}\left\|r^{k}-r^{k+1}\right\|_{2}
$$

Consequently, we have

$$
L_{\mu}\left(\beta^{k+1}, r^{k+1}, d^{k+1}\right) \leq L_{\mu}\left(\beta^{k+1}, r^{k}, d^{k}\right)+\left(\frac{n}{\mu \min \left\{c^{2}, \kappa^{2}\right\}}-\frac{\mu}{2}\right)\left\|r^{k}-r^{k+1}\right\|_{2}^{2}
$$

Note that $\beta^{k+1}$ is the value that minimizes $L_{\mu}\left(\beta, r^{k}, d^{k}\right)+\frac{1}{2}\left\|\beta-\beta^{k}\right\|_{\mathcal{S}}^{2}$ and $\mathcal{S}$ is a positive-definite matrix, then we get

$$
L_{\mu}\left(\beta^{k+1}, r^{k}, d^{k}\right) \leq L_{\mu}\left(\beta^{k}, r^{k}, d^{k}\right)
$$

As a result,

$$
L_{\mu}\left(\beta^{k+1}, r^{k+1}, d^{k+1}\right) \leq L_{\mu}\left(\beta^{k}, r^{k}, d^{k}\right)+\left(\frac{n}{\mu \min \left\{c^{2}, \kappa^{2}\right\}}-\frac{\mu}{2}\right)\left\|r^{k}-r^{k+1}\right\|_{2}^{2}
$$

Since we assume that $\mu>\frac{\sqrt{2 n}}{\min \{c, \kappa\}}$, then we have $\left(\frac{n}{\mu \min \left\{c^{2}, \kappa^{2}\right\}}-\frac{\mu}{2}\right)<0$, which implies that $\left\{L_{\mu}\left(w^{k}\right)\right\}$ is monotonically nonincreasing.

Next, we will prove that the iterative sequences generated by LADMM are bounded.
Lemma 3 (a) With $\mu>\frac{\sqrt{2 n}}{\min \{c, \kappa\}}$ and $X^{\top} X>\mu I_{p}(\mu>0)$, if $P_{\lambda}(\beta)$ is the SCAD,MCP or Capped$\ell_{1}$, then the sequence $w^{k}=\left\{\beta^{k}, r^{k}, d^{k}\right\}$ is generated by LADMM is bounded.
(b) With $\mu>\frac{\sqrt{2 n}}{\min \{c, \kappa\}}$, if $P_{\lambda}\left(\left|\beta^{k}\right|\right)$ is Mnet, Snet or Cnet, then the sequence $w^{k}=$ $\left\{\beta^{k}, r^{k}, d^{k}\right\}$ is generated by LADMM is bounded.




---

Proof Since $\left\{L_{\mu}\left(\mathbf{w}^{k}\right)\right\}$ is monotonically nonincreasing, we have

$$
\begin{aligned}
L_{\mu}\left(\mathbf{w}^{0}\right) & \geq L_{\mu}\left(\mathbf{w}^{k}\right)=L_{\tau, *}\left(\mathbf{r}^{k}\right)+P_{\lambda}\left(\left|\boldsymbol{\beta}^{k}\right|\right)-\left\langle\mathbf{d}^{k}, X \boldsymbol{\beta}^{k}+\mathbf{r}^{k}-\mathbf{y}\right\rangle+\frac{\mu}{2}\left\|X \boldsymbol{\beta}^{k}+\mathbf{r}^{k}-\mathbf{y}\right\|_{2}^{2} \\
& =L_{\tau, *}\left(\mathbf{r}^{k}\right)+P_{\lambda}\left(\left|\boldsymbol{\beta}^{k}\right|\right)+\frac{\mu}{2}\left\|X \boldsymbol{\beta}^{k}+\mathbf{r}^{k}-\mathbf{y}-\mathbf{d}^{k} / \mu\right\|_{2}^{2}-\frac{\left\|\mathbf{d}^{k}\right\|_{2}^{2}}{2 \mu}
\end{aligned}
$$

Because the value of $\mathbf{w}^{0}$ is given, $L_{\mu}\left(\mathbf{w}^{0}\right)$ is a bounded constant. The first-order optimal condition of the algorithm in (53) indicates that $\nabla L_{\tau, *}\left(\mathbf{r}^{k}\right)=\mathbf{d}^{k}$.
(a) From the expressions of $P_{\lambda}\left(\left|\boldsymbol{\beta}^{k}\right|\right)$ and $\nabla P_{\lambda}\left(\left|\boldsymbol{\beta}^{k}\right|\right)$, it can be seen that they are both positive and bounded functions. Then $\mathbf{d}^{k}$ and $P_{\lambda}\left(\left|\boldsymbol{\beta}^{k}\right|\right)-\frac{\left\|\mathbf{d}^{k}\right\|_{2}^{2}}{2 \mu}$ are bounded. Since $L_{\mu}\left(\mathbf{w}^{0}\right)$ is a bounded constant, both $L_{\tau, *}\left(\mathbf{r}^{k}\right) \geq 0$ and $\frac{\mu}{2}\left\|X \boldsymbol{\beta}^{k}+\mathbf{r}^{k}-\mathbf{y}-\mathbf{d}^{k} / \mu\right\|_{2}^{2} \geq 0$ are bounded.

Note that if $\left\|\mathbf{r}^{k}\right\|_{2} \rightarrow \infty, L_{\tau, *}\left(\mathbf{r}^{k}\right) \rightarrow \infty$, then $\mathbf{r}^{k}$ must be bounded. Then, as long as $X^{\top} X>\mu I_{p}$ $\boldsymbol{\beta}^{k}$ must be bounded. Consequently, $\mathbf{w}^{k}$ is bounded.
(b) If $P_{\lambda}\left(\left|\boldsymbol{\beta}^{k}\right|\right)$ is Mnet, Snet or Cnet, then $P_{\lambda}\left(\left|\boldsymbol{\beta}^{k}\right|\right)-\frac{\left\|\mathbf{d}^{k}\right\|_{2}^{2}}{2 \mu} \rightarrow \infty$ as long as $\left\|\boldsymbol{\beta}^{k}\right\|_{2} \rightarrow \infty$.

Hence, $\boldsymbol{\beta}^{k}$ is bounded. For the same reason as (a), we can also conclude that both $L_{\tau, *}\left(\mathbf{r}^{k}\right) \geq 0$ and $\frac{\mu}{2}\left\|X \boldsymbol{\beta}^{k}+\mathbf{r}^{k}-\mathbf{y}-\mathbf{d}^{k} / \mu\right\|_{2}^{2} \geq 0$ are bounded. $L_{\tau, *}\left(\mathbf{r}^{k}\right)$ is bounded, indicating that $\mathbf{r}^{k}$ is bounded.
When $\boldsymbol{\beta}^{k}, \mathbf{r}^{k}$ and $\frac{\mu}{2}\left\|X \boldsymbol{\beta}^{k}+\mathbf{r}^{k}-\mathbf{y}-\mathbf{d}^{k} / \mu\right\|_{2}^{2}$ are all bounded, it can be deduced that $\mathbf{d}^{k}$ is also bounded. Consequently, $\mathbf{w}^{k}$ is bounded.

Now, we derive the conclusions $\lim _{k \rightarrow+\infty}\left\|\mathbf{w}^{k}-\mathbf{w}^{k+1}\right\|_{2}=0$, which is similar to Guan et al. (2018). However, this conclusion only ensures that the difference between $\mathbf{w}^{k}$ and $\mathbf{w}^{k+1}$ is close to 0 , but it does not guarantee that $\mathbf{w}^{k}$ converges.
Lemma 4 Let the sequence $\mathbf{w}^{k}=\left\{\boldsymbol{\beta}^{k}, \mathbf{r}^{k}, \mathbf{d}^{k}\right\}$ be generated by LADMM, then $\sum_{k=0}^{\infty}\left\|\mathbf{w}^{k}-\mathbf{w}^{k+1}\right\|_{2}^{2}<+\infty$ and $\lim _{k \rightarrow+\infty}\left\|\mathbf{w}^{k}-\mathbf{w}^{k+1}\right\|_{2}=0$

Proof From Lemma 2, we know $\left\{L_{\mu}\left(\mathbf{w}^{k}\right)\right\}_{k=1}^{+\infty}$ is a monotonically nondecreasing sequence. Next, we will determine its lower bound to demonstrate that $\left\{L_{\mu}\left(\mathbf{w}^{k}\right)\right\}_{k=1}^{+\infty}$ is convergent.

Since $\left\{\mathbf{w}^{k}\right\}$ is bounded, it has at least one cluster point. We assume $\mathbf{w}^{*}$ to be an arbitrary cluster point of $\left\{\mathbf{w}^{k}\right\}$. Let $\left\{\mathbf{w}^{k_{j}}\right\}$ represent the subsequence of $\left\{\mathbf{w}^{k}\right\}$ that converges to $\mathbf{w}^{*}$, i.e. $\mathbf{w}^{k_{j}} \rightarrow \mathbf{w}^{*}$. Since $L_{\mu}$ is a continuous function and $\left\{L_{\mu}\left(\mathbf{w}^{k}\right)\right\}_{k=1}^{+\infty}$ a monotonically nondecreasing sequence, then we have

$$
L_{\mu}\left(\mathbf{w}^{k_{j}}\right) \geq L_{\mu}\left(\lim _{j \rightarrow \infty} \mathbf{w}^{k_{j}}\right)=L_{\mu}\left(\mathbf{w}^{*}\right)
$$




---

As a result, $\left\{L_{\mu}\left(w_{k_{j}}\right)\right\}_{j=1}^{+\infty}$ has a lower bound, which, together with the fact that $\left\{L_{\mu}\left(w_{k_{j}}\right)\right\}_{j=1}^{+\infty}$ is nonincreasing, means that $\left\{L_{\mu}\left(w_{k_{j}}\right)\right\}_{j=1}^{+\infty}$ converges to $L_{\mu}\left(w^{*}\right)$. Considering that $\left\{L_{\mu}\left(w_{k}\right)\right\}_{k=1}^{+\infty}$ is monotonically nonincreasing, it follows that $L_{\mu}\left(w^{*}\right)$ also serves as the lower bound for $\left\{L_{\mu}\left(w_{k}\right)\right\}_{k=1}^{+\infty}$. Thus, we can conclude that $\left\{L_{\mu}\left(w_{k}\right)\right\}_{k=1}^{+\infty}$ converges to $L_{\mu}\left(w^{*}\right)$ and $L_{\mu}\left(w_{k}\right) \geq L_{\mu}\left(w^{*}\right)$.
Rearranging terms of Equation (51) yields

$$
\left(\frac{\mu}{2}-\frac{n}{\mu \min \left\{c^{2}, \kappa^{2}\right\}}\right)\left\|r_{k}-r_{k+1}\right\|_{2}^{2} \leq L_{\mu}\left(w_{k}\right)-L_{\mu}\left(w_{k+1}\right)
$$

and summing up for $k=0, \ldots,+\infty$, it follows

$$
\left(\frac{\mu}{2}-\frac{n}{\mu \min \left\{c^{2}, \kappa^{2}\right\}}\right) \sum_{k=0}^{+\infty}\left\|r_{k}-r_{k+1}\right\|_{2}^{2} \leq L_{\mu}\left(w_{0}\right)-L_{\mu}\left(w^{*}\right)<+\infty
$$

Since $\left(\frac{\mu}{2}-\frac{n}{\mu \min \left\{c^{2}, \kappa^{2}\right\}}\right)>0$, we obtain $\sum_{k=0}^{+\infty}\left\|r_{k}-r_{k+1}\right\|_{2}^{2}<+\infty$. Consequently, it follows from Equation (59) that $\sum_{k=0}^{+\infty}\left\|d_{k}-d_{k+1}\right\|_{2}^{2}<+\infty$. Hence, to complete the proof, we just need to prove that $\sum_{k=0}^{+\infty}\left\|\beta_{k}-\beta_{k+1}\right\|_{2}^{2}<+\infty$. For the $m$-th local machine, we have

$$
\left\{\begin{array}{l}
d_{m}^{k+1}=d_{m}^{k}-\mu\left(X_{m} \beta_{k+1}+r_{m}^{k+1}-y_{m}\right) \\
d_{m}^{k}=d_{m}^{k-1}-\mu\left(X_{m} \beta_{k}+r_{m}^{k}-y_{m}\right)
\end{array}\right.
$$

and make a difference between the above two equations to obtain

$$
d_{m}^{k+1}-d_{m}^{k}=d_{m}^{k}-d_{m}^{k-1}-\mu X_{m}\left(\beta_{k+1}-\beta_{k}\right)-\mu\left(r_{m}^{k+1}-r_{m}^{k}\right)
$$

It then follows that

$$
\begin{aligned}
& \left\|\mu X_{m}\left(\beta_{k+1}-\beta_{k}\right)\right\|_{2}^{2}=\left\|\left(d_{m}^{k}-d_{m}^{k-1}\right)-\left(d_{m}^{k+1}-d_{m}^{k}\right)-\mu\left(r_{m}^{k+1}-r_{m}^{k}\right)\right\|_{2}^{2} \\
\leq & 3\left(\left\|d_{m}^{k}-d_{m}^{k-1}\right\|_{2}^{2}+\left\|d_{m}^{k+1}-d_{m}^{k}\right\|_{2}^{2}+\left\|\mu\left(r_{m}^{k+1}-r_{m}^{k}\right)\right\|_{2}^{2}\right)
\end{aligned}
$$

Since $X_{m}^{\top} X_{m}>\mu I_{p}$, we have

$$
\left\|\mu X_{m}\left(\beta_{k+1}-\beta_{k}\right)\right\|_{2}^{2} \geq \mu^{2} \mu\left\|\beta_{k+1}-\beta_{k}\right\|_{2}^{2}
$$




---

Then, combining Equations (64) and (65), we obtain

$$
\sum_{k=0}^{+\infty}\left\|\beta_{k}-\beta_{k+1}\right\|_{2}^{2}<+\infty
$$

Since $w_{k}=\left\{\beta_{k}, r_{k}, d_{k}\right\}$, we have established that

$$
\sum_{k=1}^{\infty}\left\|w_{k}-w_{k+1}\right\|_{2}^{2} \leq \infty
$$

which indicates that $\lim _{k \rightarrow+\infty}\left\|w_{k}-w_{k+1}\right\|_{2}=0$.
From Lemma 2, we know that $w_{k}$ has at least one limit point. Next, we will prove some properties about the limit points.

Lemma 5 Let $\mathcal{S}\left(w_{\infty}\right)$ denote the set of the limit points of $\left\{w_{k}\right\}$, and $\mathcal{C}\left(w^{*}\right)$ denote the set of critical points of $L_{\mu}(w) . \mathcal{S}\left(w_{\infty}\right) \subset \mathcal{C}\left(w^{*}\right)$, and $L_{\mu}(w)$ is finite and constant on $\mathcal{S}\left(w_{\infty}\right)$.

Proof We say $w^{*}$ is the critical point of $L_{\mu}(w)$, if it satisfies

$$
\left\{\begin{array}{l}
X^{\top} d^{*} \in \partial P_{\lambda}\left(\left|\beta^{*}\right|\right) \\
d^{*}=\nabla L_{\tau, *}\left(r^{*}\right) \\
0=X \beta^{*}+r^{*}-y
\end{array}\right.
$$

Recalling the optimality conditions for the $k+1$-iteration of LADMM in (52), we get

$$
\left\{\begin{array}{l}
0 \in \partial P_{\lambda}\left(\left|\beta_{k+1}\right|\right)-X^{\top} d_{k+1}+\mathcal{S}\left(\beta_{k+1}-\beta_{k}\right) \\
0=\nabla L_{\tau, *}\left(r_{k+1}\right)-d_{k+1} \\
d_{k+1}=d_{k}-\mu\left(X \beta_{k+1}+r_{k+1}-y\right)
\end{array}\right.
$$

For any limit point $w_{\infty}$, from Lemma 4 and (67), we have

$$
\left\{\begin{array}{l}
X^{\top} d_{\infty} \in \partial P_{\lambda}\left(\left|\beta_{\infty}\right|\right) \\
d_{\infty}=\nabla L_{\tau, *}\left(r_{\infty}\right) \\
0=X \beta_{\infty}+r_{\infty}-y
\end{array}\right.
$$

As a consequence, $\mathcal{S}\left(w_{\infty}\right) \subset \mathcal{S}\left(w^{*}\right)$. Note that $L_{\mu}\left(w_{k}\right)$ is nonincreasing and convergent, it follows that $L_{\mu}(w)$ is finite and constant on $\mathcal{S}\left(w_{\infty}\right)$.




---

Now, we are ready to prove Theorem 2. Despite the LADMM algorithm having an additional term of $\frac{1}{2}\left\|\boldsymbol{\beta}-\boldsymbol{\beta}^{k}\right\|_{\mathcal{S}}^{2}$ compared to the ADMM algorithm in Guo et al. (2016), the convergence proof is similar to that of Theorem 3.1 in Guo et al. (2016). Following the same steps as their proof, we can conclude that

$$
\sum_{k=0}^{+\infty}\left\|\mathbf{w}^{k+1}-\mathbf{w}^{k}\right\|^{2}<+\infty
$$

That is, $\left\{\mathbf{w}^{k}\right\}$ is a Cauchy sequence and it converges. Together with Lemma $5,\left\{\mathbf{w}^{k}\right\}$ converges to a critical point of $L_{\mu}(\mathbf{w})$.

# C Supplementary Numerical Experiments 

## C. 1 Supplementary Experiments for Section 4.1

In this subsection, we analyze the performance of LADMM in comparison to other ADMM variants when solving quantile regression with the MCP $(a=3)$ penalty. Table 6 presents the results for the nonparallel version $(M=1)$, while Table 7 displays the results for the parallel version $(M>1)$. The results of Table 6 and Table 7 demonstrate that our proposed LADMM algorithm has significant advantages over existing algorithms when solving quantile regressions with MCP.


* The meanings of the notations used in this table are as follows: $\mathrm{P}_{1}$ (\%): proportion that $x_{1}$ is selected; $\mathrm{P}_{2}$ (\%): proportion that $x_{6}, x_{12}, x_{15}$, and $x_{20}$ are selected; AE: absolute estimation error; Ite: number of iterations; Time (s): CPU running time. Numbers in the parentheses represent the corresponding standard deviations, and the optimal solution is represented in bold.




---



\footnotetext{
* P1 and P2 are not presented in Table 7 because all methods have a value of 100 for these two metrics. The Nonzero, AE, and Ite of LADMM are not greatly affected by the $M$ value.

# C. 2 Supplementary Experiments for Section 4.2 

In this subsection, we compare the performance of LADMM and ILAMM in solving least squares regressions with MCP $(a=3)$ and Capped- $\ell_{1}(a=3)$ when the errors follow the Lognormal distribution. Table 8 shows that our algorithm performs better than ILAMM in solving these two regression models.


\footnotetext{
* False Positive (FP) refers to the number of variables with a coefficient of zero that are mistakenly included in the final model, while False Negative (FN) refers to the number of variables with non-zero coefficients that are omitted from the model.

## C. 3 Supplementary Experiments for Section 4.3

In this subsection, we compare the performance of LADMM and cdaSQR in solving smooth quantile regressions with Mnet $(a=3)$ penalty. Table 9 demonstrates the results. However, cdaSQR is




---

unable to solve the Cnet $(a=3)$ penalty. Therefore, we solely present the results of LADMM in solving Cnet regression in Figure 6. Both the table and the figure illustrate the superiority of our algorithm.






---

# C. 4 Supplementary Experiments for Section 5 

In this subsection, we compare the performance of several parallel ADMM algorithms in solving MCP $(a=3)$ regressions using an online publicly available dataset. The results $(M=100)$ are presented in Table 10. The prediction error (PE) is calculated as follows: $\mathrm{PE}=\frac{1}{n_{\text {test }}} \sum_{i=1}^{n_{\text {test }}}\left|y_{i}-\hat{y}_{i}\right|$, where $n_{\text {test }}=4644$ represents the size of the test set. The experimental results presented in Table 10 indicate that our LADMM algorithm exhibits competitive performance compared to other parallel algorithms when solving MCP $(a=3)$ regressions on this real dataset.






---

