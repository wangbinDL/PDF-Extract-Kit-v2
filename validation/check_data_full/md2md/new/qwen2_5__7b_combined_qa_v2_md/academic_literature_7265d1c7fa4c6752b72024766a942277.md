# A communication-efficient, online changepoint detection method for monitoring distributed sensor networks 

Ziyang Yang ${ }^{1}$, Idris A. Eckley ${ }^{2}$, and Paul Fearnhead ${ }^{3}$<br>${ }^{1}$ STOR-i Doctoral Training Centre, Lancaster University<br>${ }^{2,3}$ Department of Mathematics and Statistics, Lancaster University

April 11, 2024


#### Abstract

We consider the challenge of efficiently detecting changes within a network of sensors, where we also need to minimise communication between sensors and the cloud. We propose an online, communication-efficient method to detect such changes. The procedure works by performing likelihood ratio tests at each time point, and two thresholds are chosen to filter unimportant test statistics and make decisions based on the aggregated test statistics respectively. We provide asymptotic theory concerning consistency and the asymptotic distribution if there are no changes. Simulation results suggest that our method can achieve similar performance to the idealised setting, where we have no constraints on communication between sensors, but substantially reduce the transmission costs.


Keywords: Changepoints, MOSUM, Online, Real-time analysis, Internet of Things, Distributed computing.



# 1 Introduction 

During the last decade, there has been a significant focus on the important challenge of efficient and accurate detection of changes in both univariate and multivariate data sequences (Cho and Fryzlewicz, 2014; Fisch et al., 2022; Kovács et al., 2023; Truong et al., 2020; Tveten et al., 2022; Wang and Samworth, 2018). More recently, focus has turned to translating the efficiency of such approaches to the online setting, typically motivated by an applied challenge such as how to deal with limited computational power (e.g. Ward et al., 2024). Recent major contributions to the online setting include Adams and MacKay (2007), Tartakovsky et al. (2014), Yu et al. (2023), Chen et al. (2022), and Romano et al. (2023). In this paper we consider a less studied scenario, monitoring edge-behaviour within distributed sensor networks, which are common architectures within the Internet of Things framework (IoT). The importance of efficiently detecting changes at the edge efficiently, whilst minimising communication between sensors and the cloud is perhaps best appreciated by considering two key applications: detecting cyber-attacks on smart cities (Alrashdi et al., 2019) and optimising the performance of base stations (Wu et al., 2019).

Consider, by way of example, Figure 1 which shows a schematic representation of real-time monitoring within a distributed network. Here we assume that $d$ data streams are monitored, each by its own sensor. Communication between the sensors and the centre is possible as shown by the dashed lines. An unusual event happens at time $\tau$, and we want to detect this event as quickly as possible. However, in modern sensor networks that deploy IoT devices the computational resources of the sensors can be substantial. Moreover, communication between the sensors and the cloud can be problematic due to the heavy energy usage involved with transmitting data (Varghese et al., 2016; Pinto and Castor, 2017). As such, we need algorithms that can identify the time when it is important for information to be shared with the cloud. More specifically, in this article, we seek to develop a new method to detect changes within such a network in real time with high statistical power and as little communication and computation as possible.

Changepoint methods which can be applied in the fully centralised problem, when the data





Figure 1: Schematic representation of a sensor network made up of $d$ sensors, where $S_{i}$ is the index for sensor $i, X_{i, t}$ is the data observed at sensor $i$, and $M_{i, t}$ is the message transmitted from sensor $i$ to centre at time $t$.
from the sensors is processed and transmitted to the centre (cloud) at every time step, are well studied. Approaches typically seek to calculate the maximum or the sum of all the test statistics (see, e.g., Mei, 2010; Xie and Siegmund, 2013; Chan, 2017; Chen et al., 2022; Gösmann et al., 2022). The rationale behind these methods is to set thresholds and raise the alarm if the aggregated test statistics from multiple streams exceed pre-defined thresholds. Numerical experiments (Mei, 2010) indicate that taking the maximum is the optimal method when there are only a few affected data streams - what we will term a sparse change. Conversely, taking the sum is optimal when most data streams are affected, also known as a dense change. Recent contributions to this distributed problem include (Rago et al., 1996; Veeravalli, 2001; Appadwedula et al., 2005; Mei, 2005, 2011; Tartakovsky and Kim, 2006; Banerjee and Veeravalli, 2015). Among them, two recent papers of particular interest develop communication efficient schemes for monitoring a large number of data streams (Zhang and Mei, 2018; Liu et al., 2019). The key idea is that each sensor computes a local monitoring statistic and then employs a thresholding step, only sending the statistic to the centre if there is some evidence of a change. The information from multiple sensors is then combined at the centre. This approach reduces unnecessary transmission by ignoring streams with little evidence for



a change, while only focusing on data streams that show signs of change. Although computationally feasible, existing works assume that the pre- and post-change mean are known. In practice, the pre-change mean can be estimated based on historical data. However assuming a known post-change mean is typically unrealistic in practice, with an incorrect value potentially leading to a failure to detect, or poor detection power. Liu et al (2019) approximate the post-change mean recursively but, as a consequence, somewhat sacrifice statistical power of the algorithm.

Our approach builds on recent work developing the moving sum (MOSUM) as a windowbased changepoint methods (see, e.g., Aue et al., 2012; Kirch and Kamgaing, 2015; Kirch and Weber, 2018). Specifically, we propose an online communication-efficient changepoint detection algorithm (distributed MOSUM) to detect changes in real-time within the distributed network setting. A local threshold is chosen to filter out unimportant information and only transmit the statistically important test statistic to the centre. The change will be alarmed when the aggregated test statistic exceeds the pre-defined global threshold in the central cloud. The low time complexity and communication efficient scheme of our proposed method makes it suitable for online monitoring. We also establish that the proposed method can achieve similar statistical power as the idealistic setting, where there is no communication constraint, at detecting large changes whilst substantially reducing the transmission cost. Moreover, we also show how to make the detection performance of distributed MOSUM close to that of the idealised setting by increasing the window size, which will only sacrifice the storage cost and a little transmission cost.

The key differences between our work and previous distributed changepoint detection contributions (e.g., Liu et al., 2019) are: Firstly, a moving window-based test statistic MOSUM is chosen to avoid the requirement of knowledge of the post-change mean. Secondly, earlier works have been based on the framework that controls the average run length (ARL) the average amount of time until incorrectly detect a change. However, such a metric gives a somewhat limited amount of information since the distribution of run length is usually



unknown. For instance, if multiple procedures end quickly while a few replications stop significantly longer, the ARL would be the same if all the replications terminated around the same time. Conversely, in this work, we present methods in terms of controlling the error rate under the null at a specific level, and with asymptotic power 1 under alternatives. Furthermore, our ideas generalise trivially to methods controlling the average run length.

The structure of this paper is as follows. In Section 2, the problem setting is outlined, before introducing the distributed MOSUM methodology in Section 3. Several theoretical results for this new approach are given in Section 4. Simulation studies are carried out in Section 5, before ending with some concluding remarks (Section 6).

# 2 Problem setting 

We begin by assuming that we have $d$ sensors, each of which is observed as follows: $\mathbf{X}_{t}=$ $\left(X_{1, t}, X_{2, t}, X_{3, t} \ldots, X_{d, t}\right)$ at every time point $t \in \mathbb{N}$. Here $\mathbf{X}_{t}$ could be raw data or the residuals after pre-processing the data. These observations are assumed to be identically distributed and independent across series. Such assumptions are common in the problem of detecting changes within a distributed system setting (Tartakovsky and Veeravalli, 2002; Mei, 2010; Xie and Siegmund, 2013; Liu et al., 2019). We do not strictly assume time independence here, but our method is optimal when this assumption holds. Moreover, the impact of time dependence will be numerically studied in Section 5.3.

We begin by assuming that at some unknown time, $\tau$, the distribution of some unknown subsets of $d$ sensors will change. For simplicity, we only consider change in mean, but the ideas below are easily extended to other changepoint settings. Therefore, in this illustrative change in mean setting, the model for the data is expressed as follows:

$$
X_{i, t}=\mu_{i}+\delta_{i} 1_{\{t>\tau\}}+\epsilon_{i, t}, \quad t \in \mathbb{N}, 1 \leq i \leq d
$$

where $\mu_{i}$ is the known pre-change mean, $\delta_{i}$ is the mean shift, and $\left\{\epsilon_{i, t}: t \in \mathbb{N}\right\}$ are strictly stationary error sequences. After time $\tau$, the mean of the $i$-th data stream changes immedi-



ately from $\mu_{i}$ to $\mu_{i}+\delta_{i}$. Here it is useful to note that our setting also permits some $\delta_{i}=0$, which means that only a subset of data streams are affected by the change. Without loss of generality, we assume $\mu_{i}=0$. Under the null hypothesis, the model for the data can be rewritten as

$$
X_{i, t}=\epsilon_{i, t}, \quad t \in \mathbb{N}, 1 \leq i \leq d
$$

Moreover under the alternative hypothesis, the model is $X_{i, t}=\delta_{i}+\epsilon_{i, t}, t \in \mathbb{N}, 1 \leq i \leq d$. Our aim is to monitor such a system and raise the alarm as soon as possible following the event at time $\tau$. One way of achieving this is to perform hypothesis testing sequentially, i.e., evaluate the null hypothesis of no change in mean at each time point $t \in \mathbb{N}$. The algorithm will stop and declare a change when we can reject the null hypothesis.

In the classical sequential changepoint detection problem, we evaluate the performance of an algorithm subject to a constraint on its false alarm rate. First, consider an open-ended stopping rule where the algorithm never we have an infinite time-window of measurements and the algorithm never halts until it detects a change. The false alarm rate can be evaluated in two ways. Assume there is no change, and let $\widehat{\tau}$ be the time at which we detect a change, with the convention that $\widehat{\tau}=\infty$ if we detect no change. One approach is to control the average run length, $\mathrm{E}_{\infty}(\widehat{\tau})$, the expected time of to a false alarm. This makes sense for procedures with a constant threshold for detection, for which we are certain to detect a change under the Null if we monitor for an infinite time period. Alternatively, one can control the false alarm rate, $\mathrm{P}_{\infty}(\widehat{\tau}<\infty)$, the probability of a false alarm. To control this over an infinite time horizon requires increasing the threshold for detecting a change over time. Equivalently, this can be achieved by multiplying the test statistic with a weight function $w(\cdot)<1$. See Leisch et al. (2000); Zeileis et al. (2005); Horváth et al. (2008); Aue et al. (2012); Kirch and Kamgaing (2015); Weber (2017); Yau et al. (2017); Kirch and Weber (2018); Kengne and Ngongo (2022) for examples of how to choose an appropriate weight function.

In our paper, we focus on controlling the false alarm rate. However Aue et al. (2012) states that "applying open-ended procedures built from the asymptotic critical values have a ten-



dency to be too conservative in finite samples". Therefore, our paper considers a close-ended stopping rule. In this approach, the algorithm will stop either upon detecting a change or upon reaching the predefined monitoring time $T$. We thus control the false alarm rate over a time time window of length $T$. However, the ideas we present can easily be adapted to the open-ended setting, and also to methods which control the average run length.

Under the context of distributed changepoint detection problem, we additionally evaluate the index - the average transmission cost $\bar{\Delta}$. This is the average number of transmissions at each time step for $d$ sensors, and should be smaller than the pre-specified transmission cost $\Delta$.

Before introducing our proposed method, we first review relevant work. At time $t$, the local monitoring statistic, $T_{i}$ is calculated for the $i^{t h}$ stream. Then all the local statistics $T_{i}$ can be combined into a global monitoring statistic $T$ at the fusion centre. There are two common choices of message combinations for monitoring changes within the distributed system. One of these two types, the SUM scheme (Mei, 2010), declares a change when the sum of all the local monitoring statistics exceeds a pre-defined threshold, that is:

$$
\hat{\tau}_{\text {sum }}\left(c_{\text {Global }}\right)=\inf \left\{t \geq 1: T \geq c_{\text {Global }}\right\}=\inf \left\{t \geq 1: \sum_{i=1}^{d} T_{i} \geq c_{\text {Global }}\right\}
$$

where $c_{\text {Global }}$ is global threshold. This way of combining statistics across streams is known to be good if the series are independent and the changes are dense. However, implementing this method on the distributed system requires sending every $T_{i}$ to the fusion centre, which is expensive. A sum-shrinkage method (Liu et al., 2019) is proposed to reduce the communication cost by thresholding the test statistics before summing them:

$$
\hat{\tau}_{\text {sum }}\left(c_{\text {Local }}, c_{\text {Global }}\right)=\inf \left\{t \geq 1: T \geq c_{\text {Global }}\right\}=\inf \left\{t \geq 1: \sum_{i=1}^{d} T_{i} I\left(T_{i} \geq c_{\text {Local }}\right) \geq c_{\text {Global }}\right\}
$$

Empirically the sum-shrinkage method could achieve similar performance as the SUM scheme in the dense case and surprisingly performs better in the sparse case.



When the change is sparse, it has been shown both theoretically and empirically (Mei, 2010; Liu et al., 2019; Chen et al., 2022) that monitoring the maximum of the test-statistics across series is best. In such a setting, the MAX procedure (Tartakovsky and Veeravalli, 2002) monitors the maximum of test statistics and raises the alarm when the maximum of the local test statistics exceeds the thresholds, that is:

$$
\hat{\tau}_{\max }\left(c_{\text {Global }}\right)=\inf \left\{t \geq 1: T \geq c_{\text {Global }}\right\}=\inf \left\{t \geq 1: \max _{1 \leq i \leq d} T_{i} \geq c_{\text {Global }}\right\}
$$

The best choice of different schemes depends on the sparsity of changes which is based on the number of affected data streams $p$. This can be made precise if we consider an asymptotic setting where $p \rightarrow \infty$ (Enikeeva and Harchaoui, 2019), and define a change to be sparse if the number of affected streams is $p=o(\sqrt{d})$, and it to be a dense change otherwise. A recent paper (Chen et al., 2022) combines both SUM procedure and MAX procedure to achieve good performance regardless of the sparsity. In the context of distributed monitoring, the MAX procedure is trivially implemented without any communication. Specifically, each sensor has the threshold for the max-statistic and flags a change if their local statistic is above this threshold. Therefore, within this paper, we only focus on developing a communicationefficient version of the SUM scheme. Our aim is a method that performs well for dense changes, but limits the communication cost. We will use the SUM scheme as the ideal method to compare against since it has no restrictions on communication.

# 3 Distributed change point detection method 

Our proposed methodology is summarized in Algorithm 1, and described in detail below. The method essentially comprises of three steps. The first step involves the parallel local monitoring of each data stream by the sensors. As the monitoring unfolds, messages are occasionally sent from the sensors to the centre to indicate the presence of a potential change. Finally, at the centre these messages are aggregated to find changes that occur across a number of data streams.



```
Algorithm 1: Centralized and distributed MOSUM
    input : historic data $x_{i, t}$ for $i=1,2, \ldots, d$, and $1 \leq t \leq m$
    Estimating the baseline parameters // can be done offline
    for $i=1$ to $d$ do
        estimate $\hat{\mu}_{i}$ and $\hat{\sigma}_{i}$
    end
    Data: $x_{i, t}$ for $i=1,2, \ldots, d$ at time $t$
    while change is detected or reached the maximum monitoring time $T$ do
        Local monitoring // parallel computing
        for $i=1$ to $d$ do
            $T_{i}(m, k, h)=\frac{1}{\hat{\sigma}_{i}}\left|\sum_{t=m+k-h+1}^{m+k} \mathcal{X}_{i, t}-\hat{\mu}_{i}\right|$
        end
        Message passing
        if $w(k, h) T_{i}(m, k, h)>c_{\text {Local }}$ then
            $M_{i, t}=T_{i}(m, k, h) \quad / /$ centralized scheme:set $c_{\text {Local }}=0$
        else $M_{i, t}=0 ;$
        end
        Global monitoring
        if $w(k, h) \sqrt{\sum_{i}^{d} M_{i, t}}>c_{\text {Global }}$ then
            stop the algorithm
            output: $\hat{\tau}=t$
        end
        $t \leftarrow t+1$
    end
```


# 3.1 Local monitoring 

### 3.1.1 Estimating the baseline parameters

Our sequential testing approach requires a historic data set of length $m$ to estimate the baseline parameters. Theoretical results are obtained later in the paper when $m \rightarrow \infty$. The parameters of interest are the mean of each data stream $\mu_{i}$ and the variance of the errors $\sigma_{i}^{2}$. For the $i$ th data stream these estimates are,

$$
\hat{\mu}_{i}=\frac{1}{m} \sum_{t=1}^{m} X_{i, t}, \quad \hat{\sigma}_{i}^{2}=\frac{1}{m} \sum_{t=1}^{m}\left(X_{i, t}-\hat{\mu}_{i}\right)^{2}
$$



If the errors cannot be assumed to be independent we can estimate the long run variance. This requires specifying a kernel function $K(\cdot)$ :

$$
\hat{\sigma}_{i}^{2}=\frac{1}{m} \sum_{t=1}^{m}\left(X_{i, t}-\hat{\mu}_{i}\right)^{2}+2 \sum_{j=1}^{m-1} K\binom{j}{l} \hat{\gamma}_{j}^{(i)}
$$

where $\hat{\gamma}_{j}^{(i)}=\frac{1}{m-j} \sum_{t=1}^{m-j}\left(X_{i, t}-\hat{\mu}_{i}\right)\left(X_{i, t+j}-\hat{\mu}_{i}\right)$.
In this setting, the Kernel function can be seen as a weighting function for sample covariance $\hat{\gamma}_{j}^{(i)}$. The kernel function must be symmetric and such that $K(0)=1$. Various kernel functions are proposed. Standard kernel functions include Truncated (White and Domowitz, 1984), Bartlett (Newey and West, 1986) and Parzen (Gallant, 2009) amongst others. Among them, the Bartlett kernel is frequently used in Econometrics. This kernel takes the form:

$$
K_{\text {Bartlett }}\binom{j}{l}= \begin{cases}1-\frac{j}{l}, & \text { for } 0 \leq j \leq l-1 \\ 0, & \text { otherwise }\end{cases}
$$

For more details, see Horváth and Hušková (2012); Kiefer and Vogelsang (2002a); Kiefer and Vogelsang (2002b).

# 3.1.2 Starting local monitoring 

Once the baseline parameters have been estimated, beginning at time $m+1$ data $X_{i, m+1}, X_{i, m+2}, \ldots$ are observed sequentially and monitored for a change. This is achieved using a MOSUM statistic which at monitoring time, $k$, takes a window containing the most recent $h$ observations:

$$
T_{i}(m, k, h)=\frac{1}{\hat{\sigma}_{i}}\left|\sum_{t=m+k-h+1}^{m+k}\left(X_{i, t}-\hat{\mu}_{i}\right)\right|
$$

Following Aue et al. (2012), the MOSUM statistic will declare a change at time $k$ when the weighted local MOSUM statistic $w(k, h) T_{i}(m, k, h)$ exceeds a pre-defined threshold. A weight function $w(\cdot, \cdot)$ is introduced to control the asymptotic size of the detection procedure.



Typically $w(\cdot, \cdot)$ depends on the monitoring time $k$, and the window size $h$,

$$
w(k, h)=\frac{1}{\sqrt{h}} \rho\left(\frac{k}{h}\right)
$$

for some appropriate $\rho(\cdot)$. The choice of the weight function controls the sensitivity of the test. A wide range of weight functions can be used as long as they are continuous functions that satisfy $\inf _{0 \leq t \leq T} \rho(t)>0$. In this paper, we use the weight function proposed in Leisch et al. (2000) and Zeileis et al. (2005):

$$
\rho(t)=\max (1, \log (1+t))^{-1 / 2}
$$

Intuitively, if there is no change the weighted MOSUM will remain small, but it will be large if there is a change. Figure 2 gives the behavior of weighted MOSUM statistic under the null and the alternative assumptions for one data stream.

# 3.2 Message passing 

The local monitoring described in the previous section is applied to each sensor independently. In order to make global decisions about the state of the system, messages from the sensors must be passed to the central hub (see Figure 1). However, since there are constraints on communication in the system, the message passing process must be carefully designed.

At time $t=m+k$, where $m$ is the historic period of length $m$, and $k$ is the monitoring time, each sensor makes a decision as to whether or not to transmit a message to the centre. This message vector is denoted as $\mathcal{M}_{t}=\left(\mathcal{M}_{1, t}, \mathcal{M}_{2, t}, \ldots, \mathcal{M}_{d, t}\right)$. We consider two different messaging regimes:

- Centralized messaging regime: $\mathcal{M}_{t}=T_{i}(m, k, h)$.
- Distributed messaging regime:

$$
\mathcal{M}_{i, t}= \begin{cases}T_{i}(m, k, h) & \text { if } w(k, h) T_{i}(m, k, h)>c_{\text {Local }} \\ \text { NULL } & \text { otherwise }\end{cases}
$$





Figure 2: Example time series with no change (a) and a single change (b) in the top row. The bottom row shows the weighted MOSUM statistic with a historic period of length $m=100$ and a window size of $h=50$.

The centralized massaging regime is one where there is no constraint on the communication between the sensors and the centre, so all sensors send a message to the centre at each time instant. This is similar to the "SUM" scheme changepoint detection method proposed by Mei (2010). However, when communication is expensive, a "distributed" messaging regime can be used where each of the sensors only send local monitoring statistics that exceed a chosen threshold. The NULL means no message is sent. The threshold $c_{\text {Local }}$ can be chosen to control the fraction of transmitting sensors when there is no change. It is worth noting that when $c_{\text {Local }}=0$, the "distributed" messaging regime is equivalent to "centralized" messaging regime.



# 3.3 Global monitoring 

In our paper, we assume that there is no communication delay between sensors and the central hub, so the message could be immediately received by the centre at time $t$. Based on the messages received, the centre will make the decision as to whether or not to flag a change.

### 3.3.1 Combining messages

Depending on different messaging regimes, the global MOSUM statistics are constructed as follows:

- Centralized global MOSUM statistic:

$$
T(m, k, h)=\sqrt{\sum_{i=1}^{d} M_{i, t}^{2}}
$$

This is similar to the SUM scheme mentioned in Section 2. By using such a scheme, Formula 3.7 is the idealistic scheme under dense change.

- Distributed global MOSUM statistic:

$$
T(m, k, h)=\sqrt{\sum_{i=1}^{d} M_{i, t}^{2} \mathbf{1}_{T_{i}(m, k, h)>c_{\text {Local }}}
$$

where NULL values in Formula 3.6 are taken to be zeros in the sum. The form of Equation (3.8) is taken from the multivariate MOSUM (Kirch and Kamgaing, 2015; Weber, 2017; Kirch and Weber, 2018).

### 3.3.2 Declaring the change

Similar to the local monitoring procedure, a change is declared as soon as the weighted global MOSUM exceeds a threshold. A closed-end stopping rule can be used when the aim is to





Figure 3: Example of the weighted global MOSUM statistic for the distributed (red dashed line) and centralized (black line) regime. The result is obtained with $T=1000, d=100, m=$ $100, h=50, \delta=0.5$ and the number of affected sensors $p=50$. A value of $c_{\text {Local }}=3.44$ was used in the distributed regime.
monitor changes within a fixed time. This can be formalised as

$$
\tau_{m, \tilde{T}}=\min \left\{1 \leq k \leq\lfloor m \tilde{T}\rfloor: w(k, h) T(m, k, h)>c_{\text {Global }}\right\}
$$

where $\min \{\emptyset\}=\infty$ and the total length of the data $T=m \tilde{T}$. If no change is detected by this stopping rule prior to $\lfloor m \tilde{T}\rfloor$, the monitoring procedure is terminated. The parameter $\tilde{T}>0$ governing the length of the monitoring period is chosen in advance (Horváth et al., 2008; Aue et al., 2012).

Figure 3 shows the weighted global MOSUM statistic for the distributed and centralized messaging regimes on the same dataset. Whenever the weighted global MOSUM of distributed regime hits zero, there is no communication between the edges and the centre at that time. In the next section, we will show the theoretical properties of our proposed method under $H_{0}$ and $H_{A}$.



# 4 Theoretical properties for distributed MOSUM 

This section considers the theoretical properties of the closed-end stopping rule, $\tau_{m}, \tilde{T}$ defined in Equation (3.9) as $m \rightarrow \infty$. Firstly, in Section 4.1 we find the limiting distribution under the null hypothesis for the different procedures. Then, appropriate choices for the thresholds, $c_{\text {Local }}$ and $c_{\text {Global }}$ are given in Section 4.2 using these results. Finally, in Section 4.3 we prove that the detection procedures we have studied are consistent under alternatives.

Three key assumptions are made in order to derive asymptotic results, which are the same in Horváth et al. (2008), Aue et al. (2012), and Weber (2017):

Assumption 1 (Clean historic data). $h \rightarrow \infty$ as $m \rightarrow \infty$ and the location of the changepoint $\tau>m$ for $1 \leq i \leq d$.

This assumption is to guarantee we can get good estimators based on the training dataset, and it can be easily achieved in real applications.

Assumption 2 (Asymptotic regime). $h \rightarrow \infty$ as $m \rightarrow \infty$ and

$$
\lim _{m \rightarrow \infty} \frac{h}{m} \rightarrow \beta \in(0,1]
$$

This assumption quantifies the long run connection between the length of the historical period $m$ and the window size $h:=h(m)$.

Assumption 3 (FCLT on errors).

$$
\lim _{m \rightarrow \infty} \frac{1}{\sqrt{m}} S_{i}(m t) \xrightarrow{\mathcal{D}} \sigma_{i} W_{i}(t)
$$

where $\sigma_{i}>0,\left\{W_{i}(t), 0 \leq t<\infty\right\}$ is a standard Brownian motion when $h \rightarrow \infty$, and $S_{i}(x)=\sum_{t=1}^{\lfloor x\rfloor} \epsilon_{i, t} . \sigma_{i}$ can be estimated by $\hat{\sigma}_{i}$. Furthermore, $\hat{\sigma}_{i}$ satisfying $\hat{\sigma}_{i} \xrightarrow{P} \sigma_{i}$ as $m \rightarrow \infty$. This assumption is a functional central limit theorem on the errors, $\epsilon$, in the model for the data (2.1).



# 4.1 Asymptotics under the null 

In this part, the asymptotic theories of our proposed method will be given, which can help guide the choice of thresholds.

The local monitoring process of our proposed method within each sensor is the same as univariate MOSUM detection process. Thus, Theorem 1 and Corollary 1.1 of the local MOSUM can be directly cited from Horváth et al. (2008), Aue et al. (2012) and Weber (2017). For simplicity, we denote

$$
Z_{i}(t)=\left|W_{i}\left(\frac{1}{\beta}+t\right)-W_{i}\left(\frac{1}{\beta}+t-1\right)-\beta W_{i}\left(\frac{1}{\beta}\right)\right|, \quad 1 \leq i \leq d
$$

where $\left\{W_{i}(t), 0 \leq t<\infty\right\}$ are independent standard Brownian motions.
Theorem 1 (Local MOSUM ). If assumption 1-3, and model 2.2 holds, then under $H_{0}$, let $k=h t$ for any $t>0$

$$
\lim _{m \rightarrow \infty} w(k, h) T_{i}(m, k, h) \xrightarrow{\mathcal{D}} \rho(t) Z_{i}(t) .
$$

Corollary 1.1 (Local MOSUM - asymptotic type-I error). Under $H_{0}$, for any $\tilde{T}>0$ and ith data stream,

$$
\lim _{m \rightarrow \infty} \mathbb{P}\left(\tau_{m, \tilde{T}}^{(i)}<\infty\right)=\mathbb{P}\left(\sup _{0 \leq t \leq \tilde{T} / \beta} \rho(t) Z_{i}(t)>c_{\text {Local }}\right) .
$$

Thus, the false alarm rate for one data stream is asymptotically equal to a pre-specified typeI-error $\in(0,1)$.

Following the results of local MOSUM, similar results for global MOSUM follow readily. These can be used to choose thresholds given the pre-defined Type-I-error. Below we obtain two limiting distributions, for the centralized and distributed regime settings of Section 3 respectively.



Theorem 2 (Global MOSUM ). Let $k=h t$ for any $t>0$, then under $H_{0}$,

$$
\lim _{m \rightarrow \infty} w(k, h) T(m, k, h) \xrightarrow{\mathscr{D}} \rho(t)\left\{\begin{array}{l}
\sqrt{\sum_{i=1}^{d} Z_{i}(t)^{2}} \quad \text { centralized case } \\
\sqrt{\sum_{i=1}^{d} Z_{i}(t)^{2}} \mathbf{1}_{\rho(t) Z_{i}(t)>c_{\text {Local }}} \quad \text { distributed case. }
\end{array}\right.
$$

Proof. See Appendix A.1.

Thus, their limiting distribution will be a function of Gaussian process. Using the Theorem 2 , the following may be obtained:

Corollary 2.1 (Global MOSUM - asymptotic type-I error). Under $H_{0}$, for any $\tilde{T}>0$,

$$
\lim _{m \rightarrow \infty} \mathbb{P}\left(\tau_{m, \tilde{T}}<\infty\right)=\left\{\begin{array}{l}
\mathbb{P}\left(\sup _{0 \leq t \leq \tilde{T} / \beta} \frac{\rho(t)}{\sqrt{\sum_{i=1}^{d} Z_{i}(t)^{2}}}>c_{\text {Global }}\right) \\
\mathbb{P}\left(\sup _{0 \leq t \leq \tilde{T} / \beta} \frac{\rho(t)}{\sqrt{\sum_{i=1}^{d} Z_{i}(t)^{2}}} \mathbf{1}_{\rho(t) Z_{i}(t)}>c_{\text {Global }}\right)
\end{array} \quad\right.
$$

centralized case,
distributed case.
This result can lead us to find the local and global thresholds which can obtain the pre-defined type-I-error.

# 4.2 Obtaining critical values 

Using the results of the previous section, appropriate critical values can be found such that the asymptotic type-I error is controlled for the different procedures. To achieve this the stochastic processes $\left\{Z_{i}(t), 0 \leq t \leq \tilde{T} / \beta, 1 \leq i \leq d\right\}$ need to be approximated on a fine grid. This is done in the same way as Aue et al. (2012), simulating the component standard Brownian motions using ten thousand i.i.d. standard normal random variables. The parameters used were $\beta=1 / 2$ and $\tilde{T}=10$. Tables 1 and 2 give critical values for $\alpha \in\{0.10,0.05,0.01\}$.

| $\alpha$ | $c_{\text {Local }}$ | $c_{\text {Centralized }}$ | Global <br> $(\alpha)$ |
| :--- | :--- | :--- | :--- |
| 0.10 | 0 | 14.1 |  |
| 0.05 | 0 | 14.4 |  |
| 0.01 | 0 | 15.0 |  |

Table 1: Critical values for the centralized procedures, results averaged over five thousand replications.



| $\alpha$ | $c_{\text {Local }}$ | $c_{\text {Distributed }}$ |  |  |  |  |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|  |  | Global <br> ( $\alpha$ ) | 0.10 | 3.15 | 7.48 |  |
| 0.05 | 3.15 | 7.89 | 3.44 | $\mathbf{6 . 7 0}$ | 4.05 | $\mathbf{5 . 2 0}$ |
| 0.01 | 3.15 | 8.74 |  |  | 3.44 |  |
|  |  |  |  |  |  |  |
|  | $\mathbf{4 . 0 5}$ | $\mathbf{6 . 0 2}$ | $\mathbf{5 . 3 \%}$ | $\mathbf{5 . 3 8 \%}$ | 5.12\% |  |

Table 2: Critical values for the distributed procedure with different values for $c_{\text {Local }}$, results averaged over five thousand replications.

Since the critical values obtained above are valid asymptotically (in $m$ ), an important question to consider is how they perform in finite samples. Numerical results of empirical size in the finite sample are shown in Table 3. Thse indicate that the implementation in the finite sample setting can be conservative, as per Aue et al. (2012). However, approximately, the type-I error is controlled at the correct level for both of the global procedures in finite samples.

| Method | $c_{\text {Local }}$ | $c_{\text {Global }}$ | Proportion of false alarms |  |  |
| :--- | :---: | :---: | :---: | :---: | :---: |
|  |  |  | $\mathrm{m}=200$ | $\mathrm{~m}=400$ | $\mathrm{~m}=500$ |
| Distributed | 3.15 | 7.89 | $5.14 \%$ | $5.18 \%$ | $5.5 \%$ |
|  | 3.44 | 7.16 | $5.3 \%$ | $5.38 \%$ | $5.12 \%$ |
|  | 4.05 | 6.02 | $5.5 \%$ | $5.64 \%$ | $5.16 \%$ |
| Centralized | - | 14.4 | $5.92 \%$ | $5.28 \%$ | $5.3 \%$ |

Table 3: Empirical size, results averaged over one thousand replications with $\alpha=0.05, \tilde{T}=$ 10 , and $\beta=1 / 2$.

# 4.2.1 The choice of local threshold $\boldsymbol{c}_{\text {Local }}$ 

The values for $c_{\text {Local }}$ used in Table 2 are somewhat arbitrary. The main influence of the value of local threshold is that it controls the proportion of messages that the system can pass (on average) per iteration. For $d$ streams, the number of sensors passing message at each time step is:

Corollary 2.2 (Transmission cost). For any $t>0$ and $\mathrm{k}=\mathrm{ht}$, the expected fraction of trans-



mitting sensors at each time step is

$$
\bar{\Delta} t=d P\left(\rho(t)|Z|>c_{\text {Local }}\right) .
$$

where $Z$ is the standard normal distribution.
Therefore, the local threshold can be chosen based on the restriction of the transmission cost. Combined with pre-defined type-I-error, the global threshold will be given based on Theorem 2 .

# 4.3 Asymptotics under the alternative 

Under the alternative it is assumed that there is a changepoint at monitoring time $k^{*}$ and a subset $S$ of the data streams have an altered mean

$$
H_{A}: \tau=m+k^{*} \quad \left\{\exists S \subset\{1,2, \ldots, d\}: \delta_{i} \neq 0 \quad \text { for } i \in S\right\}
$$

Deriving sharp asymptotic results on the detection delay of the proposed method is challenging, and thus we focus only on giving consistency results. A procedure is consistent if it stops in finite time with probability approaching one as $m \rightarrow \infty$. In other words, the test statistic should tend to infinity as $m \rightarrow \infty$. In the asymptotic regime of interest, we additionally assume that the changepoint $k^{*}$ grows at the same order as $h$, that is $\frac{k^{*}}{h} \rightarrow \gamma \geq 0$, and the size of change $\delta_{i, t}$ satisfies $\sqrt{h}\left|\delta_{i, t}\right| \rightarrow \infty$ as $m \rightarrow \infty$ and $h \rightarrow \infty$. These assumptions are the same in Aue et al. (2012).

Theorem 3 (Global MOSUM: Consistency). If the assumption above holds, under $H_{A}$,
(i) the changepoint $k^{*} \leq\left\lfloor h^{\nu}\right\rfloor$ for some $0<\nu<\tilde{T} \frac{m}{h}$,
(ii) there exists a constant $c>0$ such that $\rho(x+1) \geq c$ for all $x \in\left(\nu, \tilde{T} \frac{m}{h}-1\right)$.

Then, as $m \rightarrow \infty$ and $h \rightarrow \infty$

$$
\max _{1 \leq k \leq\lfloor m \tilde{T}\rfloor} w(k, h) \underline{T}(m, k, h) \xrightarrow{P} \infty .
$$



Proof. See Appendix A.2.
Thus, our proposed method is consistent.

# 5 Simulations 

In this section, we will present the numeric performance of our algorithm. Since the SUM procedure that is optimal when the change is dense, we will evaluate the performance in the dense case, specifically when the affected data streams $p=d$. Firstly, the different practical choices of thresholds at fixed type-I-error will be investigated. Here the performance of our proposed method was also compared against the idealistic setting. Finally, the effect of parameters and the violation of the independence assumption are investigated.

The set-up of the simulations is as follows. For simplicity, the data generating process under the null is that $X_{i, t} \sim N(0,1)$ for $1 \leq i \leq d$ and $1 \leq t \leq T$. To compare fairly, the type-I-error of all procedures is controlled to be 0.05 under the null.

The family of alternatives considered is that

$$
X_{i, t} \sim N(0,1) \quad \text { for } \quad 1 \leq i \leq d, 1 \leq t<\tau
$$

and

$$
X_{i, t} \sim N\left(\delta_{i}, 1\right) \quad \text { for } \quad 1 \leq i \leq d, \tau \leq t \leq T
$$

We assume the change will affect all the sensors instantaneously. But the size of the change is unknown. We consider two scenarios of mean shift: 1) Same size: $\delta_{i}=\delta=$ some constant values for $1 \leq i \leq d ; 2)$ Random size: $\delta_{i}=\eta N(0,1)$, where $\eta$ is the scale factor controlling the magnitude of size. The average detection delay (ADD) $\bar{D}$ and average communication $\operatorname{cost} \bar{\Delta}$ are then measured:

$$
\begin{aligned}
\bar{D} & =\mathbb{E}(\hat{\tau}-\tau \mid \hat{\tau}>\tau) \\
\bar{\Delta} & =\frac{\sum_{t=1}^{\hat{\tau}} \sum_{i=1}^{d} l\left(w(k, h)^{T_{i}}(m, k, h)>c_{\text {Local }}\right)}{\hat{\tau}-m+1}
\end{aligned}
$$



# 5.1 The numerical dependency on local thresholds 

Our proposed method requires specifying two thresholds. Usually, $c_{\text {Global }}$ can be given based on the Theorem 2.1 once $\alpha$ and $c_{\text {Local }}$ are confirmed. Therefore, it is crucial to pick an appropriate local threshold. This section gives numeric results with different values of local thresholds, which may provide some guidance in choosing the local threshold.

Figure 4 gives the average detection delay and transmission cost for different values of local thresholds. There is a trade-off between communication savings and detection performance when choosing the local threshold. Larger local thresholds can reduce the transmission cost but will also lead to longer delays, especially when the change is small. However, with the increase in the mean shift, the detecting power of larger thresholds will close to that of small thresholds.

A centralized framework can be seen as an idealistic setting, which is equivalent to distributed setting when $c_{\text {local }}=0$. Compared with the idealistic setting, the distributed MOSUM can achieve similar performance when the size of the change is not small but also reduces massive transmission costs. But we will lose power in detecting small changes. We show the result below that distributed MOSUM can approximate the performance of idealistic setting overall by increasing the window size.

### 5.2 The numerical dependency on parameters

One advantage of using MOSUM statistics is that we do not need to specify the post-change mean. Instead, our proposed method requires specifying the window size $h$ and the training size $m$. In this section, we will investigate the impact of bandwidth and training size.

### 5.2.1 The impact of bandwidth

As shown in Figure 5a, increasing the window size can increase the power of detecting small changes while leading to a slight delay in detecting large changes. Although increasing window size will increase the storage cost, it will not significantly increase the transmission cost as shown in Figure 5b. This drive us to think about whether we can improve the ability





Figure 4: The average number of messages transmitted to the centre (top) and average detection delay across varying mean shifts (bottom). Results are obtained when $m=200$, $h=100, T=10000, \tau=5000, \alpha=0.05$. Each line corresponds to a different local threshold, which is labelled on the top right. The colour changes from orange to blue as the local thresholds increase from 0 to 5.2 . When the local threshold is 5.2 , the global threshold will be 0 . So all possible combinations of thresholds are covered.



of distributed MOSUM with a large threshold to detect small changes by increasing the window size. Ideally, we would expect distributed MOSUM with increased window size can achieve similar performance as the idealistic setting.


(a) $\bar{D}$ versus $\delta$ for $h=20,50,100$.


(b) $\bar{\Delta}$ versus $h$.

Figure 5: The influence of window size. Results are obtained over 1000 replications and take $m=200, d=100, T=10000, \tau=5000, \alpha=0.05, c_{\text {Local }}=3.44$

# Recovering detectability 

For simplicity, we denote that the default window size for centralized MOSUM is $h_{0}$, and $h_{*}$ is the smallest window size that would allow distributed MOSUM to have similar performance as the idealistic setting. It is difficult to develop a neat theoretical formula between $h_{*}$ and $h_{0}$. But we can approximately find $h_{*}$ under alternatives by simulation. Our idea can be summarized as follows, and Figure 6 is the graphic explanation:

- The behaviour of $\bar{D}$ will decrease dramatically when the mean shift is within a certain range (gray area). Therefore, we can find the median or mean $\delta$ of this certain range, denoted by $\delta_{0}$. Also, the corresponding ADD $\bar{D}_{0}$ can be calculated.
- Fix $\delta_{0}$, calculate $\bar{D}_{\mathrm{c} \text { Local }}(h)$ iteratively for distributed MOSUM, where $h \in\left[h_{0}, m\right]$.
- The optimal window size $h_{*}=\arg \min \left\{\bar{D}_{\mathrm{c} \text { Local }}(h)-\bar{D}_{0}\left(h_{0}\right)\right\}$. See blue arrow $\left(h_{*}\right)$ is shorter than yellow arrow $(h)$.





Figure 7: An simple example showing that distributed MOSUM could approximate the detection power of centralized MOSUM by inflating window size. Results are obtained over 500 replications and take $m=200, d=100, T=1000, \tau=600, \alpha=0.05$, and $c_{\text {Local }} \in[0,4.4]$. When $c_{\text {Local }}=4.4, c_{\text {Global }}=0$. So all possible local thresholds are covered. For centralized setting, window size $h_{0}=50$.



Figure 6: An graphic explanation of our proposed idea. Black line is the ADD for centralized MOSUM with window size $h$. Yellow dashed line is the ADD for distributed MOSUM with window size $h$; while blue dashed is the ADD for distributed MOSUM with window size $h^{*}$.

Figure 7 displays the simulation results that, for distributed regime, we can recover the same detectability of the centralized statistic by inflating $h$.



|  | $m=80$ | $m=100$ | $m=500$ | $m=1000$ |
| :--- | ---: | ---: | ---: | ---: |
| cGlobal | 9.039 | 8.159 | 6.014 | 5.708 |
| Empirical size | 0.007 | 0.007 | 0.009 | 0.006 |
| MSE for mean | 0.0125 | 0.01 | 0.002 | 0.001 |
| MSE for sd | 0.006 | 0.005 | 0.0001 | 0.0001 |

Table 4: Empirical size, and MSE for estimated mean and standard deviation, results averaged over one thousand replications with $c_{\text {Local }}=3.44, h=50, T=6000$ and $\alpha=0.05$.

# 5.2.2 The impact of the training dataset 

Fix the bandwidth $h$, the impact of the size of the training dataset can be investigated. Table 4 gives the thresholds, empirical size, and mean square errors (MSE) of estimated baseline parameters in our simulation. As we expected, the larger the training size is, the more accurate estimators are. Figure 8 indicates that overall the detection powers of four different sizes of training datasets are similar. A larger training size could slightly increase the detection power when detecting small changes, which is attributed to more accurate estimators. Thus, in the real application, it is beneficial to choose a large-size training dataset because it is not expensive that can be done offline.



Figure 8: $\bar{D}$ versus $\delta$ when varing the size of training dataset. Result averaged over 500 replications with $\alpha=0.05, c_{\text {Local }}=3.44, T=6000, \tau=3000$ and $h=50$. The corresponding global thresholds are shown in Table 4.



# 5.3 The violation of the independence assumption 

Before, we assume that there is temporal independence among observations. However, it may not always hold in the real application. This section will investigate the performance when this assumption is violated. Here we measure our algorithm under $A R(1)$ noise process, that is

$$
X_{i, t}=\delta i, t 1_{\{t>\tau\}}+\epsilon_{i, t}
$$

where $\epsilon_{i, t}=\phi \epsilon_{i, t-1}+v_{t}$ with $v_{t} \sim N(0,1) .|\phi|<1$ is used to measure the strength of the auto-correlation.

Auto-correlation will inflate the variance of data. There are two possible ways to handle this problem. The first one is to estimate the long-run variance as shown in Section 3.2. And one can also inflate the thresholds. We measure the false positives, average detection delay and the number of transmitted messages with fixed type-I-error of these two solutions under different scenarios. For better comparison, we also show the result of MOSUM without any adjustment. This will give us hints that to what extent our method fails to detect the change if we ignore the auto-correlation.

As Table 5 shows, our proposed method without adjustment can lose the ability to detect changes when introducing auto-correlation, that it fails to detect the change and always alarms. The performances of MOSUM with inflating thresholds are generally better than MOSUM with LRV since the former can detect the change in most scenarios. However, for those scenarios that the MOSUM with LRV can detect (usually $\delta$ is not small and $\phi$ is not large), it always has the lowest transmission cost and reasonable detection power. For example, when $p=100, \delta=1$, and $\phi=0.25$, both solutions have similar false positive rates and average detection delay, while MOSUM with LRV has lower transmission cost. It is surprising that estimating LRV has the lowest false positive rates and average detection delay when $\phi=0$ and $p=100 / 50$. This may be because it underestimates the variance.







# 6 Conclusion 

Within this paper, we proposed an online communication-efficient distributed changepoint detection method, and it can achieve similar performance as an idealistic setting but save many transmission costs. Numerically, we show that the local threshold and window size have an impact on the performance of our algorithm, and there is a trade-off in choosing a local threshold and window size. In application, we recommend choosing a large local threshold in general cases. But when the change is extremely small, the choice of the local threshold depends on the communication and storage budgets. If the communication budget is much more limited, choosing a large threshold with a large window size is sensible. If the storage cost is much more expensive, choosing a small threshold with small window size will approximately achieve the idealistic performance.

The violation of independent assumptions will negatively affect the power of our proposed method. We tried to solve this problem by inflating thresholds or estimating the long-run variance. Both ways can, to some extent, improve our algorithm when the auto-correlation problem is not severe. However, both approaches fail to detect changes in highly auto- correlated data. Therefore, one of the future research directions is how to detect change within highly auto-correlated data in real-time.

## Acknowledgements

Yang gratefully acknowledges the financial support of the EPSRC via the STOR-i Centre for Doctoral Training (EP/S022252/1). This research was also supported by EPSRC grant EP/R004935/1 (Eckley) together with financial support from BT Research (Eckley, Fearnhead, Yang). The authors are also grateful to Lawrence Bardwell who played a key role in inspiring this work, and Dave Yearling (BT) for several helpful conversations that helped shape this research.



# References 

Adams, R. P. and MacKay, D. J. (2007). Bayesian online changepoint detection. arXiv preprint arXiv:0710.3742.

Alrashdi, I., Alqazzaz, A., Aloufi, E., Alharthi, R., Zohdy, M., and Ming, H. (2019). Ad-iot: Anomaly detection of iot cyberattacks in smart city using machine learning. In 2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC), pages $0305-0310$.

Appadwedula, S., Veeravalli, V. V., and Jones, D. L. (2005). Energy-efficient detection in sensor networks. IEEE Journal on Selected Areas in Communications, 23(4):693-702.

Aue, A., Horváth, L., Kühn, M., and Steinebach, J. (2012). On the reaction time of moving sum detectors. Journal of Statistical Planning and Inference, 142(8):2271 - 2288.

Banerjee, T. and Veeravalli, V. V. (2015). Data-efficient quickest change detection in sensor networks. IEEE Transactions on Signal Processing, 63(14):3727-3735.

Chan, H. P. (2017). Optimal sequential detection in multi-stream data. The Annals of Statistics, 45(6):2736-2763.

Chen, Y., Wang, T., and Samworth, R. J. (2022). High-dimensional, multiscale online changepoint detection. Journal of the Royal Statistical Society Series B: Statistical Methodology, 84(1):234-266.

Cho, H. and Fryzlewicz, P. (2014). Multiple-Change-Point Detection for High Dimensional Time Series via Sparsified Binary Segmentation. Journal of the Royal Statistical Society Series B: Statistical Methodology, 77:475-507.

Enikeeva, F. and Harchaoui, Z. (2019). High-dimensional change-point detection under sparse alternatives. The Annals of Statistics, 47(4):2051-2079.

Fisch, A. T. M., Eckley, I. A., and Fearnhead, P. (2022). A linear time method for the detection of collective and point anomalies. Statistical Analysis and Data Mining: The ASA Data Science Journal, 15(4):494-508.



Gallant, A. R. (2009). Nonlinear statistical models. John Wiley \& Sons.
Gösmann, J., Stoehr, C., Heiny, J., and Dette, H. (2022). Sequential change point detection in high dimensional time series. Electronic Journal of Statistics, 16(1):3608-3671.

Horváth, L. and Hušková, M. (2012). Change-point detection in panel data. Journal of Time Series Analysis, 33(4):631-648.

Horváth, L., Kühn, M., and Steinebach, J. (2008). On the performance of the fluctuation test for structural change. Sequential Analysis, 27(2):126-140.

Kengne, W. and Ngongo, I. S. (2022). Inference for nonstationary time series of counts with application to change-point problems. Annals of the Institute of Statistical Mathematics, $74(4): 801-835$.

Kiefer, N. M. and Vogelsang, T. J. (2002a). Heteroskedasticity-autocorrelation robust standard errors using the Bartlett kernel without truncation. Econometrica, 70(5):2093-2095.

Kiefer, N. M. and Vogelsang, T. J. (2002b). Heteroskedasticity-autocorrelation robust testing using bandwidth equal to sample size. Econometric Theory, 18(6):1350-1366.

Kirch, C. and Kamgaing, J. T. (2015). On the use of estimating functions in monitoring time series for change points. Journal of Statistical Planning and Inference, 161:25-49.

Kirch, C. and Weber, S. (2018). Modified sequential change point procedures based on estimating functions. Electronic Journal of Statistics, 12(1):1579-1613.

Kovács, S., Bühlmann, P., Li, H., and Munk, A. (2023). Seeded binary segmentation: a general methodology for fast and optimal changepoint detection. Biometrika, 110(1):249256.

Leisch, F., Hornik, K., and Kuan, C.-M. (2000). Monitoring structural changes with the generalized fluctuation test. Econometric Theory, 16(6):835-854.

Liu, K., Zhang, R., and Mei, Y. (2019). Scalable SUM-shrinkage schemes for distributed monitoring large-scale data streams. Statistica Sinica, 29:1-22.



Mei, Y. (2005). Information bounds and quickest change detection in decentralized decision systems. IEEE Transactions on Information Theory, 51(7):2669-2681.

Mei, Y. (2010). Efficient scalable schemes for monitoring a large number of data streams. Biometrika, 97(2):419-433.

Mei, Y. (2011). Quickest detection in censoring sensor networks. In 2011 IEEE International Symposium on Information Theory Proceedings, pages 2148-2152.

Newey, W. K. and West, K. D. (1986). A simple, positive semi-definite, heteroskedasticity and autocorrelationconsistent covariance matrix. National Bureau of Economic Research Cambridge, Mass., USA.

Pinto, G. and Castor, F. (2017). Energy efficiency: A new concern for application software developers. Communications of the ACM, 60(12):68-75.

Rago, C., Willett, P., and Bar-Shalom, Y. (1996). Censoring sensors: a low-communicationrate scheme for distributed detection. IEEE Transactions on Aerospace and Electronic Systems, 32(2):554-568.

Romano, G., Eckley, I. A., Fearnhead, P., and Rigaill, G. (2023). Fast online changepoint detection via functional pruning cusum statistics. Journal of Machine Learning Research, 24:1-36.

Tartakovsky, A., Nikiforov, I., and Basseville, M. (2014). Sequential analysis: Hypothesis testing and changepoint detection. CRC press.

Tartakovsky, A. G. and Kim, H. (2006). Performance of certain decentralized distributed change detection procedures. In 2006 9th International Conference on Information Fusion, pages 1-8. IEEE.

Tartakovsky, A. G. and Veeravalli, V. V. (2002). An efficient sequential procedure for detecting changes in multichannel and distributed systems. In Proceedings of the Fifth International Conference on Information Fusion. FUSION 2002. (IEEE Cat.No.02EX5997), volume 1, pages 41-48 vol. 1.



Truong, C., Oudre, L., and Vayatis, N. (2020). Selective review of offline change point detection methods. Signal Processing, 167:107299.

Tveten, M., Eckley, I. A., and Fearnhead, P. (2022). Scalable change-point and anomaly detection in cross-correlated data with an application to condition monitoring. The Annals of Applied Statistics, 16(2):721-743.

Varghese, B., Wang, N., Barbhuiya, S., Kilpatrick, P., and Nikolopoulos, D. S. (2016). Challenges and opportunities in edge computing. In 2016 IEEE International Conference on Smart Cloud (SmartCloud), pages 20-26.

Veeravalli, V. V. (2001). Decentralized quickest change detection. IEEE Transactions on Information Theory, 47(4):1657-1665.

Wang, T. and Samworth, R. J. (2018). High dimensional change point estimation via sparse projection. Journal of the Royal Statistical Society Series B: Statistical Methodology, 80(1):57-83.

Ward, K., Dilillo, G., Eckley, I., and Fearnhead, P. (2024). Poisson-FOCuS: An efficient online method for detecting count bursts with application to gamma ray burst detection. Journal of the American Statistical Association, page (to appear).

Weber, S. M. (2017). Change-Point Procedures for Multivariate Dependent Data. PhD thesis, Karlsruher Institut für Technologie (KIT).

White, H. and Domowitz, I. (1984). Nonlinear regression with dependent observations. Econometrica: Journal of the Econometric Society, pages 143-161.
Wu, H., Hu, J., Sun, J., and Sun, D. (2019). Edge computing in an IoT base station system: Reprogramming and real-time tasks. Complexity, 2019:4027638:1-4027638:10.

Xie, Y. and Siegmund, D. (2013). Sequential multi-sensor change-point detection. The Annals of Statistics, 41(2):670-692.

Yau, C. Y., Sze Him Isaac, L., and Ng, W. L. (2017). Sequential change-point detection in time series models based on pairwise likelihood. Statistica Sinica, 27.



# A Proofs 

## A. 1 Proof of Theorem 2

Squaring and expanding the weighted global MOSUM statistic in Equation (3.8) for the two different cases gives

$$
\left(\mathbf{w}(\mathrm{k}, \mathrm{h})^{\top}(\mathrm{m}, \mathrm{k}, \mathrm{h})\right)^{2}= \begin{cases}\sum_{i=1}^{d}\left(\mathbf{w}(\mathrm{k}, \mathrm{h})^{\top i}(\mathrm{~m}, \mathrm{k}, \mathrm{h})\right)^{2} & \text { dense case } \\ \sum_{i=1}^{d}\left(\mathbf{w}(\mathrm{k}, \mathrm{h})^{\top i}(\mathrm{~m}, \mathrm{k}, \mathrm{h}) \mathbb{1}_{\left.\left\{\mathbf{w}(\mathrm{k}, \mathrm{h})^{\top i}(\mathrm{~m}, \mathrm{k}, \mathrm{h})>\mathrm{c}_{\text {Local }}\right\}\right)^{2}} & \text { sparse case }\end{cases}
$$

From Theorem 1 with $\mathrm{k}=\mathrm{h}_{\mathrm{t}}$, the weighted local MOSUM and its hard-thresholded counterpart have limit

$$
\begin{aligned}
\lim _{\mathrm{m} \rightarrow \infty}\left(\mathbf{w}(\mathrm{k}, \mathrm{h})^{\top i}(\mathrm{~m}, \mathrm{k}, \mathrm{h})\right)^{2} & \xrightarrow{\mathcal{D}} \rho(\mathrm{t})^{2} \mathrm{Z}_{i}(\mathrm{t})^{2} \\
\lim _{\mathrm{m} \rightarrow \infty}\left(\mathbf{w}(\mathrm{k}, \mathrm{h})_{\top i}(\mathrm{~m}, \mathrm{k}, \mathrm{h}) \mathbb{1}_{\left\{\mathbf{w}(\mathrm{k}, \mathrm{h})^{\top i}(\mathrm{~m}, \mathrm{k}, \mathrm{h})>\mathrm{c}_{\text {Local }}\right\}}\right)^{2} & \xrightarrow{\mathcal{D}} \rho(\mathrm{t})^{2} \mathrm{Z}_{i}(\mathrm{t})^{2} \mathbb{1}_{\left\{\rho(\mathrm{t}) \mathrm{Z}_{i}(\mathrm{t})>\mathrm{c}_{\text {Local }}\right\}}
\end{aligned}
$$

where $\mathrm{Z}_{\mathrm{i}}(\mathrm{t})$ is defined in Equation 4.1. Taking the limit of (A.1) as $\mathrm{m} \rightarrow \infty$ gives the result.

## A. 2 Proof of Theorem 3

It is enough to show that

$$
\left(\mathbf{w}(\tilde{k}, h)^{\top}(m, \tilde{k}, h)\right)^{2} \xrightarrow{\mathcal{P}} \infty
$$

for a time $\tilde{\mathrm{k}}$ later than the change point $\mathrm{k}^{*}$ but before the end of the monitoring time $\lfloor m \tilde{T}\rfloor$. Since $k^{*} \leq\lfloor\mathrm{h} \nu\rfloor$, we can choose $\tilde{\mathrm{k}}=\left\lfloor\mathrm{x}_{0} \mathrm{~h}\right\rfloor+\mathrm{h}$ where $\nu<\mathrm{x}_{0}<\frac{\tilde{\mathrm{Tm}}}{\mathrm{h}}-1$ so that $\mathrm{k}^{*} \leq \tilde{\mathrm{k}}-\mathrm{h}$.



If data stream $i$ is affected by the change, so that $i \in \mathcal{S}$ and $\delta_{i} \neq 0$ then

$$
\begin{aligned}
\frac{1}{h} T_{i}(m, \tilde{k}, h) & =\frac{1}{h \hat{\sigma}_{i}}\left|\sum_{t=m+\left\lfloor x_{0} h\right\rfloor+1}^{m+\left\lfloor x_{0} h\right\rfloor+h}\left(X_{i, t}-\hat{\mu}_{i}\right)\right| \\
& =\frac{1}{h \hat{\sigma}_{i}}\left|\sum_{t=m+\left\lfloor x_{0} h\right\rfloor+1}^{m+\left\lfloor x_{0} h\right\rfloor+h}\left(\mu_{i}+\delta_{i}+\epsilon_{i, t}-\hat{\mu}_{i}\right)\right| \\
& =\frac{1}{h \hat{\sigma}_{i}}\left|h\left(\mu_{i}-\hat{\mu}_{i}\right)+h \delta_{i}+\sum_{t=m+\left\lfloor x_{0} h\right\rfloor+1}^{m+\left\lfloor x_{0} h\right\rfloor+h} \epsilon_{i, t}\right| \\
& =\frac{1}{\hat{\sigma}_{i}}\left|\mu_{i}-\hat{\mu}_{i}+\delta_{i}+\frac{1}{h} \sum_{t=m+\left\lfloor x_{0} h\right\rfloor+1}^{m+\left\lfloor x_{0} h\right\rfloor+h} \epsilon_{i, t}\right| \\
& =\frac{1}{\hat{\sigma}_{i}}\left|\delta_{i}+o_{P}(1)\right|
\end{aligned}
$$

On the other hand if $i \notin \mathcal{S}$,

$$
\frac{1}{h} T_{i}(m, \tilde{k}, h)=\frac{1}{\hat{\sigma}_{i}}\left|o_{P}(1)\right|
$$

For the global dense procedure

$$
\begin{aligned}
\left(w(\tilde{k}, h) T(m, \tilde{k}, h)\right)^{2} & =w(\tilde{k}, h)^{2} \sum_{i=1}^{d} T_{i}(m, \tilde{k}, h)^{2} \\
& =\left(\frac{1}{\sqrt{h}} \rho\left(\frac{\tilde{k}}{h}\right)\right)^{2} \times h^{2} \times \sum_{i=1}^{d}\left(\frac{1}{h} T_{i}(m, \tilde{k}, h)\right)^{2} \\
& =h \rho\left(\frac{\tilde{k}}{h}\right)^{2} \sum_{i=1}^{d}\left(\frac{1}{h} T_{i}(m, \tilde{k}, h)\right)^{2} \\
& =h \rho\left(x_{0}+1+o(1)\right)^{2} \sum_{i \in \mathcal{S}}\left(\frac{\delta_{i}}{\hat{\sigma}_{i}}\right)^{2}+o_{P}(1) \\
& \xrightarrow{P} \infty
\end{aligned}
$$

as $m, h \rightarrow \infty$.
For the global sparse procedure the local MOSUM's $w(\tilde{k}, h) T(m, \tilde{k}, h)$ are hard thresholded. Since these diverge to infinity individually then the same argument used for the dense procedure applies.



