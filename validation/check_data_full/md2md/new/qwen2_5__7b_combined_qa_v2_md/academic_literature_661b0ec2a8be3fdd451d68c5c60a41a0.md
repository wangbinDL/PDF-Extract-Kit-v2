# Minimum stationary values of sparse random directed graphs 

Xing Shi Cai* and Guillem Perarnau ${ }^{* *}$<br>*Mathematics Department, Uppsala University, Sweden. Email: xingshi.cai@tutanota.com.<br>**Departament de Matemàtiques (MAT), Universitat Politècnica de Catalunya (UPC), Barcelona, Spain.<br>Email: guillem.perarnau@upc.edu.

November 2, 2021


#### Abstract

We consider the stationary distribution of the simple random walk on the directed configuration model with bounded degrees. Provided that the minimum out-degree is at least 2 , with high probability (whp) there is a unique stationary distribution (unicity regime). We show that the minimum positive stationary value is whp $n^{-(1+C+o(1))}$ for some constant $C \geq 0$ determined by the degree distribution, answering a question raised by Bordenave, Caputo and Salez [5]. In particular, $C$ is the competing combination of two factors: (1) the contribution of atypically "thin" in-neighbourhoods, controlled by subcritical branching processes; and (2) the contribution of atypically "light" trajectories, controlled by large deviation rate functions. Additionally, we give estimates for the expected lower tail of the empirical stationary distribution. As a byproduct of our proof, we obtain that the maximal hitting and the cover time are both $n^{1+C+o(1)}$ whp. Our results are in sharp contrast to those of Caputo and Quattropani [11] who showed that under the additional condition of minimum in-degree at least 2 (ergodicity regime), stationary values only have logarithmic fluctuations around $n^{-1}$.


## 1 Introduction

### 1.1 The directed configuration model

The directed configuration model was introduced by Cooper and Frieze in [14]. Let $[n]:=\{1, \ldots, n\}$ be a set of $n$ vertices. Let $\vec{d}_{n}=\left(\left(d_{1}^{-}, d_{1}^{+}\right), \ldots,\left(d_{n}^{-}, d_{n}^{+}\right)\right)$be a bi-degree sequence with $m:=$ $\sum_{i \in[n]} d_{i}^{+}=\sum_{i \in[n]} d_{i}^{-}$. Let $\delta^{ \pm}$and $\Delta^{ \pm}$be the minimum and maximum in/out-degree, respectively. The directed configuration model, which we denote by $\vec{G}_{n}=\vec{G}_{n}\left(\vec{d}_{n}\right)$, is the random directed multigraph on $[n]$ generated by giving $d_{i}^{-}$heads (in-half-edges) and $d_{i}^{+}$tails (out-half-edges) to vertex $i$, and then pairing the heads and the tails uniformly at random.

The directed configuration model is of practical importance as many complex real-world networks are directed. For instance, it has been used to study neural networks [2], Google's PageRank algorithm [13], and social networks [19].

The original paper by Cooper and Frieze [14] studies the birth of a linear size strongly connected component (scc) in $\vec{G}_{n}$. Their result was recently improved by Graf [17] and by the two authors [8].

Lately, there has been some progress on the distances in the directed configuration model for bi-degree sequences with finite covariances. Typical distances in $\vec{G}_{n}$ were studied by van der



Hoorn and Olvera-Cravioto [26]. Caputo and Quattropani [11] showed that the diameter of $\vec{G}_{n}$ is asymptotically equal to the typical distance, provided that $\delta_{ \pm} \geq 2$ and $\Delta_{ \pm}=O(1)$. In our previous work [9], we showed that the diameter has different behaviour if no constraints on the minimum degree are imposed.

One motivation to the study of distances in $\vec{G}_{n}$ is its close connection to certain properties of random walks, in particular, to their stationary distribution, denoted by $\pi$. While $\pi$ is trivially determined by the degree sequence in undirected graphs, in the directed case $\pi$ is a complicated random measure that depends on the geometry of the random digraph. Cooper and Frieze [15] initiated the study of $\pi$ in random digraphs, determining it on the strong connectivity regime of the directed Erdős-Rényi random graph. They also established a relation between the minimum stationary value and stopping times such as the hitting and the cover time. Extremal stationary values for the $r$-out random digraph were studied by Addario-Berry, Balle, and the second author [1].

Regarding the directed configuration model, Bordenave, Caputo and Salez [5, 6] studied the mixing time of a random walk on $\vec{G}_{n}$ and showed that it exhibits cutoff. Additionally, they proved that for a vertex $i \in[n], \pi(i)$ is essentially determined by the local in-neighbourhood of $i$ and wellapproximated by a deterministic law (see Remark 1.8). These results provide a precise description of typical stationary probabilities but fall short to capture the exceptional values of $\pi$.

In [5], the authors raised the question of studying the extremal values of the stationary distribution in $\vec{G}_{n}$. Let $\pi_{\text {min }}$ and $\pi_{\max }$ be the smallest and largest positive values of $\pi$, respectively. From now on and throughout this paper, we will assume that all degrees are bounded; i.e., $\Delta_{ \pm}=O$ (1). In this context, the condition $\delta_{+} \geq 2$ is essentially necessary to avoid (possibly many) trivial stationary measures (see Remark 1.4). Under the additional condition $\delta_{-} \geq 2$, Caputo and Quattropani [11] showed that the random walk is ergodic with high probability (whp), so we call it the ergodicity regime, and that there exists $C \geq 1$ such that, whp

$$
\begin{aligned}
& C^{-1} \frac{\log ^{1-\gamma_{0}} n}{n} \leq \pi_{\min } \leq C \frac{\log ^{1-\gamma_{1}} n}{n} \\
& C^{-1} \frac{\log ^{1-\kappa_{1}} n}{n} \leq \pi_{\max } \leq C \frac{\log ^{1-\kappa_{0}} n}{n}
\end{aligned}
$$

where $\gamma_{0} \geq \gamma_{1} \geq 1$ are defined in terms of $\delta_{-}$and $\Delta_{+}$, and $\kappa_{0} \leq \kappa_{1} \leq 1$ are defined in terms of $\delta_{+}$ and $\Delta_{-}$. While these constants are different in general, $\gamma_{0}=\gamma_{1}$ and $\kappa_{0}=\kappa_{1}$ if there are a linearly many vertices with degrees $\left(\delta_{-}, \Delta_{+}\right)$and $\left(\delta_{+}, \Delta_{-}\right)$, respectively. Finally, using the bound on $\pi_{\text {min }}$, the authors showed that in the ergodicity regime the cover time satisfies whp

$$
C^{-1} n \log ^{\gamma_{1}} n \leq \tau_{\mathrm{cov}} \leq C n \log ^{\gamma_{0}} n
$$

The main purpose of this paper is to study the extremal values of the stationary distribution outside the ergodicity regime. If $\delta_{+} \geq 2$ but no condition on the minimum in-degree is imposed, the random walk might fail to be ergodic, but the stationary measure is whp unique; we call it the unicity regime. While (1.1) and (1.2) indicate that the extremal values exhibit logarithmic fluctuations in the ergodicity regime, our main result shows in that unicity regime $\pi_{\text {min }}$ has polynomial deviations with respect to the typical stationary values. As an easy consequence of our proof, we determine the maximal hitting and the cover time up to subpolynomial multiplicative terms.



# 1.2 Notations and results 

Before stating our results, we need to define some parameters of the bi-degree sequence $\vec{d}_{n}$. Although $n$ does not appear in many of the notations, the reader should keep in mind that all the parameters defined here depend on $n$.

Let $n_{k, \ell}:=\left|\left\{i:\left(d_{i}^{-}, d_{i}^{+}\right)=(k, \ell)\right\}\right|$ be the number of $(k, \ell)$ in $\vec{d}_{n}$. Let $\Delta^{ \pm}=\max _{i \in[n]}\left\{d_{i}^{ \pm}\right\}, \delta^{ \pm}=$ $\min _{i \in[n]}\left\{d_{i}^{ \pm}\right\}$. Let $D=\left(D^{-}, D^{+}\right)$be the degrees (number of heads and tails) of a uniform random vertex. In other words, $\mathbb{P}\{D=(k, \ell)\}=n_{k, \ell} / n$.

The bivariate generating function of $D$ is defined by

$$
f(z, w):=\sum_{k, \ell \geq 0} \mathbb{P}\{D=(k, \ell)\} z^{k} w^{\ell}
$$

Let $\lambda:=m / n$. Consider a branching process with offspring distribution that has generating function $\frac{1}{\lambda} \frac{\partial f}{\partial w}(z, 1)$. Let $s^{-}$be its survival probability and let $\nu:=\frac{1}{\lambda} \frac{\partial f}{\partial w}(1,1)$ be its expected number of offspring, which we call the expansion rate. Define the subcritical in-expansion rate by

$$
\hat{\nu}^{-}:=\frac{1}{\lambda} \frac{\partial^{2} f}{\partial z \partial w}\left(1-s^{-}, 1\right) \in[0,1)
$$

Define $D^{\text {out }}=\left(D_{\text {out }}^{-}, D_{\text {out }}^{+}\right)$, the out-size-biased distribution of $D$, by

$$
\mathbb{P}\left\{D^{\text {out }}=(k, \ell)\right\}:=\frac{\ell}{\lambda} \mathbb{P}\{D=(k, \ell)\}, \quad \text { for } k \geq 0, \ell \geq 0
$$

In words, $D^{\text {out }}$ is the degree distribution of a vertex incident to a uniform random tail. We define $D^{\text {in }}$ analogously.

Let $\tilde{D}^{\text {out }}=\left(\tilde{D}_{\text {out }}^{-}, \tilde{D}_{\text {out }}^{+}\right)$be the random vector with distribution

$$
\mathbb{P}\left\{\tilde{D}^{\text {out }}=(k, \ell)\right\}:=\frac{k\left(1-s^{-}\right)^{k-1}}{\hat{\nu}^{-}} \mathbb{P}\left\{D^{\text {out }}=(k, \ell)\right\}, \quad \text { for } k \geq 0, \ell \geq 0
$$

Define the subcritical in-entropy by

$$
\hat{H}^{-}:=\mathbb{E}\left[\log \tilde{D}_{\text {out }}^{+}\right]=\frac{1}{\hat{\nu}^{-}} \sum_{i \in[n]} d_{i}^{-} d_{i}^{+}\left(1-s^{-}\right)^{d_{i}^{+}-1} \log d_{i}^{+}
$$

This parameter can be seen as an average row entropy of certain transition matrix (see [6]) and is related to the typical weight of trajectories under subcritical in-growth.

The large deviation rate function (or Cramér function) of $Z=\log \tilde{D}_{\text {out }}^{+}$is defined by

$$
I(z):=\sup _{x \in \mathbb{R}}\left\{x z-\log \mathbb{E}\left[e^{x Z}\right]\right\}, \quad \text { for } z \in \mathbb{R}
$$

Note that $I\left(\hat{H}^{-}\right)=0$ and $I(z)=\infty$ if $z>\log \Delta^{+}$or $z<\log \delta^{+}$. Let

$$
\phi(a):=\frac{1}{a}\left(\left|\log \hat{\nu}^{-}\right|+I\left(a \hat{H}^{-}\right)\right)
$$

and let $a_{0}$ be its minimising value in $[0, \infty)$, which is attained in $[1, \infty)$ by the properties of $I(z)$.



Conditioned on $\overrightarrow{G_{n}}$, a simple random walk on $\overrightarrow{G_{n}}$ is a Markov process $\left(Z_{t}\right)_{t \geq 0}$ with state space $[n]$. Given the current vertex $Z_{t}$, the walk chooses an out-neighbour of $Z_{t}$ uniformly at random as $Z_{t+1}$, which is always possible as we assume $\delta^{+} \geq 2$. If as $t \rightarrow \infty$ the distribution of $Z_{t}$ converges to the same distribution $\pi$ regardless of the choice of $Z_{0}$, i.e., if there exists a probability density function $\pi$ on $[n]$ such that

$$
\lim _{t \rightarrow \infty} \sup _{i, j \in[n]}\left|\mathbb{P}\left\{Z_{t}=j \mid Z_{0}=i\right\}-\pi(j)\right|=0
$$

we say $\left(Z_{t}\right)_{t \geq 0}$ has unique stationary distribution $\pi$. Let

$$
\pi_{\min }=\min \{\pi(i): i \in[n], \pi(i)>0\}, \quad \pi_{\max }=\max \{\pi(i): i \in[n]\}
$$

Our main result is the following:
Theorem 1.1. Assume that $\delta^{+} \geq 2$ and $\Delta^{ \pm} \leq M$ where $M \in \mathbb{N}$ is a fixed integer. With high probability,

$$
\pi_{\min }=n^{-\left(1+\hat{H}^{-} / \phi\left(a_{0}\right)+o(1)\right)}
$$

If $\delta^{-} \geq 2, \pi_{\min }$ satisfies (1.1). In this case, Theorem 1.1 implies the weaker statement $\pi_{\min }=$ $n^{-1+o(1)}$ as we have $\hat{H}^{-} \leq \log M$ and $\phi(a)=\infty$ for any $a \geq 0$, since $\hat{v}^{-}=0$. Thus, our main contribution is to show that when $\delta^{-} \in\{0,1\}$, the polynomial exponent of $\pi_{\min }$ is not -1 any more. The additional exponent $\hat{H}^{-} / \phi\left(\alpha_{0}\right)$ comes from the fact that whp some vertices are exceedingly difficult to reach by a simple random walk. In fact, determining the minimal stationary value can be seen as a competitive combination of two factors: (1) being far from the bulk of other vertices, which is controlled by the term $\left|\log \hat{v}^{-}\right|$in (1.10) and (2) having large branching factors in the trajectories leading to a vertex, which is controlled by the term $I\left(a \hat{H}^{-}\right)$in (1.10). The optimal ratio is given by $a_{0}$. In particular, the vertex that minimises the stationary value is at distance $\left(\frac{1}{a_{0} \phi\left(a_{0}\right)}+\frac{1}{\log \nu}+o(1)\right) \log n$ from the bulk of other vertices. Only when $a_{0}=1$, this vertex coincides with the vertex that is furthest from the bulk, which is at distance $\left(\frac{1}{\left|\log \hat{v}^{-}\right|}+\frac{1}{\log \nu}+o(1)\right) \log n$ (see $[9]$ ).

Remark 1.2. Formally, (1.13) should be read as

$$
\frac{\log \pi_{\min }^{-1}}{\log n} \rightarrow 1+\frac{\hat{H}^{-}}{\phi\left(a_{0}\right)} \quad \text { in probability, }
$$

where we let $\pi_{\min }$ take an arbitrary value in case that the stationary distribution is non-unique. From our proof, one can obtain an upper bound on the speed of convergence of order $(\log n)^{-1 / 2}$. Whp bounds for $\pi_{\min }$ that are tight up to a constant, like the ones in (1.1), are unlikely to hold in this setting due to the use of large deviation theory. In fact, we believe that the rate of convergence $(\log n)^{-1 / 2}$ cannot be vastly improved, as this is the rate of convergence in Cramer's theorem (see Theorem 2.4 in Section 2.3).
Remark 1.3. Let $\overrightarrow{G_{n}}{ }^{s}$ be $\overrightarrow{G_{n}}$ conditioned on being a simple directed graph. Then $\overrightarrow{G_{n}}^{s}$ is distributed uniformly among all simple directed graphs with degree sequence $\vec{d}_{n}$. It is well-known that the probability of $\overrightarrow{G_{n}}$ being simple is bounded away from zero when the maximum degrees are bounded (see, e.g., $[4,18]$ ). Thus, Theorem 1.1 also holds for $\overrightarrow{G_{n}}^{s}$.



Remark 1.4 (Uniqueness of $\pi$ ). Theorem 1.1 requires $\delta_{+} \geq 2$. If $\delta_{+}=0$, then the stationary distribution is almost surely either trivial or non-unique. If $\delta_{+}=1$ and $\mathbb{P}\left\{D_{+}=1\right\}$ is bounded away from 0 as $n \rightarrow \infty$, then with constant probability there will be multiple loops in $\vec{G}_{n}$ giving rise to multiple trivial stationary distributions. Proposition 4.1 shows that under $\delta_{+} \geq 2,\left(Z_{t}\right)_{t \geq 0}$ has a unique stationary distribution whp. Unicity of the equilibrium measure whp can be also shown if $\delta_{+}=1$ and $\mathbb{P}\left\{D_{+}=1\right\}=o(1)$. It is likely that the conclusion of Theorem 1.1 still holds in this situation.

Remark 1.5 (Maximum stationary value). In this paper we turned our attention to $\pi_{\min }$. By averaging, $\pi_{\max } \geq 1 / n$. Moreover, one can check that the proof of the second inequality in (1.2) (see $\left[11\right.$, Section 3.5$]$ ) does not use any condition on $\delta_{-}$. Therefore, $\pi_{\max }=n^{-1+o(1)}$ holds in the setting of Theorem 1.1. It would be interesting to understand the behaviour of $\pi_{\max }$ when the maximum in-degree goes to infinity as $n \rightarrow \infty$.

Remark 1.6 (Explicit polynomial exponents for $\pi_{\min }$ ). Since for a general distribution there is no closed-form expression for $I(z)$, Theorem 1.1 provides an implicit polynomial exponent. Nevertheless, $I(z)$ can be computed explicitly for some particular bi-degree sequences, yielding explicit polynomial exponents. In Subsection 5.2, we give two such examples: one where $I(z)=\infty$ for any $z \neq \hat{H}_{-}$and the other where $I(z)$ is the large deviation rate function of the Bernoulli distribution with an affine transformation. If explicit bounds are required, there is an extensive literature on concentration inequalities for the sum of independent bounded random variables, such as Bernstein's and Bennett's inequalities, (see, e.g., [22]) which provide explicit lower bounds on $I(z)$ in terms of the moments of the distribution. Alternatively, rigorous numeric bounds can be computed with interval arithmetic libraries such as [24].

Remark 1.7. We can choose $\boldsymbol{D}$ to make the polynomial exponent in (1.13) arbitrarily small. For instance, fixing $M \in \mathbb{N}$ and letting

$$
\mathbb{P}\left\{D_{-}=1\right\}=\frac{M-2}{M-1}, \quad \mathbb{P}\left\{D_{-}=M\right\}=\frac{1}{M-1}, \quad \mathbb{P}\left\{D_{+}=2\right\}=1
$$

we have $\hat{\nu}_{-}=1+O\left(M^{-1}\right), \hat{H}_{-}=\log 2$ and $I\left(a \hat{H}_{-}\right)=\infty$ for $a \neq 1$. So $a_{0}=1$ and $\varphi\left(a_{0}\right)=\left|\log \hat{\nu}_{-}\right|$. Thus, $\hat{H}_{-} / \varphi\left(a_{0}\right)=\Omega(M)$ and we can make it as large as we want by increasing $M$.

Remark 1.8. Consider the empirical measure $\psi=\frac{1}{n} \sum_{i \in[n]} \delta_{\{n \pi(i)\}}$; that is, $n$ times the stationary value of a uniform random vertex. In [5], it was shown that there exists a deterministic law $\mathcal{L}$ such that $d_{W}(\psi, \mathcal{L}) \rightarrow 0$ in probability, where $d_{W}$ is the 1-Wasserstein metric related to optimal transport problems. Our proof of Theorem 1.1 allows us to control the lower tail of $\psi$ : for every $\alpha \in\left[0, \hat{H}_{-} / \varphi\left(a_{0}\right)\right]$ and letting $\beta=\frac{\alpha \varphi\left(a_{0}\right)}{\hat{H}_{-}} \in[0,1]$, we have

$$
\mathbb{E}\left[\psi\left(\left(0, n^{-\alpha}\right]\right)\right]=\frac{1}{n} \sum_{i \in[n]} \mathbb{P}\{0<\pi(i) \leq n^{-(1+\alpha)}\}=n^{-\beta+o(1)}
$$

By computing the second moment, it can be shown that $\psi\left(\left(0, n^{-\alpha}\right]\right)$ is concentrated around it expected value. See Subsection 4.6 for a discussion of the proof of (1.16).

Let $\mathcal{G}$ be a directed graph with vertex set $[n]$ having an attractive SCC $C_{0}$ with vertex set $V_{0}$. Let $\left(Z_{t}\right)_{t \geq 0}$ be a simple random walk on $\mathcal{G}$. Let $\tau_{x}(y):=\inf \left\{t \geq 0: Z_{t}=y, Z_{0}=x\right\}$. The maximal



hitting time is defined as

$$
\tau^{\mathrm{hit}}:=\max _{\substack{x \in[n] \\ y \in V_{0}}} \mathbf{E}\left[\tau_{x}(y)\right]
$$

Let $\tau_{x}^{\mathcal{C}}:=\inf \left\{t \geq 0: V_{0} \subseteq \cup_{r=0}^{t}\left\{Z_{r}\right\}, Z_{0}=x\right\}$. The cover time is defined as

$$
\tau^{\mathrm{cov}}:=\max _{x \in[n]} \mathbf{E}\left[\tau_{x}^{\mathcal{C}}\right]
$$

As a consequence of the proof of Theorem 1.1, we determine the maximal hitting and the cover time, up to subpolynomial terms.

Theorem 1.9. Under the hypothesis of Theorem 1.1, whp

$$
\tau^{\mathrm{hit}}=n^{1+\hat{H}}-/\varphi\left(a_{0}\right)+o(1), \quad \tau^{\mathrm{cov}}=n^{1+\hat{H}}-/\varphi\left(a_{0}\right)+o(1)
$$

To simplify the notations, we avoid using $\lceil\cdot\rceil$ and $\lfloor\cdot\rfloor$ to make certain parameters integers. Such omissions should be clear from the context and do not affect the validity of the proofs.

The rest of paper contains four sections: Section 2 studies the properties of marked branching processes; Section 3 describes a graph exploration process and shows that it can be coupled with a marked branching process; using these results, Section 4 proves Theorem 1.1; finally in Section 5 we give some applications, including the proof of Theorem 1.9.

# 2 Marked branching processes 

In this section, we prove some general results for the marked branching process defined below. This part of the paper may be of independent interest.

### 2.1 Marked branching processes

Let $\eta=(\xi, \zeta)$ be a random vector on $\mathbb{Z}_{\geq 0}^{2}$ and let $\left(\eta_{i, t}=\left(\xi_{i, t}, \zeta_{i, t}\right)\right)_{i \geq 1, t \geq 0}$ be iid (independent and identically distributed) copies of $\eta$. The branching process, also known as the Galton-Watson tree, $\left(X_{t}\right)_{t \geq 0}$ with offspring distribution $\xi$ is defined by

$$
X_{t}= \begin{cases}1 & \text { if } t=0 \\ \mathbf{P}_{X_{t-1}} \sum_{i=1}^{\xi_{i, t-1}} & \text { if } t \geq 1\end{cases}
$$

We call $X_{t}$ the $t$-th generation and refer to $\left(X_{r}\right)_{t \geq r \geq 0}$ as the first $t$ generations. For an individual $(i, t)$, we call $i$ its sibling index and $t$ its generation index. If the individual $(i, t)$ is marked (labelled) by the integer $\zeta_{i, t}$, then we call $\left(X_{t}\right)_{t \geq 0}$ the marked branching process with offspring distribution $\eta$.

Let $g$ be the bivariate probability generating function of $\eta$, i.e.,

$$
g(z, w):=\sum_{k, \ell \geq 0} \mathbf{P}\{\eta=(k, \ell)\} z^{k} w^{\ell}
$$

and let $g(z):=g(z, 1)$ be the probability generating function of $\xi$. Define

$$
\nu:=\mathbf{E}[\xi]=g^{\prime}(1)
$$



Given $i \in\left[X_{t}\right]:=\left\{1, \ldots, X_{t}\right\}$ and $r \in[0, t]$, let $f_{r}(i, t) \in\left[X_{t-r}\right]$ be the sibling index of $(i, t)$ 's ancestor $r$ generations away. We write $f(i, t)=f_{1}(i, t)$ and note $f_{0}(i, t)=i$. Let

$$
\Gamma_{i, t}:=\prod_{r=1}^{t} \frac{1}{\zeta_{f_{t-r}(i, t), r}}, \quad \Gamma_{t}:=\sum_{i=1}^{X_{t}} \Gamma_{i, t}
$$

In this section, we will mostly be interested in the sequence of random variables $\left(\Gamma_{t}\right)_{t \geq 1}$. Let $\mathcal{F}_{t}$ be the $\sigma$-algebra generated by $\left(\xi_{i, r}\right)_{i \geq 1, t>r \geq 0},\left(\zeta_{i, r}\right)_{i \geq 1, t \geq r \geq 0}$. Note that $X_{t}$ and $\Gamma_{t}$ are measurable with respect to $\mathcal{F}_{t}$. Moreover, $\Gamma_{t}>0$ if and only if $X_{t}>0$.

The following lemma is similar to [10, Lemma 17].
Lemma 2.1. If $\mathbb{E}[\xi / \zeta] \in(0, \infty)$, then $\Gamma_{t} \mathbb{E}[\xi / \zeta]^{-t}$ is a martingale with respect to $\left(\mathcal{F}_{t}\right)_{t \geq 0}$.
Proof. We have

$$
\begin{aligned}
\mathbb{E}\left[\Gamma_{t} \mid \mathcal{F}_{t-1}\right] & =\sum_{j=1}^{X_{t-1}} \mathbb{E}\left[\sum_{i: f(i, t)=j} \Gamma_{i, t} \mid \mathcal{F}_{t-1}\right] \\
& =\sum_{j=1}^{X_{t-1}} \Gamma_{j, t-1} \mathbb{E}\left[\sum_{i: f(i, t)=j} 1 / \zeta_{i, t} \mid \mathcal{F}_{t-1}\right] \\
& =\sum_{j=1}^{X_{t-1}} \Gamma_{j, t-1} \mathbb{E}\left[\xi_{j, t-1} \mid \mathcal{F}_{t-1}\right] \mathbb{E}\left[1 / \zeta_{i, t} \mid \mathcal{F}_{t-1}\right] \\
& =\sum_{j=1}^{X_{t-1}} \Gamma_{j, t-1} \mathbb{E}[\xi / \zeta]=\mathbb{E}[\xi / \zeta] \Gamma_{t-1}
\end{aligned}
$$

where we use Wald's equation and that $\left\{\xi_{j, t-1}\right\} \cup\left\{\zeta_{i, t}\right\}_{i: f(i)=j}$ is a mutually independent collection of random variables, conditional on $\mathcal{F}_{t-1}$.

Remark 2.2. Recall the definition of the distribution $D_{\text {out }}$ in (1.6) and let $\eta \stackrel{\mathcal{L}}{=} D_{\text {out }}$. Since $\ell \geq$ $\delta_{+} \geq 2$, we have

$$
\mathbb{E}[\xi / \zeta]=\mathbb{E}\left[D_{\text {out }}^{-} / D_{\text {out }}^{+}\right]=\sum_{k, \ell \geq 1} \frac{k}{\ell \cdot \ell n_{k, \ell}} \frac{m}{m}=\frac{1}{m} \sum_{k, \ell \geq 1} k n_{k, \ell}=1
$$

In other words, if we take $\eta=D_{\text {out }}, \Gamma_{t}$ is a martingale by Lemma 2.1. We have formulated the lemma in a way that will allow us to deal with small perturbations of $D_{\text {out }}$.

# 2.2 Conditioned branching processes 

Before introducing the results, some more definitions are needed.



# 2.2.1 Conditioned on extinction 

Let $s:=\mathbb{P}\left\{\cap_{t \geq 0}\left[X_{t}>0\right]\right\}$ be the survival probability of $\left(X_{t}\right)_{t \geq 0}$. The conjugate probability distribution of $\xi$, denoted by $\hat{\xi}$, is defined as

$$
\mathbb{P}\{\hat{\xi}=k\}:=(1-s)^{k-1} \mathbb{P}\{\xi=k\}
$$

when $s<1$, while

$$
\mathbb{P}\{\hat{\xi}=1\}=\mathbb{P}\{\xi=1\}, \quad \mathbb{P}\{\hat{\xi}=0\}=1-\mathbb{P}\{\xi=1\}
$$

when $s=1$. Define the subcritical expansion rate as

$$
\hat{\nu}:=\mathbb{E}[\hat{\xi}]=g^{\prime}(1-s) \in[0,1)
$$

If $\eta \stackrel{D}{=} \mathbf{D}_{\text {out }}$, then $\hat{\nu}=\hat{\nu}_{-}$where $\hat{\nu}_{-}$is defined in (1.5).
The following duality is well-known:
Theorem 2.3 (see, e.g., Theorem 3.7 in [25]). Let $\left(X_{t}\right)_{t \geq 0}$ be a branching process with offspring distribution $\xi$ and survival probability s. If $\mathrm{s}<1$, then the branching process $\left(X_{t}\right)_{t \geq 0}$ conditioned on extinction is distributed as a branching process with offspring distribution $\hat{\xi}$.

### 2.2.2 Conditioned on survival

Let $\left(X_{t}^{*}\right)_{t \geq 0} \subseteq\left(X_{t}\right)_{t \geq 0}$ be the subprocess of the individuals that have some surviving progeny. Thus, $\mathbb{P}\left\{X_{0}^{*}=0\right\}=1-s$ and $\mathbb{P}\left\{X_{0}^{*}=1\right\}=s$. Conditioning on $\left[X_{0}^{*}=1\right]$, i.e., survival of $\left(X_{t}\right)_{t \geq 0}$, $\left(X_{t}^{*}\right)_{t \geq 0}$ is a branching process with offspring distribution $\xi^{*}$, defined by

$$
\mathbb{P}\left\{\xi^{*}=k\right\}=\frac{\sum_{m \geq k} \mathbb{P}\{\xi=m\}\binom{m}{k} s^{k}(1-s)^{m-k}}{s}=s^{k-1} \frac{g^{(k)}(1-s)}{k!}, \quad \text { for } k \geq 1
$$

A simple computation gives

$$
\mathbb{E}\left[\xi^{*}\right]=\sum_{k=1}^{\infty} k s^{k-1} \frac{g^{(k)}(1-s)}{k!}=\sum_{k=0}^{\infty} s^{k} \frac{g^{(k+1)}(1-s)}{k!}=g^{\prime}(1)=\nu
$$

Moreover,

$$
\mathbb{P}\left\{\xi^{*}=1\right\}=\mathbb{P}\left\{X_{1}^{*}=1 \mid X_{0}^{*}=1\right\}=g^{\prime}(1-s)=\hat{\nu}
$$

Let $\left(\tilde{X}_{t}\right)_{t \geq 0} \subseteq\left(X_{t}^{*}\right)_{t \geq 0}$ be the subprocess of the individuals that have exactly one surviving element in their offspring. (Note that $\left(\tilde{X}_{t}\right)_{t \geq 0}$ is not necessarily a connected process.) Then conditioned on $(i, t) \in\left(\tilde{X}_{t}\right)_{t \geq 0}, \xi_{i, t}$ is distributed as $\tilde{\xi}$, defined by

$$
\mathbb{P}\{\tilde{\xi}=k\}=\mathbb{P}\left\{X_{1}=k \mid X_{1}^{*}=1\right\}=\frac{k(1-s)^{k-1}}{\hat{\nu}} \mathbb{P}\{\xi=k\}, \quad \text { for } k \geq 1
$$

Let $\tilde{\eta}=(\tilde{\xi}, \tilde{\zeta})$ be the distribution of $\eta_{i, t}$ conditioned on $(i, t) \in\left(\tilde{X}_{t}\right)_{t \geq 0}$. Define the subcritical entropy of $\eta$ as

$$
\hat{H}:=\mathbb{E}[\log \tilde{\zeta}]=\frac{1}{\hat{\nu}} \sum_{k \geq 1, \ell \geq 0} k(\log \ell)(1-s)^{k-1} \mathbb{P}\{\eta=(k, \ell)\}=\frac{\mathbb{E}[\hat{\xi} \log \zeta]}{\mathbb{E}[\hat{\xi}]}
$$



This parameter is central in our results. If $\eta^{\mathrm{L}}=D_{\text {out }}$, then $\hat{H}=\hat{H}^{-}$where $\hat{H}^{-}$is defined in (1.8). Later, we will also consider the inhomogeneous branching process $\left(\hat{X}_{t}\right)_{t \geq 0}$ in which the root has offspring distribution $\tilde{\xi}-1$ and all other individuals have offspring distribution $\hat{\xi}$. Note that such a process will almost surely become extinct.

# 2.3 Large deviation theory 

We will use Cramér's theorem, a classical result in large deviation theory.
Theorem 2.4 (see, e.g., Corollary 2.2.19 in [16]). Let $Z_{1}, \ldots, Z_{t}$ be iid copies of a random variable $Z$ satisfying $\mathbb{E}\left[e^{\lambda Z}\right]<\infty$ for all $\lambda \in \mathbb{R}$. Define

$$
\bar{Z}_{t}=\frac{1}{t} \sum_{i=1}^{t} Z_{i}
$$

Then, for any $z \geq \mathbb{E}[Z]$

$$
\lim _{t \rightarrow \infty} \frac{1}{t} \log \mathbb{P}\left\{\bar{Z}_{t} \geq z\right\}=-I(z)
$$

where

$$
I(z):=\sup _{\lambda \in \mathbb{R}}\left\{z \lambda-\log \mathbb{E}\left[e^{\lambda Z}\right]\right\}, \text { for } z \in \mathbb{R}
$$

is the Fenchel-Legendre transform of the cumulant generating function of $Z$.
From now on we will take $Z$ to be the discrete, random variable $\log \tilde{\zeta}$ with support a subset of $\{\log 2, \log 3, \ldots, \log M\}$, where $M$ is a fixed integer, and expected value $\hat{H}$. Thus, $I(z)$ will refer to the large deviation rate function of $\log \tilde{\zeta}$ which has the properties that on $z \in[\hat{H}, \log M), I(z)$ is continuous, non-decreasing, with $I^{\prime}(z) \in[0, \infty)$ and $I(\hat{H})=0$. Moreover, $I(z)$ is non-increasing on $(-\infty, \hat{H})$. The proof for these properties follow along the line of Lemma 2.2 .5 in [16].

### 2.4 Subcritical growth: a lower bound

Theorem 2.5. Let $\left(X_{r}\right)_{r \geq 0}$ be a marked branching process with offspring distribution $\eta=(\xi, \zeta)$ with $\mathbb{E}[\xi] \in(1, \infty)$. Suppose that $\xi \leq M$ and $2 \leq \zeta \leq M$ almost surely. Then for any $a \in[1, \log (M) / \hat{H}]$, $t \rightarrow \infty$ and $\omega \geq t$,

$$
\begin{gathered}
\mathbb{P}\left(\left\{0<\Gamma_{t}<e^{-a \hat{H} t}\right\} \cap \bigcap_{r=1}^{t}\left[0<X_{r}<\omega\right]\right) \\
\geq \exp \{-(|\log \hat{\nu}|+I(a \hat{H})+o(1)) t\}
\end{gathered}
$$

The important event in the previous theorem is the first one, regarding $\Gamma_{t}$. The event controlling the size of the first $t$ generations is only added to allow coupling the branching process with the graph exploration later in the paper.

Proof. For the sake of simplicity, we first prove the theorem assuming that 1 is in the support of $\xi$. The modifications needed otherwise, are detailed at the end of the proof.

Consider the events $E_{1}=\left[X_{t}^{*}=1\right], E_{2}=\left[X_{t}=1\right]$ and $E_{3}=\bigcap_{r=1}^{t}\left[0<X_{r}<\omega\right]$. The idea of the proof is to lower bound the probability in (2.17) conditioning on these events.

When the event $E_{1}$ happens, we call the first $t$ generations of $\left(X_{r}^{*}\right)_{r \geq 0}$ the spine. We may assume without loss of generality that the spine of survival individuals corresponds to the first



individual in each generation, since reordering sibling indices does not change the value of $\Gamma_{t}$ or $X_{r}$. Moreover, the number of children (in $\left(X_{r}\right)_{r \geq 0}$ ) and the mark of each individual in the spine is jointly distributed as $\tilde{\eta}=(\tilde{\xi}, \tilde{\zeta})$.

For $\mathbf{x}=\left(x_{1}, \ldots, x_{t}\right) \in \mathfrak{M}:=\{1\} \times[M]^{t-1}$, let $\mathcal{F}(\mathbf{x})$ be the intersection of the event $\mathcal{E}_{1}$ and the event that $\xi_{1,0}=x_{t}, \ldots, \xi_{1, t-1}=x_{1}$, i.e., the $r$-generation of the spine has $x_{t-r}$ children. (We require $x_{1}=1$, which is in the support of $\xi$, so that $\mathcal{F}(\mathbf{x}) \cap \mathcal{E}_{2}$ is not empty.) On the event $\mathcal{F}(\mathbf{x})$, the first $t$ generations of the Galton-Watson tree $\left(X_{r}\right)_{r \geq 0}$ can be constructed equivalently as follows - First start with a one-ary tree (a path) of length $t$ which serves as the spine. Then for the $r$-generation individual in the spine with $r \in\{0, \ldots, t-1\}$, attach an independent copy of the branching process $\left(\hat{X}_{j}\right)_{j \geq 0}$ (defined in Subsection 2.2.2), conditioned on its root having $x_{t-r}-1$ children.

On $\mathcal{E}_{1}$, let us write

$$
\Gamma_{t}=\Gamma_{1, t}+\sum_{i=2}^{X_{t}} \Gamma_{i, t}=: \Gamma_{t}^{*}+\Gamma_{t}^{0}
$$

As $\mathcal{E}_{2}$ implies $\left[\Gamma_{t}^{0}=0\right]$, the desired probability is at least

$$
\mathbb{P}\left\{\left[0<\Gamma_{t}^{*}<e^{-a \hat{H} t}\right] \cap \mathcal{E}_{1} \cap \mathcal{E}_{2} \cap \mathcal{E}_{3}\right\} \geq \mathbb{P}\left\{\mathcal{E}_{1}\right\} \mathbb{P}\left\{\Gamma_{t}^{*}<e^{-a \hat{H} t} \mid \mathcal{E}_{1}\right\} \min _{\mathbf{x} \in \mathfrak{M}} \mathbb{P}\left\{\mathcal{E}_{2} \cap \mathcal{E}_{3} \mid \mathcal{F}(\mathbf{x})\right\}
$$

where we use that given $\mathcal{F}(\mathbf{x}), \mathcal{E}_{2} \cap \mathcal{E}_{3}$ is independent from [ $\left.\Gamma_{t}^{*}<e^{-a \hat{H} t}\right]$.
Let us first bound the probability of $\mathcal{E}_{1}$. By (2.11), one has

$$
\mathbb{P}\left\{\mathcal{E}_{1}\right\}=\mathbb{P}\left\{X_{0}^{*}=1\right\} \prod_{i=1}^{t} \mathbb{P}\left\{X_{i}^{*}=1 \mid X_{i-1}^{*}=1\right\}=\mathbb{P}\left\{X_{0}^{*}=1\right\} \mathbb{P}\left\{\xi^{*}=1\right\}^{t}=s^{\hat{\nu} t}
$$

We now bound $\Gamma_{t}^{*}$ on $\mathcal{E}_{1}$. Let $\zeta_{r}$ be the mark of the spine individual in generation $r$. Then $\zeta_{r}$ are IID copies of $\tilde{\zeta}$. Letting $Z_{r}=\log \zeta_{r}$ we have

$$
\Gamma_{t}^{*}=\prod_{r=1}^{t}\left(\zeta_{r}\right)^{-1}=e^{-\sum_{r=1}^{t} Z_{r}}
$$

As $t \rightarrow \infty$, it follows from Cramér's theorem (Theorem 2.4) that

$$
\mathbb{P}\left\{\Gamma_{t}^{*}<e^{-a \hat{H} t} \mid \mathcal{E}_{1}\right\}=e^{-(1+o(1)) I(a \hat{H}) t}
$$

We finally obtain a bound on the probability of $\mathcal{E}_{2} \cap \mathcal{E}_{3}$ conditional on $\mathcal{F}(\mathbf{x})$, uniform over $\mathbf{x} \in \mathfrak{M}$. By the independence of the trees attached to the spine,

$$
\mathbb{P}\left\{\mathcal{E}_{2} \mid \mathcal{F}(\mathbf{x})\right\}=\prod_{r=0}^{t-1} \mathbb{P}\left\{\hat{X}_{t-r}=0 \mid \hat{X}_{1}=x_{t-r}-1\right\}=\prod_{r=2}^{t} \mathbb{P}\left\{\hat{X}_{r}=0 \mid \hat{X}_{1}=x_{r}-1\right\}
$$

where the last step uses that $x_{1}=1$. For $r \geq 2$, using that $x_{r} \leq M$, we have

$$
\mathbb{P}\left\{\hat{X}_{r}=0 \mid \hat{X}_{1}=x_{r}-1\right\} \geq \mathbb{P}\left\{\hat{X}_{2}=0 \mid \hat{X}_{1}=M\right\}=\mathbb{P}\{\hat{\xi}=0\}^{M}
$$



Also, by Markov inequality, there exists a constant $r_{0}$ such that for all $r \geq r_{0}$

$$
\mathbb{P}\left\{\hat{X}_{r} \geq 1 \mid \hat{X}_{1}=x_{r}\right\} \leq \mathbb{E}\left[\hat{X}_{r} \mid \hat{X}_{1}=M\right]=M \hat{v}^{r-1} \leq 1 / 2
$$

It follows that

$$
\mathbb{P}\left\{\mathcal{E}_{2} \mid \mathcal{F}(x)\right\} \geq \mathbb{P}\left\{\hat{\xi}=0 \quad \text { or }_{r>r_{0}} M \prod \left(1-M \hat{v}^{r-1}\right)>c_{0}\right\}
$$

for some constant $c_{0}>0$.
To bound the probability of $\mathcal{E}_{3}$, we use the same argument as in [9, Theorem 3.4]. Note that $X_{r}>0$ is already implied by $\mathcal{E}_{1}$, it suffices to bound the probability $X_{r}$ is not too large. By linearity of expectation,

$$
\mathbb{E}\left[X_{r} \mid \mathcal{F}(x)\right]=1+\sum_{j=1}^{r}\left(x_{j-r+t}-1\right) \hat{v}^{j-1}=O(1)
$$

and by independence of the branching subtrees

$$
\begin{aligned}
\operatorname{Var}\left(X_{r} \mid \mathcal{F}(x)\right) & \leq \sum_{j=1}^{r}\left(x_{j-r+t}-1\right) \operatorname{Var}(\hat{\xi}) \hat{v}^{j-2}\left(\hat{v}^{j-1}-1\right) \hat{v}-1 \\
& =O(\operatorname{Var}(\hat{\xi}))=O(1)
\end{aligned}
$$

where we use the moment formula in [3, pp. 4]. Thus, we have $\mathbb{E}\left[\hat{X}_{r}^{2}\right]=O(1)$ and it follows from Chebyshev's inequality that

$$
\mathbb{P}\left\{\mathcal{E}_{3}^{c} \mid \mathcal{F}(x)\right\} \leq \sum_{r=0}^{t} \mathbb{P}\left\{X_{r} \geq \omega \mid \mathcal{F}(x)\right\} \leq \sum_{r=1}^{t} \frac{\mathbb{E}\left[X_{r}^{2} \mid \mathcal{F}(x)\right]}{\omega^{2}}=O\left(t / \omega^{2}\right)=O\left(t^{-1}\right)
$$

From (2.26) and (2.27), we obtain

$$
\mathbb{P}\left\{\mathcal{E}_{2} \cap \mathcal{E}_{3} \mid \mathcal{F}(x)\right\} \geq \mathbb{P}\left\{\mathcal{E}_{2} \mid \mathcal{F}(x)\right\}-\mathbb{P}\left\{\mathcal{E}_{3}^{c} \mid \mathcal{F}(x)\right\} \geq c_{0} / 2
$$

The desired bound follows from plugging (2.20), (2.22) and (2.28) into (2.19).
If the minimal positive support of $\xi$ is $k_{0} \geq 2$, then the only change needed is to let $\mathcal{E}_{2}=\left[X_{t}=\right.$ $\left.k_{0}\right]$. The extra $k_{0}-1$ individuals in generation $t$ contribute at most $k_{0} M \Gamma_{1, t}$ to $\Gamma_{t}$. Thus the same argument still works.

# 2.5 Subcritical growth: an upper bound 

Given a fixed $\gamma>0$, let $\mathcal{G}_{t}(\gamma)$ be the event $\cap_{i \in\left[X_{t}\right]}\left[\Gamma_{i, t} \geq \gamma\right]$. In the rest of Subsection 2.5, we will prove the following theorem:

Theorem 2.6. Let $\left(X_{r}\right)_{r \geq 0}$ be a marked branching process with offspring distribution $\eta=(\xi, \zeta)$ with $\mathbb{E}[\xi] \in(1, \infty)$. Suppose that $\xi \leq M$ and $2 \leq \zeta \leq M$ almost surely. Let $\omega \rightarrow \infty, t \in\left(\log _{2} \omega, \omega^{1 / 2}\right)$ and $a \geq 1$. Then, we have

$$
\mathbb{P}\left\{\left(\mathcal{G}_{t}\left(e^{-a \hat{H}_{t}}\right)\right)^{c} \cap\left[0<X_{t}<\omega\right]\right\} \leq \exp \left\{-\left(|\log \hat{\nu}|+I(a \hat{H})+o(1)\right) t\right\}
$$



# 2.5.1 An inhomogeneous branching process 

Fix $t$ and let $\left(\mathfrak{X}_{r}^{(t)}\right)_{t \geq r \geq 0} \subseteq\left(\mathfrak{X}_{r}\right)_{r \geq 0}$ be the finite subprocess containing individuals in the first $t$ generations that have progeny in generation $t$. Similar to $\mathfrak{X}_{r}^{*}, \mathfrak{X}_{r}^{(t)}$ is non-decreasing in $r$ Conditioned on the event $\left[\mathfrak{X}_{t}>0\right],\left(\mathfrak{X}_{r}^{(t)}\right)_{t \geq r \geq 0}$ can be seen as an inhomogeneous branching process. The offspring distribution of the individuals in generation $r=t-a$ in this process is $\xi^{(a)}$, defined by

$$
\mathbb{P}\left\{\xi^{(a)}=k\right\}=\frac{1}{s_{a}} \sum_{m \geq k} \mathbb{P}\{\xi=m\}\binom{m}{k} \frac{s_{k}^{k-1}\left(1-s_{a^{-1}}\right)^{m-k}}{s_{a}^{k} k!} h^{(k)}\left(1-s_{a^{-1}}\right), \quad \text { for } k \geq 1
$$

where $s_{a}:=\mathbb{P}\left\{\mathfrak{X}_{a}>0\right\}$.
Note the similarity between $\xi^{(a)}$ and $\xi^{*}$ which is defined in (2.9). We have $s_{a}=s+O\left(\hat{\nu}^{a}\right)$ (see $\left.[9, \mathrm{Eq} .(3.6)]\right)$. Using the Taylor expansion of $h^{(k)}$ around $1-s$, we get

$$
\mathbb{P}\left\{\xi^{(a)}=k\right\}=\frac{s^{k-1}}{k!} h^{(k)}(1-s)+O\left(\hat{\nu}^{a}\right)=\mathbb{P}\left\{\xi^{*}=k\right\}+O\left(\hat{\nu}^{a}\right), \quad \text { for } 0 \leq a \leq t, k \geq 1
$$

In particular, by $(2.11)$

$$
\mathbb{P}\left\{\xi^{(a)}=1\right\}=\mathbb{P}\left\{\xi^{*}=1\right\}+O\left(\hat{\nu}^{a}\right)=\hat{\nu}+O\left(\hat{\nu}^{a}\right)
$$

When $0 \leq \xi \leq M$ almost surely, it follows from (2.31) that

$$
\mathbb{E}\left[\xi^{(a)}\right]=\mathbb{E}\left[\xi^{*}\right]+O\left(\hat{\nu}^{a}\right)=\nu+O\left(\hat{\nu}^{a}\right)
$$

### 2.5.2 Control the surviving process

Denote by $\mathbb{P}_{t}\{\cdot\}:=\mathbb{P}\left\{\cdot \mid \mathfrak{X}_{t}>0\right\}$ the probability conditioned to survival at time $t$.
The argument for the following lemma is similar that of Theorem 3.4 in our previous work [9]. We give a proof for completeness.

Lemma 2.7. Let $t$ and $\omega$ be as in Theorem 2.6. Set $t_{0}:=\left(\frac{1}{|\log \hat{\nu}|}+\frac{1}{\log \nu}\right) \log \omega=o(t)$. There exists a constant $C_{0} \geq 1$ such that for any $h \leq t-t_{0}, i \leq h$,

$$
\mathbb{P}_{t}\left\{\mathfrak{X}_{i}^{(t-h+i)}<\omega\right\} \leq C_{0} \hat{\nu}^{i-t_{0}}
$$

Proof. Conditioned on survival at time $t-r,\left(\mathfrak{X}_{j}^{(t-r)}\right)_{t-r \geq j \geq 0}$ is a branching process where the individuals at generation $j$ have offspring distribution $\xi^{(t-r-j)}$ defined in (2.30). Recall that $\mathbb{E}\left[\xi^{*}\right]=$ $\nu>1$. By (2.33) and since $r+j \leq h \leq t-t_{0}$, by the choice of $t_{0}$, we have

$$
\mathbb{P}\left\{\xi^{(t-r-j)}=k\right\}=\mathbb{P}\left\{\xi^{*}=k\right\}+O\left(\hat{\nu}^{t_{0}}\right)=\mathbb{P}\left\{\xi^{*}=k\right\}+O\left(\omega^{-1}\right), \quad \text { for } k \geq 1
$$

and similarly

$$
\mathbb{E}\left[\xi^{(t-r-j)}\right]=\nu\left(1+O\left(\omega^{-1}\right)\right)
$$

Choose $\varepsilon>0$ so $((1-\varepsilon) \nu)^{t_{0}} \geq \omega^{-1}$; this is possible as $\hat{\nu}<1$. Let $\xi_{-}$be a fixed distribution such that each $\xi^{(t-r-j)}$ stochastically dominates $\xi_{-}$and $\nu_{-}:=\mathbb{E}\left[\xi_{-}\right] \geq \nu(1-\varepsilon)$. Let $\left(\mathfrak{X}_{j}^{-}\right)_{j \geq 0}$ be $a$



branching process with offspring distribution $\xi^{-}$. The processes $\left(X_{j}^{(t-r)}\right)_{h-r \geq j \geq 0}$ and $\left(X_{j}^{-}\right)_{j \geq 0}$ can be coupled so $X_{j}^{(t-r)} \geq X_{j}^{-}$almost surely for every $j \leq h-r$.

By a theorem due to Kesten and Stigum [9, Theorem 3.1], conditioned on survival, $\left(\nu^{-}\right)^{-j} X_{j}^{-}$ converges almost surely to a non-degenerated random variable $W$ which is absolutely continuous on $(0, \infty)$. Writing $i=h-r$, let $a_{i}:=\mathbb{P}_{t-h+i}\left\{X_{i}^{(t-h+i)}<\omega\right\}$. It follows that

$$
\begin{aligned}
1-a_{t_{0}} & =\mathbb{P}_{t-h+t_{0}}\left\{X_{t_{0}}^{(t-h+t_{0})} \geq \omega\right\} \\
& \geq \mathbb{P}\left\{X_{t_{0}}^{-} \geq \omega\right\} \\
& \geq \mathbb{P}\left\{X_{t_{0}}^{-} \geq\left(\nu^{-}\right)^{t_{0}}\right\} \\
& \geq \mathbb{P}\left\{1 \leq\left(\nu^{-}\right)^{-t_{0}} X_{t_{0}}^{-} \leq 2\right\} \\
& \rightarrow \mathbb{P}\{1 \leq W \leq 2\}>0
\end{aligned}
$$

As $t_{0} \rightarrow \infty, 1-a_{t_{0}}$ is bounded away from 0 . So we have $a_{t_{0}} \leq 1-c_{0}$, for some $c_{0}>0$.
By splitting depending on whether the offspring of the first generation is one or larger, we have the simple recursive inequality for $i>t_{0}$ :

$$
\begin{aligned}
a_{i} & \leq \mathbb{P}\left\{\xi^{(t-h+i)}=1\right\} a_{i-1}+\left(1-\mathbb{P}\left\{\xi^{(t-h+i)}=1\right\}\right) a_{i-1}^{2} \\
& =\hat{v} a_{i-1}+(1-\hat{v}) a_{i-1}^{2}+\mathcal{O}\left(\hat{\nu}^{t-h+i} a_{i-1}\right)
\end{aligned}
$$

This recursion has exactly the same form of as $[23$, Eq. (2.4)] and can be solved the same way to show that there exists a constant $C_{0}$ such that for $i \leq h$

$$
a_{i}=\mathbb{P}_{t}\left\{X_{i}^{(t-h+i)}<\omega\right\} \leq C_{0} \hat{\nu}^{i-t_{0}}
$$

# 2.5.3 Ramifications in the spine 

Conditioned on survival at time $t$, let $x$ be an individual of generation $h$ of $\left(X_{r}^{(t)}\right)_{t \geq r \geq 0}$. Let $y_{0}, y_{1}, \ldots, y_{h}=x$ be the path connecting the root $y_{0}$ to $x$, which we refer to as the spine associated to $x$. An index $r \in\{0, \ldots, h-1\}$ is a ramification ${ }^{1}$ of the spine, if $y_{r}$ has offspring at least 2 in $\left(X_{r}^{(t)}\right)_{t \geq r \geq 0}$. Let $R(x)$ to be the number ramifications of the spine associated to $x$. The following result refines Lemma 2.7 by considering the number of ramifications.

Lemma 2.8. Let $t_{0}$ be as in Lemma 2.7. There exists a constant $C_{1}>0$ such that, for $h \leq t-t_{0}$ and every individual $x$ of generation $h$ of $\left(X_{r}^{(t)}\right)_{t \geq r \geq 0}$, we have

$$
\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega, R(x) \geq \ell\right\} \leq \hat{\nu}^{h+\left(t_{0}-C_{1}\right)\left(\ell-2 t_{0}\right)}, \quad \text { for } 2 t_{0}<\ell \leq h
$$

Proof. By permuting the sibling index of individuals, we may assume that $x$ is the first individual of $X_{h}^{(t)}$. One can decompose the set of individuals in each generation of $\left(X_{r}^{(t)}\right)_{t \geq r \geq 0}$ according to their the last ancestor with $x$, i.e., the last of their ancestors that belongs to the spine $y_{0}, \ldots, y_{h}=x$. Conditioning on $\left[X_{t}>0\right]$, the number of children of $y_{0}, \ldots, y_{h-1}$ in $\left(X_{r}^{(t)}\right)_{t \geq r \geq 0}$ is distributed as independent random variables $\xi_{0}, \ldots, \xi_{h-1}$ where $\xi_{L}^{r}=\xi^{(t-r)}$.

Therefore, we can generate $\left(X_{r}^{(t)}\right)_{h \geq r \geq 0}$ equivalently as follows: (i) construct the spine; (ii) for every $r \in[h-1]$ attach $\xi_{r}-1$ independent copies of $\left(X_{j}^{(t-(r+1))}\right)_{h-r-1 \geq j \geq 0}$ conditioned on $\left[X_{t}>0\right]$, which we denote by $\left(W_{j}^{r, 2}\right)_{h-(r+1) \geq j \geq 0}, \ldots,\left(W_{j}^{r, \xi_{r}}\right)_{h-(r+1) \geq j \geq 0}$, to $y_{r}$.

[^0]
[^0]:    ${ }^{1}$ The word "ramification" means "a complex or unwelcome consequence of an action or event."



Looking at the generation $h$, this decomposition gives the following recursive inequality:

$$
X_{h}^{(t)}=1+\sum_{r=0}^{h-1} \xi_{r} \sum_{k \geq 2} W_{h-(r+1)}^{r, k} \succeq \sum_{i=1}^{h} Z_{i} \succeq \sum_{i=2 t_{0}+1}^{h} Z_{i},
$$

where $\succeq$ denotes stochastically domination and $Z_{1}, \ldots, Z_{h}$ are independent random variables with distribution

$$
Z_{i} \stackrel{\mathcal{L}}{=} \begin{cases}0 & \text { with probability } b_{i}, \\ \left(X_{i-1}^{(t-h+i-1)} \mid X_{t-h+i-1}>0\right) & \text { with probability } 1-b_{i},\end{cases}
$$

where $b_{i}:=\mathbb{P}\left\{\xi^{(t-h+i)}=1\right\}$.
Let $R_{0}(x)$ be the number of ramifications of the spine associated to $x$ with index at most $h-2 t_{0}-1$. Let $p_{\ell}:=\mathbb{P}_{n}\left\{X_{h}^{(t)}<\omega, R_{0}(x)=\ell\right\}$. Recalling that $a_{i}:=\mathbb{P}_{t-h+i}\left\{X_{i}^{(t-h+i)}<\omega\right\}$, we have

$$
p_{\ell} \leq \sum_{i_{1}<i_{2} \cdots<i_{\ell}} \prod_{j=1}^{\ell} a_{i_{j}-1}\left(1-b_{i_{j}}\right) \prod_{2 t_{0}<j \leq T} \frac{j \notin\left\{i_{1}, \ldots, i_{\ell}\right\}}{b_{j}}
$$

where the sum is over all choices of $\ell$ ordered and distinct indices from $\left\{2 t_{0}+1, T\right\}$, which indicate where the ramifications occur. Since $t-h \geq t_{0}$ and by (2.32), we have that $b_{i}=\hat{\nu}+O\left(\hat{\nu}^{t-h+i}\right)=$ $\hat{\nu}+O\left(\hat{\nu}^{3 t_{0}}\right)$ for all $2 t_{0} \leq i \leq h$. Thus, (2.42) implies that

$$
\begin{aligned}
p_{\ell} & \leq\left(1-\hat{\nu}+O\left(\hat{\nu}^{3 t_{0}}\right)\right)^{\ell}\left(\hat{\nu}+O\left(\hat{\nu}^{3 t_{0}}\right)\right)^{h-2 t_{0}-\ell} \sum_{i_{1}<i_{2} \cdots<i_{\ell}} \prod_{j=1}^{\ell} a_{i_{j}-1} \\
& =(1+o(1))(1-\hat{\nu})^{\ell} \hat{\nu}^{h-2 t_{0}-\ell} \sum_{i_{1}<i_{2} \cdots<i_{\ell}} \prod_{j=1}^{\ell} a_{i_{j}-1}
\end{aligned}
$$

where the last step uses that $\hat{\nu}^{t_{0}} \leq \omega^{-1}$, that $h \leq t \leq \sqrt{\omega}$ and $\omega \rightarrow \infty$. Moreover,

$$
\sum_{i_{1}<\cdots<i_{\ell}} \prod_{j=1}^{\ell} a_{i_{j}-1} \leq\left(\sum_{i=2 t_{0}+1}^{h} a_{i-1}\right)^{\ell} \leq\left(\sum_{i=2 t_{0}+1}^{\infty} C_{0} \hat{\nu}^{i-t_{0}-1}\right)^{\ell} \leq\left(\frac{C_{0}}{\hat{\nu}^{t_{0}}(1-\hat{\nu})}\right)^{\ell}
$$

Putting this back to (2.43) we have

$$
p_{\ell}=(1+o(1)) \hat{\nu}^{h-2 t_{0}}\left(C_{0} \hat{\nu}^{t_{0}-1}\right)^{\ell}
$$

Since there are at most $2 t_{0}-1$ ramifications in the last $2 t_{0}-1$ indices of the spine (excluding $x$ ), it follows that for $\ell>2 t_{0}$

$$
\begin{aligned}
\mathbb{P}_{n}\left\{X_{h}^{(t)}<\omega, R(x) \geq \ell\right\} & \leq \mathbb{P}_{n}\left\{X_{h}^{(t)}<\omega, R_{0}(x) \geq \ell-2 t_{0}\right\}=\sum_{j \geq \ell-2 t_{0}} p_{j} \\
& =O(1) \hat{\nu}^{h-2 t_{0}}\left(C_{0} \hat{\nu}^{t_{0}-1}\right)^{\ell-2 t_{0}} \\
& \leq \hat{\nu}^{h+\left(t_{0}-C_{1}\right)\left(\ell-2 t_{0}\right)}
\end{aligned}
$$

for some constant $C_{1}>0$



# 2.5.4 Finishing the proof of Theorem 2.6 

Let $h=t-t_{0}$ and define

$$
\ell(t):=\frac{2 t_{0}+t+t_{0}}{t_{0}-C_{1}}>2 t_{0}
$$

Then, by Lemma 2.8

$$
\mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega, R(x) \geq \ell(t)\right\} \leq \hat{\nu}^{2 t}
$$

Clearly, the event $\left[X_{t}<\omega\right]$ implies $\left[X_{h}^{(t)}<\omega\right]$, for every $h \leq t$. Let $x_{1}, \ldots, x_{X_{h}^{(t)}}$ denote the individuals in generation $h$ of $\left(X_{r}^{(t)}\right)_{t \geq r \geq 0}$. Let $E_{1}$ be the event $\cap_{i=1}^{X_{h}^{(t)}}\left[R\left(x_{i}\right)<\ell(t)\right]$. By a union bound over the choice of $i$ and using (2.48)

$$
\mathbb{P}_{t}\left\{X_{t}<\omega, E_{1}^{c}\right\} \leq \mathbb{P}_{t}\left\{X_{h}^{(t)}<\omega, E_{1}^{c}\right\} \leq \sum_{j=1}^{\omega-1} \sum_{i=1}^{j} \mathbb{P}_{t}\left\{X_{h}^{(t)}=j, R\left(x_{i}\right) \geq \ell(t)\right\} \leq \omega \hat{\nu}^{2 t}
$$

Let $E_{2}:=\left[0<X_{t}<\omega\right] \cap E_{1}$. Since $X_{h}^{(t)}$ is non-decreasing, by Lemma 2.7 with $i=h=t-t_{0}$, we have

$$
\mathbb{P}_{t}\left\{E_{2}\right\} \leq \mathbb{P}_{t}\left\{X_{t}<\omega\right\} \leq \mathbb{P}_{t}\left\{X_{t-t_{0}}^{(t)}<\omega\right\} \leq C_{0} \hat{\nu}^{t-2 t_{0}}=O\left(\omega^{C_{2}}\right) \hat{\nu}^{t}
$$

where $C_{2}:=\frac{2+2|\log \hat{\nu}|}{\log \nu}$. Let $\gamma:=e^{-a \hat{H}_{t}}$. It follows from (2.49) and (2.50) that

$$
\begin{aligned}
\mathbb{P}_{t}\left\{G_{t}^{c}(\gamma), X_{t}<\omega\right\} & \leq \mathbb{P}_{t}\left\{G_{t}^{c}(\gamma), X_{t}<\omega, E_{1}\right\}+\mathbb{P}_{t}\left\{X_{t}<\omega, E_{1}^{c}\right\} \\
& \leq \mathbb{P}\left\{G_{t}^{c}(\gamma) \mid E_{2}\right\} \mathbb{P}_{t}\left\{E_{2}\right\}+\omega \hat{\nu}^{2 t} \\
& \leq O\left(\omega^{C_{2}}\right) \hat{\nu}^{t} \sum_{j=1}^{\omega-1} \sum_{i=1}^{j} \mathbb{P}\left\{X_{t}=j, \Gamma_{i, t} \leq \gamma \mid E_{2}\right\}+\omega \hat{\nu}^{2 t} \\
& \leq O\left(\omega^{C_{2}+1}\right) \hat{\nu}^{t} \mathbb{P}\left\{\Gamma_{1, t}<\gamma \mid E_{2}\right\}+\omega \hat{\nu}^{2 t}
\end{aligned}
$$

where the two last lines use a union bound and the symmetry of all individuals in generation $t$.
Thus, it suffices to upper bound the probability of $\left[\Gamma_{1, t}<\gamma\right]$ conditioned on $E_{2}$. Let $\mathcal{A}_{t, \omega}$ be the set of rooted trees $T$ with width less than $\omega$, height exactly $t$ and such that the spine associated to each leaf has at most $\ell(t)$ ramifications. The trees in $\mathcal{A}_{t, \omega}$ are the candidates for $G W_{t}$ - the tree induced by the first $t$ generations of the process $\left(X_{r}^{(t)}\right)_{t \geq r \geq 0}$, conditioned on $E_{2}$. We will obtain an upper bound for the probability of $\left[\Gamma_{1, t}<\gamma\right]$ conditioned on $E_{2} \cap\left[G W_{t}=\mathrm{T}\right]$, uniform for all $T \in \mathcal{A}_{t, \omega}$, which will also be an upper bound of $\mathbb{P}\left\{\Gamma_{1, t}<\gamma \mid E_{2}\right\}$.

Let $y_{0}, \ldots, y_{t}$ be the individuals of the spine associated to $x_{1}$. If $r \in\{0, \ldots, h-1\}$ is not a ramification of $T$, we first sample $\tilde{\xi}^{(t-r)}$, the number of children of $y_{r}$ in $\left(X_{r}\right)_{r \geq 0}$. Then conditioned on $\tilde{\xi}^{(t-r)}$, we sample the mark of $y_{r}$. If $r \in\{0, \ldots, h-1\}$ is a ramification of $T$, or if $r \in\{h, \ldots, t\}$, we simply give $y_{r}$ the mark $M$. This procedure gives a stochastic lower bound of $\Gamma_{1, t}$.

Similar to $\tilde{\boldsymbol{\xi}}$ defined in (2.12), the distribution of $\tilde{\xi}^{(a)}$ is given by

$$
\mathbb{P}\left\{\tilde{\xi}^{(a)}=k\right\}=\mathbb{P}\left\{X_{1}=k \mid X_{1}^{(a)}=1\right\}=\frac{k s_{a-1}\left(1-s_{a-1}\right)^{k-1} \mathbb{P}\{\xi=k\}}{s_{a} \mathbb{P}\left\{\xi^{(a)}=1\right\}}=P\{\tilde{\xi}=k\}+O\left(\hat{\nu}^{a}\right)
$$



for $k \geq 1$. Therefore, we can couple $\tilde{\xi}(t), \ldots, \tilde{\xi}(t-(h+1))$ with $\tilde{\xi}_{1}, \ldots, \tilde{\xi}_{h}$, which are iid copies of $\tilde{\xi}$, such that $\mathbb{P}\left\{\tilde{\xi}(t-r) \neq \tilde{\xi}_{r+1}\right\}<\omega^{-1}$. Thus, the number of positions where the two sequences differ is stochastically bounded from above by a binomial random variable with parameters $(h, \omega^{-1})$.

Let $E_{3}$ be the event that $\left(\tilde{\xi}(t), \ldots, \tilde{\xi}(t-(h+1))\right)$ and $\left(\tilde{\xi}_{1}, \ldots, \tilde{\xi}_{h}\right)$ differ at at least $m(t):=t(\log t)^{-1 / 2}$ positions. It follows that

$$
\mathbb{P}\left\{E_{3}^{c}\right\} \leq(h / \omega)^{m(t)} \leq t^{-m(t)}=e^{-t \sqrt{\log t}}
$$

where we used that $h<t \leq \sqrt{\omega}$. Thus, we obtain

$$
\mathbb{P}\left\{\Gamma_{1, t}<\gamma \mid E_{2} \cap\left[G \mathcal{W}_{t} \cong \mathbb{T}\right]\right\} \leq \mathbb{P}\left\{\Gamma_{1, t}<\gamma \mid E_{2} \cap E_{3} \cap\left[G \mathcal{W}_{t} \cong \mathbb{T}\right]\right\}+e^{-t \sqrt{\log t}}
$$

Let $\left(\tilde{\zeta}_{r}\right)_{r \geq 0}$ be iid copies of $\tilde{\zeta}$ as defined in Subsection 2.2.2. Conditioning on $E_{2} \cap E_{3} \cap\left[G \mathcal{W}_{h} \cong \mathbb{T}\right]$, we have

$$
\Gamma_{1, t} \succeq M^{-\left(\ell(t)+t_{0}+m(t)\right)} \prod_{r=0}^{t-1}\left(\tilde{\zeta}_{r}\right)^{-1}
$$

where $\succeq$ denotes stochastical domination. It follows from Cramér's theorem (Theorem 2.4) that

$$
\begin{aligned}
& \mathbb{P}\left\{\Gamma_{1, t}<e^{-a \hat{H} t} \mid E_{2} \cap E_{3} \cap\left[G \mathcal{W}_{t} \cong \mathbb{T}\right]\right\} \leq \mathbb{P}\left\{\frac{\sum_{r=0}^{t-1} \log \tilde{\zeta}_{r}}{t}>a \hat{H}-\frac{\left(\ell(t)+t_{0}+m(t)\right) \log M}{t}\right\} \\
& \leq \exp \left\{-\left(\mathrm{I}\left(a \hat{H}-\frac{\left(\ell(t)+t_{0}+m(t)\right) \log M}{t}\right)+o(1)\right) t\right\} \\
& \quad=e^{-(\mathcal{I}(a \hat{H})+o(1)) t},
\end{aligned}
$$

where in the last step we use that $\ell(t), m(t), t_{0}=o(t), \hat{H} \leq a \hat{H}<\log M, \mathcal{I}(z)$ is continuous on $[\hat{H}, \infty)$, and $\mathcal{I}^{\prime}(z)<\infty$ for all $z>\hat{H}$. Putting this into (2.54), we have

$$
\mathbb{P}\left\{\Gamma_{1, t}<e^{-a \hat{H} t} \mid E_{2} \cap\left[G \mathcal{W}_{t} \cong \mathbb{T}\right]\right\} \leq e^{-(\mathcal{I}(a \hat{H})+o(1)) t}
$$

Theorem 2.6 follows by putting the above into (2.51).

# 2.5.5 A corollary 

The following corollary is convenient for our later use.
Corollary 2.9. Similarly as in (1.10), consider

$$
\phi(a):=\frac{1}{a}(|\log \hat{\nu}|+\mathcal{I}(a \hat{H}))
$$

and $a_{0}$ its minimum in $[0, \infty)$ which satisfies $a_{0} \geq 1$. For any $p \rightarrow 0$, let $\omega \in\left(|\log p|^{3}, e|\log p|^{1 / 3}\right)$ and $\gamma_{p}=e^{-\hat{H}|\log p| / \phi\left(a_{0}\right)}$. We have

$$
\mathbb{P}\left\{\mathcal{G}_{t \omega}^{c}\left(\gamma_{p}\right) \cap\left[t \omega<\infty\right]\right\} \leq p^{1+o(1)}
$$

In particular, if $p=n^{-\beta}$ for $\beta>0$, we have $\gamma_{p}=n^{-\beta \hat{H}} / \phi\left(a_{0}\right)$.



Proof. For $t \leq \log _{2} \omega$, we deterministically have

$$
M^{-t}>M^{-\log _{2} \omega}>\gamma^{p}
$$

which implies

$$
\mathbb{P}\left\{\mathcal{G}_{t}^{c}\left(\gamma^{p}\right)\right\}=\mathbb{P}\left\{\cup_{i=1}^{X_{t}}\left[\Gamma_{i, t} \leq \gamma^{p}\right]\right\} \leq \mathbb{P}\left\{\cup_{i=1}^{X_{t}}\left[M^{-t} \leq \gamma^{p}\right]\right\}=0
$$

Let $t^{*}=|\log p| /|\log \hat{\nu}|<\sqrt{\omega}$. Choose $t \in\left(\log _{2} \omega, t^{*}\right]$ and set $a:=\frac{|\log p|}{t \phi\left(a_{0}\right)}$. As $\phi\left(a_{0}\right) \leq \phi(1)=|\log \hat{\nu}|$, we have $a \geq 1$. It follows from Theorem 2.6 that

$$
\begin{aligned}
\mathbb{P}\left\{\mathcal{G}_{t}\left(\gamma^{p}\right)^{c} \cap\left[0<X_{t}<\omega\right]\right\} & =\mathbb{P}\left\{\mathcal{G}_{t}\left(e^{-a \hat{H}_{t}}\right)^{c} \cap\left[0<X_{t}<\omega\right]\right\} \\
& \leq \exp \left\{-(1+o(1)) \frac{|\log p| \phi(a)}{\phi\left(a_{0}\right)}\right\} \leq p^{1+o(1)}
\end{aligned}
$$

where the last step uses the fact that $a_{0}$ minimises $\phi(a)$ for all $a \geq 0$. Thus,

$$
\begin{aligned}
\mathbb{P}\left\{\mathcal{G}_{t \omega}^{c}\left(\gamma^{p}\right) \cap[t \omega<\infty]\right\} & =\sum_{t \geq 0} \mathbb{P}\left\{\mathcal{G}_{t \omega}\left(\gamma^{p}\right)^{c}, t \omega=t+1\right\} \\
& \leq \sum_{t=\log _{2} \omega}^{t^{*}} \mathbb{P}\left\{\mathcal{G}_{t}\left(\gamma^{p}\right)^{c}, 0<X_{t}<\omega\right\}+\mathbb{P}\left\{t \omega \geq t^{*}\right\} \\
& \leq t^{*} p^{1+o(1)}+C_{0} \hat{\nu}^{t^{*}-2 t_{0}} \\
& \leq p^{1+o(1)}
\end{aligned}
$$

where we used Lemma 2.7 to bound $\mathbb{P}\left\{t \omega>t^{*}\right\}$, as in (2.50).

# 2.6 A truncated Martingale 

We will consider a truncated version of $\Gamma_{t}$ defined in (2.4). Fix $t_{0} \in \mathbb{N}$ and $\gamma>0$. Recall that $\left(f_{t-t_{0}}(i, t), t_{0}\right)$ is the ancestor of the node $(i, t)$ in generation $t_{0}$. For every $t \geq t_{0}$ and $i \in\left[X_{t}\right]$, define

$$
\hat{\Gamma}_{i, t}:=\gamma\left(\prod_{r=1}^{t-t_{0}} \zeta_{f_{t-r}(i, t), r}\right)^{-1}, \quad \hat{\Gamma}_{t}:=\sum_{i \in\left[X_{t}\right]} \hat{\Gamma}_{i, t}
$$

Given that $\Gamma_{i, t_{0}} \geq \gamma$ for all $i \in\left[X_{t_{0}}\right]$, we have $\hat{\Gamma}_{i, t} \leq \Gamma_{i, t}$ for $i \in\left[X_{t}\right]$ and $\hat{\Gamma}_{t} \leq \Gamma_{t}$. By the same argument of Lemma $2.1,\left(\hat{\Gamma}_{t} \mathbb{E}[\xi / \zeta]-\left(t-t_{0}\right)\right)_{t \geq t_{0}}$ is also a martingale with respect to $\left(\mathcal{F}_{r}\right)_{r \geq t_{0}}$.
Proposition 2.10. Let $\left(X_{r}\right)_{r \geq 0}$ be a marked branching process with offspring distribution $\eta=(\xi, \zeta)$ with $\mathbb{E}[\xi] \in(1, \infty)$. Let $M \in \mathbb{N}$ and $\omega \rightarrow \infty$. Suppose that $|\mathbb{E}[\xi / \zeta]-1| \leq M \omega^{-1 / 3}, \xi \leq M$ and $\zeta \geq 1$. There exists a constant $c_{0}>0$ such that for any $t_{0} \in \mathbb{N}$ and $t \in\left[t_{0}, \omega^{1 / 4}\right]$ we have

$$
\mathbb{P}\left\{\hat{\Gamma}_{t} \geq \omega \gamma / 2 \mid\left[X_{t_{0}} \geq \omega\right] \cap \mathcal{G}_{t_{0}}(\gamma)\right\} \geq 1-e^{-c_{0} \omega^{1 / 3}}
$$

Proof. Consider the collection of events

$$
E_{r}=\left[\hat{\Gamma}_{r} / \hat{\Gamma}_{t_{0}} \in(1 / 2,3 / 2)\right] \quad r \in\left(t_{0}, t\right]
$$



We will lower bound the probability of $\mathcal{E}_{r}$ using Azuma's inequality, see, e.g., [21, Chapter 11]. Note that, given $\mathcal{F}_{r}, m:=X_{r}$ and $\hat{\Gamma}_{r}$ are measurable. Let $T_{1}, T_{1}^{\prime}, \ldots, T_{m}, T_{m}^{\prime}$ be iid marked Galton-Watson trees with offspring $\eta$, where $T_{i}$ is rooted at node $i$ of generation $r$. Given $\mathcal{F}_{r}$, $\hat{\Gamma}_{r+1}=g\left(T_{1}, \ldots, T_{m}\right)$, where $g$ is a function depending on $\mathcal{F}_{r}$. Note that for every choice of $T_{1}, \ldots, T_{m}$ and $T_{i}^{\prime}$,

$$
\left|g\left(T_{1}, \ldots, T_{i}, \ldots T_{m}\right)-g\left(T_{1}, \ldots, T_{i}^{\prime}, \ldots, T_{m}\right)\right| \leq M \hat{\Gamma}_{i, r} \leq M \gamma
$$

since the tree $T_{i}$ contributes to $\hat{\Gamma}_{r+1}$ with at most

$$
\sum_{j \geq 0} \mathbf{1}_{f(j, r+1)=i} \hat{\Gamma}_{j, r+1}=\hat{\Gamma}_{i, r} \sum_{j \geq 0} \mathbf{1}_{f(j, r+1)=i} \zeta_{j, r+1} \leq \xi_{i, r} \hat{\Gamma}_{i, r} \leq M \hat{\Gamma}_{i, r}
$$

Since $\left(\hat{\Gamma}_{t} \mathbb{E}[\xi / \zeta]-\left(t-t_{0}\right)\right)_{t \geq t_{0}}$ is a martingale, we have

$$
\left|\hat{\Gamma}_{r}-\mathbb{E}\left[\hat{\Gamma}_{r+1} \mid \mathcal{F}_{r}\right]\right|=\left|\hat{\Gamma}_{r}-\mathbb{E}[\xi / \zeta] \hat{\Gamma}_{r}\right| \leq M \omega^{-1 / 3} \hat{\Gamma}_{r}=: s
$$

Thus, it follows from (2.68) and Azuma's inequality [21, pp. 92] that

$$
\begin{aligned}
\mathbb{P}\left\{\left|\hat{\Gamma}_{r+1}-\hat{\Gamma}_{r}\right|>2 s \mid \mathcal{F}_{r}\right\} & \leq \mathbb{P}\left\{\left|\hat{\Gamma}_{r+1}-\mathbb{E}\left[\hat{\Gamma}_{r+1} \mid \mathcal{F}_{r}\right]\right|>s \mid \mathcal{F}_{r}\right\}+\mathbb{P}\left\{\left|\hat{\Gamma}_{r}-\mathbb{E}\left[\hat{\Gamma}_{r+1} \mid \mathcal{F}_{r}\right]\right|>s \mid \mathcal{F}_{r}\right\} \\
& \leq 2 \exp \left\{-\frac{2 s^{2}}{\sum_{i=1}^{m}\left(M \hat{\Gamma}_{i, r}\right)^{2}}\right\}
\end{aligned}
$$

Note that $\sum_{i=1}^{m}\left(M \hat{\Gamma}_{i, r}\right)^{2} \leq \gamma M^{2} \sum_{i=1}^{m} \hat{\Gamma}_{i, r}=\gamma M^{2} \hat{\Gamma}_{r}$. Thus on the event $\mathcal{E}_{r}$, we have

$$
\mathbb{P}\left\{\left|\hat{\Gamma}_{r+1}-\hat{\Gamma}_{r}\right|>2 s \mid \mathcal{F}_{r}\right\} \leq 2 \exp \left\{-\frac{2 \omega^{-2 / 3} \hat{\Gamma}_{r}^{2}}{\gamma \hat{\Gamma}_{r}}\right\} \leq 2 e^{-\omega^{1 / 3}}
$$

where we used $\hat{\Gamma}_{r} \geq \omega \gamma / 2$ on $\mathcal{E}_{r}$ in the last inequality.
Let $\mathcal{F}_{t_{0}}=\left[X_{t_{0}} \geq \omega\right] \cap \mathcal{G}_{t_{0}}(\gamma)$ and let $\mathcal{F}_{r}=\left[\left|\hat{\Gamma}_{r+1} / \hat{\Gamma}_{r}-1\right| \leq M \omega^{-1 / 3}\right]$ for $r>t_{0}$. By (2.70), there exists $c_{0}>0$ such that

$$
\mathbb{P}\left\{\cup_{r=t_{0}+1}^{t} \mathcal{F}_{r}^{c} \mid \mathcal{F}_{t_{0}}\right\} \leq \sum_{r=t_{0}}^{t-1} \mathbb{P}\left\{\mathcal{F}_{r+1}^{c} \mid \cap_{t_{0} \leq s \leq r} \mathcal{F}_{s}\right\} \leq 2 t e^{-\omega^{1 / 3}} \leq e^{-c_{0} \omega^{1 / 3}}
$$

where we used $\cap_{s \leq r} \mathcal{F}_{s} \subseteq \mathcal{E}_{r}$. So, with the desired probability

$$
\hat{\Gamma}_{t} \geq\left(1-2 M \omega^{-1 / 3}\right) t \omega \gamma \geq \frac{\omega \gamma}{2}
$$

as $t \leq \omega^{1 / 4}$.



# 3 Exploring the graph 

### 3.1 The exploration process

In this section, we introduce a marked version of the Breadth First (BFS) Search graph exploration process defined in [9].

For a set of vertices $I \subseteq[n]$, let $E^{ \pm}(I)$ be the tails/heads incident to $I$. Let $E^{ \pm}$be the set of all tails/heads. For a set of half-edges $X$, let $V(X)$ be the vertices incident to $X$. For $e^{ \pm} \in E^{ \pm}$, we use $v\left(e^{ \pm}\right)$to denote the vertex incident to $e^{ \pm}$.

We start from an arbitrary head $e_{0}^{-} \in E^{-}$. In this process, we create random pairings of halfedges one by one and keep each half-edge in exactly one of the three states - active, paired, or undiscovered. Let $A_{i}^{ \pm}, P_{i}^{ \pm}$and $U_{i}^{ \pm}$denote the set of tails/heads in the four states respectively after the $i$-th pairing of half-edges. Initially, let

$$
A_{0}^{-}=\left\{e_{0}^{-}\right\}, A_{0}^{+}=E^{-}\left(v\left(e_{0}^{-}\right)\right), P_{0}^{ \pm}=\emptyset, U_{0}^{ \pm}=E^{ \pm} \backslash\left(A_{0}^{ \pm} \cup P_{0}^{ \pm}\right)
$$

Then set $i=1$ and proceed as follows:
(i) Let $e_{i}^{-}$be one of the heads which became active earliest in $A_{i-1}^{-}$.
(ii) Pair $e_{i}^{-}$with a tail $e_{i}^{+}$chosen uniformly at random from $E^{+} \backslash P_{i-1}^{+}$. Let $v_{i}=v\left(e_{i}^{-}\right)$and $P_{i}^{ \pm}=P_{i-1}^{ \pm} \cup\left\{e_{i}^{ \pm}\right\}$
(iii) If $e_{i}^{+} \in A_{i-1}^{+}$, then $A_{i}^{ \pm}=A_{i-1}^{ \pm} \backslash\left\{e_{i}^{ \pm}\right\}$; and if $e_{i}^{+} \in U_{i-1}^{+}$, then $A_{i}^{ \pm}=\left(A_{i-1}^{ \pm} \cup E^{ \pm}\left(v_{i}\right)\right) \backslash\left\{e_{i}^{ \pm}\right\}$
(iv) If $A_{i}^{-}=\emptyset$, terminate; otherwise, let $U_{i}^{ \pm}=E^{ \pm} \backslash\left(A_{i}^{ \pm} \cup P_{i}^{ \pm}\right), i=i+1$ and go to (i).

If we are in the first case of step (iii), we say that a collision has happened. If there is no collision in the process up to certain time, the exposed in-neighbourhood of $e_{0}^{-}$is a tree.

In parallel to the BFS process, we construct a tree with nodes corresponding to heads in the in-neighbourhood of $e_{0}^{-}$. Write $f=e_{0}^{-}$and let $T_{f}^{-}(0)$ be a tree with one root node corresponding to $f$. Given $T_{f}^{-}(i-1), T_{f}^{-}(i)$ is constructed as follows: if $e_{i}^{-} \in U_{i-1}^{-}$, then construct $T_{f}^{-}(i)$ from $T_{f}^{-}(i-1)$ by adding $\left|E^{-}\left(v_{i}\right)\right|$ child nodes to the node representing $e_{i}^{-}$, each of which representing a head in $E^{-}\left(v_{i}\right)$; otherwise, let $T_{f}^{-}(i)=T_{f}^{-}(i-1)$. See Figure 1 for an example the exploration process and its associated tree.

The nodes in $T_{f}^{-}(i)$ correspond to the heads in $P_{i}^{-} \cup A_{i}^{-}$. So we can assign a label paired or active to each node of $T_{f}^{-}(i)$. We also give the node corresponding to $e_{i}^{-}$a mark which equals the out-degree of $v_{i}$.

For half-edges $e_{1}$ and $e_{2}$, we define the distance from $e_{1}$ to $e_{2}$, denoted by $\operatorname{dist}\left(e_{1}, e_{2}\right)$, to be the length of the shortest path from $v\left(e_{1}\right)$ to $v\left(e_{2}\right)$ which starts with the edge containing $e_{1}$ if $e_{1}$ is a tail, and which ends with the edge containing $e_{2}$ if $e_{2}$ is head. For example, in Figure 1, $\operatorname{dist}\left(e_{2}^{-}, e_{1}^{-}\right)=\operatorname{dist}\left(e_{2}^{+}, e_{1}^{+}\right)=1, \operatorname{dist}\left(e_{3}^{+}, e_{1}^{-}\right)=2, \operatorname{dist}\left(e_{4}^{-}, e_{1}^{+}\right)=0$.

If it is the last step where a head at distance $t$ from $f$ is paired, then $T_{f}^{-}(i t)$ satisfies: (i) the height is $t$; and (ii) the set of actives nodes is the $t$-th level. We call a rooted tree $T$ incomplete if it satisfies (i)-(ii). We let $p(T)$ be the number of paired nodes in $T$.





Figure 1: An ongoing exploration process and its incomplete tree

# 3.2 Coupling the exploration and branching processes 

We will couple a marked incomplete tree constructed from the exploration process with a marked branching process with offspring distribution close to $\eta=(\xi, \zeta) \stackrel{\mathrm{L}}{=} \mathcal{D}_{\text {out }}$. Thus we assume that $\xi \leq M, 2 \leq \zeta \leq M$ and $\mathbb{E}[\xi / \zeta]=1$ (by Remark 2.2).

We will need two slightly perturbed versions of $\eta$. Let $\beta \in(0,1 / 4)$. Consider the probability distribution $\eta_{\downarrow}=\eta_{\downarrow}(\beta)=\left(\xi_{\downarrow}(\beta), \zeta_{\downarrow}(\beta)\right)$ defined by

$$
\mathbb{P}\left\{\eta_{\downarrow}=(k, \ell)\right\}:= \begin{cases}c_{\downarrow} \mathbb{P}\{\eta=(k, \ell)\} & \text { if } \mathbb{P}\{\eta=(k, \ell)\} \geq n^{-1+\beta} \\ 0 & \text { otherwise }\end{cases}
$$

where $c_{\downarrow}$ is a normalising constant. It is easy to check that our assumptions on $\eta$ implies that $c_{\downarrow}=1+O\left(n^{-1+\beta}\right)$.

Similarly, the probability distribution $\eta^{\uparrow}=\eta^{\uparrow}(\beta)$ is defined by

$$
\mathbb{P}\left\{\eta^{\uparrow}=(k, \ell)\right\}:= \begin{cases}c^{\uparrow} \mathbb{P}\{\eta=(k, \ell)\} & k \geq 1 \\ c^{\uparrow} \mathbb{P}\{\eta=(0, \ell)\}+n^{-1+\beta} & k=0\end{cases}
$$

where $c^{\uparrow}=1-O\left(n^{-1+\beta}\right)$ is a normalising constant.
One can show that $\eta_{\downarrow}$ satisfies $\left|\mathbb{E}\left[\xi_{\downarrow} / \zeta_{\downarrow}\right]-1\right|=O\left(n^{-1+\beta}\right)$ and similarly for $\eta^{\uparrow}$. If $\omega$ is polylogarithmic, it will be possible to apply Proposition 2.10 to them.

Let $\mathrm{GW}_{\eta}$ be a marked Galton-Watson tree with offspring distribution $\eta$. For a marked incomplete rooted tree $T$, we use the notation $\mathrm{GW}_{\eta} \cong T$ to denote that $T$ is isomorphic to a



root subtree of $\mathcal{G} \mathcal{W}^{\eta}$, i.e., the degree of paired nodes of $T$ agrees with $\mathcal{G} \mathcal{W}^{\eta}$, and all marks in $T$ and $\mathcal{G} \mathcal{W}^{\eta}$ agree. For a set of incomplete trees $\mathcal{T}$, let $\left[\mathcal{G} \mathcal{W}_{\eta} \in \mathcal{T}\right]:=\cup_{T \in \mathcal{T}}\left[\mathcal{G} \mathcal{W}^{\eta} \cong T\right]$ and [ $\left.T_{f}^{-} \in \mathcal{T}\right]:=\cup_{T \in \mathcal{T}}\left[T_{f}(p(T))=T\right]$. The following is a simple adaption of [9, Lemma 5.3]; we omit the proof.

Lemma 3.1. Let $\beta \in(0,1 / 4)$ and let $\mathcal{T}$ be a set of incomplete trees such that $p(T)<\frac{n^{\beta}}{10}$ for all $T \in \mathcal{T}$. We have

$$
(1+o(1)) P\left\{\mathcal{G W}{ }^{\eta_{\downarrow}}(\beta) \in \mathcal{T}\right\} \leq P\left\{\mathcal{T}_{f}^{-} \in \mathcal{T}\right\} \leq(1+o(1)) P\left\{\mathcal{G W}^{\eta_{0}}(\beta) \in \mathcal{T}\right\} .
$$

# 4 Stationary distributions 

We will proceed to prove Theorem 1.1 as in [11], using the results obtained in previous sections.

### 4.1 The largest strongly connected component

For $\delta^{+} \geq 2$, whp there is a linear size scc in $\overrightarrow{G_{n}}[8,14]$. We will first show that whp this component is attractive (and so unique), which implies that the simple random walk on $\vec{G}_{n}$ has a unique stationary distribution whp.

For a tail/head $e^{ \pm} \in E^{ \pm}$, let $N_{k}^{ \pm}\left(e^{ \pm}\right)$and $N_{\leq k}^{ \pm}\left(e^{ \pm}\right)$be the sets of tails/heads at distance $k$ and at most $k$ from $/ \mathrm{to} e^{ \pm}$, respectively; that is,

$$
\begin{aligned}
N_{k}^{+}\left(e^{+}\right) & :=\left\{f^{+} \in E^{+}: \operatorname{dist}\left(e^{+}, f^{+}\right)=k\right\} \\
N_{k}^{-}\left(e^{-}\right) & :=\left\{f^{-} \in E^{-}: \operatorname{dist}\left(f^{-}, e^{-}\right)=k\right\} \\
N_{\leq k}^{+}\left(e^{+}\right) & :=\left\{f^{+} \in E^{+}: \operatorname{dist}\left(e^{+}, f^{+}\right) \leq k\right\} \\
N_{\leq k}^{-}\left(e^{-}\right) & :=\left\{f^{-} \in E^{-}: \operatorname{dist}\left(f^{-}, e^{-}\right) \leq k\right\}
\end{aligned}
$$

Similarly, for a vertex $x \in[n]$, let $N_{k}^{ \pm}(x)$ and $N_{\leq k}^{ \pm}(x)$ be the sets of vertices at distance $k$ and at most $k$ from/to $x$, respectively.

Throughout this section, let $\omega:=\log _{6} n$. For every tail/head $e^{ \pm} \in E^{ \pm}$, consider the stopping time

$$
t_{\omega}^{ \pm}\left(e^{ \pm}\right):=\inf \left\{t \geq 0:\left|N_{t}^{ \pm}\left(e^{ \pm}\right)\right| \geq \omega\right\}
$$

that is, the first time when there are at least $\omega$ half-edges in the in/out-edge neighbourhood of $e^{ \pm}$.
Proposition 4.1. Let $M \in \mathbb{N}$ and suppose that $\delta^{+} \geq 2$ and $\Delta^{ \pm} \leq M$. Let $C_{0}$ denote the largest strongly connected component in $\vec{G}_{n}$. Let

$$
E_{0}^{-}:=\left\{f \in E^{-}: t_{\omega}^{-}(f)<\infty\right\}
$$

Let $E_{0}$ be the event that $C_{0}$ is attractive (i.e., there is a directed path from every vertex to $C_{0}$ ) and has vertex set $V_{0}=V\left(E_{0}^{-}\right)$. Then $P\left\{E_{0}\right\}=1-o(1)$. Thus, whp the simple random walk on $\vec{G}_{n}$ has a unique stationary distribution supported on $V_{0}$.

Proof. Let $h:=1+\log _{2}(2 \omega)=O(\log \log n)$. Then, it follows from [11, Lemma 2.2] that

$$
\begin{aligned}
P\left\{\cup_{e \in E^{+}}\left[t_{\omega}^{+}(e)>h\right]\right\} & \leq P\left\{\cup_{y \in[n]}\left[\left|N_{h-1}^{+}(y)\right| \leq \omega\right]\right\} \\
& \leq P\left\{\cup_{y \in[n]}\left[\left|N_{h-1}^{+}(y)\right| \leq \frac{1}{2}\left(\delta^{+}\right)^{h-1}\right]\right\}=o\left(n^{-1}\right)
\end{aligned}
$$



where we used $\left|N_{h}^{+}(e)\right| \geq\left|N_{h-1}^{+}(y)\right|$, where $y$ is the vertex incident to the head paired with $e$. So, $t_{\omega}^{+}(e) \leq h$ for all $e \in E^{+}$whp.

By $[9$, Lemma 6.2 $]^{2}$, whp every $f \in E^{-}$either has $t_{\omega}^{-}(f)=\infty$ or $t_{\omega}^{-}(f)=O(\log n)$. Conditioning on $t_{\omega}^{+}(e)=O(\log \log n)$ and $t_{\omega}^{-}(f)=O(\log n)$, [9, Proposition 7.2] implies that there is a path from $e$ to $f$ with probability $1-o\left(n^{-2}\right)$. Thus, by a union bound over all choices of $e$ and $f$, we have that whp there is path from every $e \in E^{+}$to every $f \in E_{0}^{-}$; and in particular, from every $x \in[n]$ to every $y \in V_{0}$. In other words, whp $\mathcal{C}_{0}$ contains all vertices in $V_{0}$ and is attractive. Using [9, Proposition 6.1] with $t=n / \omega$, it is easy to check that whp for every $x \in[n] \backslash V_{0}$, there exists $y_{x}$ such that $y_{x}$ cannot be reached from $x$. It follows that whp $\mathcal{C}_{0}$ has vertex set $V_{0}$.

# 4.2 Random walk on heads 

It will be more convenient to perform a random walk on heads instead of vertices. Consider the random process $\left(Z_{t}^{e}\right)_{t \geq 0}$ with state space $E^{-}$and, conditioning on $\vec{G}_{n}$ and $Z_{t}^{e}=f$, let $Z_{t+1}^{e}$ be chosen uniformly at random among all heads paired with tails of $v(f)$, the endpoint of $f$. It follows from Proposition 4.1 that $\left(Z_{t}^{e}\right)_{t \geq 0}$ has a unique stationary distribution supported on $E_{0}^{-}$, denoted by $\pi^{e}$.

Recall that $\pi_{\min }:=\min \{\pi(y): y \in[n], \pi(y)>0\}$. Let $\pi_{\min }^{e}:=\min \left\{\pi^{e}(f): f \in E^{-}, \pi^{e}(f)>0\right\}$. Define

$$
\pi_{0}:=\min \left\{\pi(y): y \in V_{0}\right\}, \quad \pi_{0}^{e}:=\min \left\{\pi^{e}(f): f \in E_{0}^{-}\right\}
$$

By Proposition 4.1, whp $\pi_{0}=\pi_{\min }$ and $\pi_{0}^{e}=\pi_{\min }^{e}$. The following lemma shows that whp $\pi_{0}^{e}$ differs from $\pi_{0}$ by a bounded factor. Thus, it suffices to prove Theorem 1.1 for $\pi_{0}^{e}$.

Lemma 4.2. Whp $\pi_{0}^{e} \leq \pi_{0} \leq M \pi_{0}^{e}$.
Proof. Let $V_{0}$ and $E_{0}^{-}$and $E_{0}$ be as in Proposition 4.1. Since $E_{0}$ happens whp, it suffices to prove the lemma conditioning on $\vec{G}_{n} \in E_{0}$. So we may assume there is a unique stationary distribution $\pi$ and $\lim _{t \rightarrow \infty} \mathbb{P}\left\{Z_{t}=y \mid Z_{0}=x\right\}=\pi(y)$, uniformly for all $x \in[n]$.

Let $x, y \in[n]$ and $e \in E^{-}(x)$. Note that $\left(v\left(Z_{t}^{e}\right)\right)_{t \geq 0}$ (i.e., the endpoints of the heads in $\left(Z_{t}^{e}\right)_{t \geq 0}$ ) is itself a simple random walk on $[n]$ with the same law as $\left(Z_{t}\right)_{t \geq 0}$. Therefore,

$$
\mathbb{P}\left\{Z_{t}=y \mid Z_{0}=x\right\}=\sum_{f \in E^{-}(y)} \mathbb{P}\left\{Z_{t}^{e}=f \mid Z_{0}^{e}=e\right\}
$$

If $y \in V_{0}$, then letting $t \rightarrow \infty$ on both sides we have

$$
\pi(y)=\sum_{f \in E^{-}(y)} \pi^{e}(f) \geq \pi_{0}^{e}
$$

where the last inequality holds by definition of $V_{0}$. Since $y \in V_{0}$ is arbitrary, it follows that $\pi_{0} \geq \pi_{0}^{e}$.
For the other direction, let $f_{0} \in E_{0}^{-}$with $\pi^{e}\left(f_{0}\right)=\pi_{0}^{e}$. Let $z$ be the vertex that has a tail paired with $f_{0}$. Let $e \in E^{-}$and let $x$ be its endpoint. We have

$$
\mathbb{P}\left\{Z_{t}^{e}=f_{0} \mid Z_{0}^{e}=e\right\}=\frac{1}{d_{z}^{+}} \mathbb{P}\left\{Z_{t-1}=z \mid Z_{0}=x\right\}
$$

[^0]
[^0]:    ${ }^{2}$ In this paper we will use previous works on the directed configuration model $[8,9]$. Let $D_{n}=D$ be the in- and out-degrees of a uniform random vertex. In $[8,9]$, we assumed that $D_{n}$ converges in distribution as $n \rightarrow \infty$. This is not the case in this paper. However, for each subsequence $\left(a_{n}\right)_{n \geq 1}$ of $(n)_{n \geq 1}$, we can take a further subsequence $\left(a b(n)\right)_{n \geq 1}$ such that $D_{a b(n)}$ converges in distribution. Thus we can still apply previous results to prove an event happens with high probability.



Again, letting $t \rightarrow \infty$ we have

$$
\pi_{0}^{\mathrm{e}}=\pi^{\mathrm{e}}(f)=\frac{1}{d_{z}^{+}} \pi(z) \geq \frac{1}{M} \pi_{0}
$$

Thus $\pi_{0} \leq M \pi_{0}^{\mathrm{e}}$.

# 4.3 Lower bound for $\boldsymbol{\pi}_{\mathbf{0}}^{\mathrm{e}}$ 

To prove a lower bound it will suffice to lower bound the probability of reaching a particular head $f \in E_{0}^{-}$, uniformly for all starting points $e \in E^{-}$(see Lemma 4.4 below). We use the ideas introduced in $[5,6,11]$ to capture the weight (defined below) of typical trajectories departing from $e$. Our main contribution is to control the total weight of the trajectories landing at $f$.

Define the out-entropy $H^{+}$and the entropic time $\tau^{\text {ent }}$ as

$$
H^{+}:=\frac{1}{m} \sum_{x \in[n]} d_{x}^{-} \log d_{x}^{+}, \quad \tau^{\mathrm{ent}}:=\frac{\log n}{H^{+}}
$$

We stress the similarity between $H^{+}$and $\hat{H}^{-}$defined in (1.8); in particular, $H^{+}=\mathbb{E}\left[\log D_{\mathrm{in}}^{\oplus}\right]$. In this case, the out-entropy is related to typical trajectories in supercritical out-growth.

Fix $\varepsilon>0$ small enough and let

$$
h^{+}:=(1-\varepsilon) \tau^{\mathrm{ent}}, \quad h^{-}:=\frac{3 \varepsilon}{\log \delta^{+}} \log n
$$

For $f \in E^{-}$, let $h(f):=t_{\omega}^{-}(f) \wedge \omega$. Define

$$
\tau(f):=h^{+}+h^{-}+h(f)
$$

Remark 4.3. Bordenave, Caputo and Salez [5, 6] showed that the mixing time of the random walk on $\vec{G}_{n}$ coincides with the entropic time, and exhibits cutoff. Jensen's inequality implies that $\tau^{\text {ent }} \geq(1+o(1)) \log _{\nu} n$, which is whp the typical distance of the $\vec{G}_{n}$. Thus, by the results in [9], $\tau^{\text {ent }}+\max _{f \in E_{0}^{-}} h(f)$ is at least the diameter of $\vec{G}_{n}$ whp.

Throughout this section, we will use letters $a, b$ for tails in $E^{+}$and letters $e, f, g$ for heads in $E^{-}$. Let $\mathbb{P}^{t}(e, f):=\mathbb{P}\left\{Z_{t}^{e}=f \mid Z_{0}^{e}=e\right\}$. To lower bound $\pi_{0}^{\mathrm{e}}$ it suffices to prove the following:

Lemma 4.4. With high probability, for all $e \in E^{-}$and $f \in E_{0}^{-}$,

$$
\mathbb{P}^{\tau(f)}(e, f) \geq n^{-\left(1+\hat{H}^{-} / \phi\left(a_{0}\right)+o(1)\right)}
$$

Let $f \in E_{0}^{-}$. Assuming Lemma 4.4 and by stationarity at time $\tau(f)$, whp we have

$$
\pi^{\mathrm{e}}(f)=\sum_{e \in E^{-}} \pi^{e}(e) \mathbb{P}^{\tau(f)}(e, f) \geq n^{-\left(1+\hat{H}^{-} / \phi\left(a_{0}\right)+o(1)\right)}
$$

and the lower bound in Theorem 1.1 follows.



From now on, we fix two heads $e, f \in E^{-}$. We sequentially expose the out-neighbourhood of $e$ (out-phase) and the in-neighbourhood of $f$ (in-phase) as explained below. To control the trajectories departing from $e$, we will use the idea of nice paths introduced in [5] as described in $[11]$.

In the out-phase we build a directed rooted tree $T_{e}^{+}$, partially exposing the out-neighbourhood of $e$. The root of $T_{e}^{+}$represents the head $e$ and all its other nodes represent tails in $E^{+}$. For a tail $a$ represented in $T_{e}^{+}$, let $h(a)$ denote its height in $T_{e}^{+}$. Define its weight by

$$
w(a):=\prod_{i=1}^{h(a)} \frac{1}{d_{v\left(a_{i}\right)}^{+}}
$$

where $a_{1}, \ldots, a_{h(a)}$ are the tails in the path from $e$ to $a$ in $T_{e}^{+}$.
To construct $T_{e}^{+}$we use a procedure similar to Subsection 3.1 , with the roles of heads and tails reversed and the following two modifications:
a) There is no initial tail $e_{0}^{+}$. We start at step $i=2$ with $e_{1}^{-}=e$ and $A_{1}^{ \pm}=E^{ \pm}(v(e))$.
b) For each $i \geq 2$, at step $(i)$ we choose $e_{i}^{+}$to be the tail in $A_{i-1}^{+}$that maximises $w(a)$ among all $a \in A_{i-1}^{+}$with $h(a) \leq h^{+}-1$ and $w(a) \geq n^{-1+\varepsilon^{2}}$.

The tree $T_{e}^{+}$is not constructed in a BFS manner, but maximising the weight of its nodes; by construction, $h(a) \geq \operatorname{dist}(e, a)+1$. The process stops if there are no more eligible tails. In [6, Lemma 7], the authors showed that deterministically constructing $T_{e}^{+}$exposes at most $\kappa^{+}:=n^{1-\varepsilon^{2} / 2}$ edges. Moreover, if $g$ is the head paired with a tail $a$ represented in $T_{e}^{+}$, we have $P^{h(a)}(e, g) \geq w(a)$.

In the in-phase we build a directed rooted tree $T_{f}^{-}$, exposing $N_{\leq h^{-}+h(f)}^{-}(f)$ conditionally on $T_{e}^{+}$. We use the exploration process defined in Subsection 3.1 with one modification: if $e_{i}^{-}$has already been paired with $e^{+}$in the exploration of $T_{e}^{+}$, we let $e_{i}^{+}=e^{+}$. We stop once all heads at distance $h^{-}+h(f)$ from $f$ have been activated and we let $T_{f}^{-}$be the final incomplete tree generated up to this point. For a head $g$ in $T_{f}^{-}$, let $h(g)$ be its height in $T_{f}^{-}$; note that $h(g)=\operatorname{dist}(g, f)$. We define its weight by

$$
w(g):=\prod_{i=1}^{h(g)} \frac{1}{d_{v\left(g_{i}\right)}^{+}}
$$

where $f=g_{0}, \ldots, g_{h(g)}$ are the heads in the path from $f$ to $g$ in $T_{f}^{-}$. Similarly as before, $P^{h(g)}(g, f) \geq w(g)$. For any $\alpha>0$, we can let $\varepsilon$ be small enough such that the number of edges exposed in the in-phase is at most

$$
\kappa^{-}:=\omega_{h(f)}+\omega\left(\Delta^{-}\right)^{h^{-}+1}=n^{3\left(\log \Delta^{-} / \log \delta^{+}\right) \varepsilon+o(1)}=\mathcal{O}\left(n^{\alpha}\right)
$$

Let $\sigma_{0}$ be a partial realisation of the configuration model in which at most $\kappa:=\kappa^{+}+\kappa^{-}$edges are revealed during the out- and in-phases. Then $T_{e}^{+}, T_{f}^{-}, h(f), N_{\leq h^{-}+h(f)}^{-}(f)$ and $\tau(f)$ are all measurable with respect to $\sigma_{0}$. Given $\sigma_{0}$, let $\mathcal{A}$ be the set of unpaired tails $a$ in $T_{e}^{+}$with $h(a)=h^{+}$, and let $\mathcal{G}$ be the set of unpaired heads $g$ in $T_{f}^{-}$which satisfy $h(g)=h^{-}+h(y)$.

To bound the desired probability, it will be convenient to consider trajectories that are not too heavy. Following the ideas in $[5,6,11]$, it suffices to only consider nice paths in $T_{f}^{-}$; that is, we restrict our attention to the tails $a \in \mathcal{A}$ such that $w(a) \leq n^{-1+2 \epsilon}$. For heads in $\mathcal{G}$, we define a



truncated version of the weight. Let $\gamma:=n^{-(1+\varepsilon)} \hat{H}_{-} / \varphi\left(a_{0}\right)$. For $g$ in $T_{f}^{-}$with $h(g) \geq h(f)$, we define its truncated weight by

$$
\hat{w}(g):=(w(\hat{g}) \wedge \gamma) \prod_{i=h(f)+1}^{h(g)} \frac{1}{d_{v\left(g_{i}\right)}^{+}}
$$

where $\hat{g}$ is the unique head in the path from $f$ to $g$ in $T_{f}^{-}$with $h(\hat{g})=h(f)$. In words, the truncated weight caps the contribution of the last $h(f)$ steps by $\gamma$. By the definition of $h_{-}$in (4.11), for $g \in \mathcal{G}$ we have

$$
\hat{w}(g) \leq\left(\delta_{+}\right)^{-h_{-}} \gamma \leq \gamma n^{-3 \varepsilon}
$$

Let $\sigma$ be a complete pairing of half-edges compatible with $\sigma_{0}$. Let $[\sigma(a)=g]$ be the event that $a$ and $g$ are paired in $\sigma$. Conditioning on $\sigma_{0}$, we can lower bound the desired probability as follows

$$
\mathbb{P}^{\tau(f)}(e, f) \geq \hat{\mathbb{P}}^{\tau(f)}(e, f):=\sum_{a \in \mathcal{A}} \sum_{g \in \mathcal{G}} w(a) \hat{w}(g) \mathbb{1}_{\sigma(a)=g} \mathbb{1}_{w(a) \leq n^{-1+2 \varepsilon}}
$$

Let

$$
A_{e, f}\left(\sigma_{0}\right):=\sum_{a \in \mathcal{A}} w(a) \mathbb{1}_{w(a) \leq n^{-1+2 \varepsilon}}, \quad B_{e, f}\left(\sigma_{0}\right):=\sum_{g \in \mathcal{G}} \hat{w}(g)
$$

which are measurable with respect to $\sigma_{0}$. Consider the event

$$
\mathcal{Y}_{e, f}=\left\{\sigma_{0}: A_{e, f}\left(\sigma_{0}\right) \geq \frac{1}{2}, B_{e, f}\left(\sigma_{0}\right) \geq \frac{\omega \gamma}{4}\right\}
$$

If $\sigma_{0} \in \mathcal{Y}_{e, f}$, then

$$
\mathbb{E}\left[\hat{\mathbb{P}}^{\tau(f)}(e, f) \mid \sigma_{0}\right] \geq \frac{1}{m} A_{e, f}\left(\sigma_{0}\right) B_{e, f}\left(\sigma_{0}\right) \geq \frac{\omega \gamma}{8 m} \geq \frac{2 \gamma}{n}
$$

We prove a concentration result similar to [11, Lemma 3.6], exploiting the truncated nature of $\hat{\mathbb{P}}^{t}$ :
Lemma 4.5. For every $\sigma_{0} \in \mathcal{Y}_{e, f}$ and every $a \in(0,1)$

$$
\mathbb{P}\left\{\hat{\mathbb{P}}^{\tau(f)}(e, f) \leq(1-a) \mathbb{E}\left[\hat{\mathbb{P}}^{\tau(f)}(e, f) \mid \sigma_{0}\right] \mid \sigma_{0}\right\} \leq \exp \left(-\frac{a^{2} n \varepsilon}{3}\right)
$$

Proof. One can write $\hat{\mathbb{P}}^{\tau(f)}(e, f)=\sum_{a \in \mathcal{E}^{+}} c(a, \sigma(a))$ where

$$
c(a, g)=w(a) \hat{w}(g) \mathbb{1}_{w(a) \leq n^{2 \varepsilon-1}} \mathbb{1}_{a \in \mathcal{A}, g \in \mathcal{G}}
$$

By (4.19), it follows that

$$
\|c\|_{\infty}:=\max _{a \in \mathcal{E}^{+}} c(a, \sigma(a)) \leq \gamma n^{-(1+\varepsilon)}
$$

Using the one-sided version of Chatterjee's inequality for uniformly random pairings [12, Proposition 1.1], we get that

$$
\begin{aligned}
\mathbb{P}\left\{\hat{\mathbb{P}}^{\tau(f)}(e, f) \leq(1-a) \mathbb{E}\left[\hat{\mathbb{P}}^{\tau(f)}(e, f) \mid \sigma_{0}\right] \mid \sigma_{0}\right\} & \leq \exp \left(-\frac{a^{2} \mathbb{E}\left[\hat{\mathbb{P}}^{\tau(f)}(e, f) \mid \sigma_{0}\right]}{6\|c\|_{\infty}}\right) \leq e^{-\frac{a^{2} n \varepsilon}{3}} \\
\text {, where we used (4.23) in the last line. }
\end{aligned}
$$



Lemma 4.6. Define the event

$$
\mathcal{Y}=\bigcap_{e, f \in E^{-}}\left(\mathcal{Y}_{e, f} \cup\left[t_{\omega}^{-}(f)=\infty\right]\right)
$$

We have $\mathbb{P}\{\mathcal{Y}\}=1-o(1)$.
Let us prove Lemma 4.4 assuming that Lemma 4.6 holds. The proof of Lemma 4.6 is postponed to Subsection 4.4 .

Proof of Lemma 4.4. Write

$$
E_{f}:=\left[t_{\omega}^{-}(f)=\infty\right], \quad E_{e, f}:=\left[\hat{\mathbb{P}}^{\tau(f)}(e, f) \geq \frac{\gamma}{n}\right]
$$

Using (4.23) and applying Lemma 4.5 with $a=1 / 3$,

$$
\mathbb{P}\left\{E_{e, f}^{c} \mid \mathcal{Y}_{e, f}\right\}=\mathbb{P}\left\{\hat{\mathbb{P}}^{\tau(f)}(e, f)<\frac{\gamma}{n} \mid \mathcal{Y}_{e, f}\right\} \leq \max _{\sigma_{0} \in \mathcal{Y}_{e, f}} \mathbb{P}\left\{\hat{\mathbb{P}}^{\tau(f)}(e, f)<\frac{\gamma}{n} \mid \sigma_{0}\right\}=o\left(n^{-2}\right)
$$

It follows that,

$$
\begin{aligned}
\mathbb{P}\left\{\cap_{e, f \in E^{-}}\left(F_{e, f} \cup E_{f}\right)\right\} & \geq \mathbb{P}\left\{\mathcal{Y} \cap\left(\cap_{e, f \in E^{-}}\left(F_{e, f} \cup E_{f}\right)\right)\right\}=\mathbb{P}\{\mathcal{Y}\}-\mathbb{P}\left\{\mathcal{Y} \cap\left(\cap_{e, f \in E^{-}}\left(F_{e, f} \cup E_{f}\right)\right)^{c}\right\} \\
& \geq(1-o(1))-\sum_{e, f \in E^{-}} \mathbb{P}\left\{\mathcal{Y} \cap F_{e, f}^{c} \cap E_{f}^{c}\right\} \\
& \geq(1-o(1))-\sum_{e, f \in E^{-}} \mathbb{P}\left\{F_{e, f}^{c} \cap \mathcal{Y}_{e, f}\right\} \\
& \geq(1-o(1))-\sum_{e, f \in E^{-}} \mathbb{P}\left\{F_{e, f}^{c} \mid \mathcal{Y}_{e, f}\right\} \\
& =1-o(1)
\end{aligned}
$$

where we used that $\mathcal{Y} \cap E_{f}^{c}$ implies $\mathcal{Y}_{e, f}$.
So, whp, if $t_{\omega}^{-}(f)<\infty$, then $F_{e, f}$ holds. In other words, we have shown that for all $e \in E^{-}$and $f \in E_{0}^{-}$,

$$
\mathbb{P}^{\tau(f)}(e, f) \geq \hat{\mathbb{P}}^{\tau(f)}(e, f) \geq \frac{\gamma}{n} \geq n^{-\left(1+(1+\epsilon) \hat{H}_{-} / \phi\left(a_{0}\right)\right)}
$$

Since $\varepsilon$ can be arbitrarily small, this proves the lemma.

# 4.4 Proof of Lemma 4.6 

Define the following events

$$
\mathcal{A}_{1}=\bigcap_{e, f \in E^{-}}\left[\mathcal{A}_{e, f} \geq \frac{1}{2}\right], \quad \mathcal{A}_{2}=\bigcap_{e, f \in E^{-}}\left[\mathcal{B}_{e, f} \geq \frac{\omega \gamma}{4}\right] \cup\left[t_{\omega}^{-}(f)=\infty\right]
$$

and note that $\mathcal{Y} \supseteq \mathcal{A}_{1} \cap \mathcal{A}_{2}$.
The vertex analogue of the event $\mathcal{A}_{1}$ was studied in [11, Lemma 3.7]. Note that its proof does not use any assumption on $\delta^{-}$, is valid for tail-trees instead of vertex-trees and accommodates the $\kappa^{-}=O\left(n^{\alpha}\right)$ half-edges revealed during the in-phase. Thus the conclusion $\mathbb{P}\left\{\mathcal{A}_{1}\right\}=1-o(1)$ still holds in our setting.

We are left with showing the following lemma:



Lemma 4.7. We have $\mathbb{P}\left\{\mathcal{A}_{2}\right\}=1-o(1)$.
Proof. In order to analyse the probability of $\mathcal{A}_{2}$, it will be convenient to swap the order of the phases: we first run the in-phase unconditionally, and then the out-phase. Write $h=h(\mathrm{f})+h_{-}$. We will first prove that

$$
\hat{\Gamma}_{h}^{T}(\mathrm{f}):=\sum_{g \in \mathcal{N}_{h}^{-}(\mathrm{f})} \hat{w}(g)
$$

is large using branching processes. Note that the difference between $\hat{\Gamma}_{h}^{T}(\mathrm{f})$ and $B_{e, f}$ (defined in (4.21)) is that the latter does not include the weight of heads that are paired in the out-phase.

Consider the marked branching process $X_{t}$ with offspring distribution $\eta^{\uparrow}=\eta^{\uparrow}(\beta)$, where $\eta=$ $\mathrm{D}^{\text {out }}$ and $\beta<1 / 4$ will be fixed later. We define $\hat{\nu}^{\uparrow}, \hat{H}^{\uparrow}, \varphi^{\uparrow}(a), a_{0}^{\uparrow}$ for $\eta^{\uparrow}$ analogously to $\hat{\nu}, \hat{H}$, $\varphi(a)$ and $a_{0}$ for $\eta$ (see $(2.8),(2.13)$ and (2.58)). It is easy to verify that $\hat{\nu}^{\uparrow}=(1+o(1)) \hat{\nu}, \hat{H}^{\uparrow}=$ $(1+o(1)) \hat{H}$ and $\varphi^{\uparrow}(a)=(1+o(1)) \varphi(a)$. By continuity of $\varphi(a)$, we have $\varphi^{\uparrow}\left(a_{0}^{\uparrow}\right)=(1+o(1)) \varphi\left(a_{0}\right)$. So $\hat{H}^{\uparrow} / \varphi^{\uparrow}\left(a_{0}^{\uparrow}\right)=(1+o(1)) \hat{H} / \varphi\left(a_{0}\right)$. Define $\gamma^{\uparrow}:=n^{-(1+\varepsilon / 2)} \hat{H}^{\uparrow} / \varphi^{\uparrow}\left(a_{0}^{\uparrow}\right)>\gamma$.

Let $\mathcal{E}_{1}(\mathrm{f}):=\left[\hat{\Gamma}_{h}^{T}(\mathrm{f})<\omega_{\gamma / 2}\right]$ and $\mathcal{E}_{2}(\mathrm{f}):=\left[t_{\omega}^{-}(\mathrm{f})<\omega\right]$. Recall the definition of $\hat{\Gamma}_{t}$ in (2.63) and let $\mathcal{E}_{1}:=\left[\hat{\Gamma}_{t_{\omega}+h_{-}}<\omega_{\gamma / 2}\right]$ and $\mathcal{E}_{2}:=\left[t_{\omega}<\omega\right]$. Let $\mathcal{E}_{1}^{\uparrow}:=\left[\hat{\Gamma}_{t_{\omega}+h_{-}}<\omega_{\gamma^{\uparrow} / 2}\right]$ and note that $\mathcal{E}_{1} \subseteq \mathcal{E}_{1}^{\uparrow}$

Corollary 2.9 with $p=n^{-(1+\varepsilon / 2)}$ shows that

$$
\mathbb{P}\left\{\left(\mathcal{G}_{t_{\omega}}\left(\gamma^{\uparrow}\right)\right)^{c} \cap \mathcal{E}_{2}\right\} \leq \mathbb{P}\left\{\left(\mathcal{G}_{t_{\omega}}\left(\gamma^{\uparrow}\right)\right)^{c} \cap\left[t_{\omega}<\infty\right]\right\}=O\left(n^{-(1+\varepsilon / 4)}\right)
$$

So using Proposition 2.10

$$
\begin{aligned}
\mathbb{P}\left\{\mathcal{E}_{1}^{\uparrow} \cap \mathcal{E}_{2}\right\} & \leq \mathbb{P}\left\{\mathcal{E}_{1}^{\uparrow} \cap \mathcal{E}_{2} \cap \mathcal{G}_{t_{\omega}}\left(\gamma^{\uparrow}\right)\right\}+\mathbb{P}\left\{\left(\mathcal{G}_{t_{\omega}}\left(\gamma^{\uparrow}\right)\right)^{c} \cap \mathcal{E}_{2}\right\} \\
& \leq \sum_{t_{0}=1}^{\omega} \mathbb{P}\left\{\mathcal{E}_{1}^{\uparrow} \mid\left[t_{\omega}=t_{0}\right] \cap \mathcal{G}_{t_{0}}(\gamma)\right\} \mathbb{P}\left\{t_{\omega}=t_{0} \mid t_{\omega}<\omega\right\}+O\left(n^{-(1+\varepsilon / 4)}\right) \\
& =e^{-c_{0} \omega^{1 / 3}}+O\left(n^{-(1+\varepsilon / 4)}\right)=O\left(n^{-(1+\varepsilon / 4)}\right)
\end{aligned}
$$

Let $T$ be a marked incomplete tree of height $\omega+h_{-} \geq h$. Recall that $p(T)$ is the number of paired nodes in $T$ and that $T_{f}^{-}(i)$ is the marked incomplete tree constructed after the $i$-th pairing in the graph exploration process. Conditional on $\left[T_{f}^{-}(p(T))=T\right]$, the events $\mathcal{E}_{1}(\mathrm{f})$ and $\mathcal{E}_{2}(\mathrm{f})$ are measurable. Let $\mathcal{T}$ be the set of incomplete trees $T$ of height $\omega+h_{-}$that satisfy $\mathcal{E}_{1}(\mathrm{f}) \cap \mathcal{E}_{2}(\mathrm{f})$ Note that $p(T) \leq \kappa_{-}=O\left(n^{\alpha}\right)$ for a constant $\alpha>0$ as small as needed. Thus, by Lemma 3.1 with $\beta=10 \alpha$ and (4.33), we have

$$
\begin{aligned}
\mathbb{P}\left\{\mathcal{E}_{1}(\mathrm{f}) \cap \mathcal{E}_{2}(\mathrm{f})\right\}= & \mathbb{P}\left\{T_{f}^{-} \in \mathcal{T}\right\} \leq(1+o(1)) \mathbb{P}\left\{\mathcal{G}_{W_{\eta \uparrow}} \in \mathcal{T}\right\} \\
= & (1+o(1)) \mathbb{P}\left\{\mathcal{E}_{1} \cap \mathcal{E}_{2}\right\} \\
\leq & (1+o(1)) \mathbb{P}\left\{\mathcal{E}_{1}^{\uparrow} \cap \mathcal{E}_{2}\right\}=O\left(n^{-(1+\varepsilon / 4)}\right)
\end{aligned}
$$



By Lemma 2.7, we have $\mathbb{P}\left\{\mathcal{E}_{2}^{c} \mid t_{\omega}<\infty\right\}=o\left(n^{-2}\right)$. Applying Lemma 3.1 similarly as before shows that $\mathbb{P}\left\{\left(\mathcal{E}^{2}(f)\right)^{c} \mid t_{\omega}^{-}(f)<\infty\right\}=o\left(n^{-2}\right)$. Therefore, it follows from (4.34) that

$$
\mathbb{P}\left\{\left[\hat{\Gamma}_{h}^{T}(f) \geq \omega^{\gamma / 2}\right] \cup\left[t_{\omega}^{-}(f)=\infty\right]\right\}=1-O\left(n^{-(1+\varepsilon / 2)}\right)
$$

Thus, by applying the union bound first over $f \in \mathcal{E}^{-}$and then over $e \in \mathcal{E}^{-}$, we have

$$
\mathbb{P}\left\{\mathcal{A}_{2}^{c}\right\}=o(1)+\sum_{e, f \in \mathcal{E}^{-}} \mathbb{P}\left\{B_{e, f}<\frac{\omega^{\gamma}}{4}\right\} \cap\left\{\hat{\Gamma}_{h}^{T}(f) \geq \frac{\omega^{\gamma}}{2}\right\}\right\}
$$

Let $\sigma^{-}$be a partial pairing of the at most $\kappa^{-}=o(n)$ half-edges that expose $N_{\leq h(f)}^{-}$. We now perform the out-phase to construct the tree $T_{e}^{+}$conditional on $\sigma^{-}$. Recall that during the outphase at most $\kappa^{+}=o(n)$ edges are formed. Thus,

$$
\begin{aligned}
\mathbb{E}\left[B_{e, f} \mid \sigma^{-}\right] & =\sum_{g \in N_{h(f)}^{-}} \hat{w}(g) \mathbb{E}\left[\mathbf{1}_{g \in \mathcal{G}} \mid \sigma^{-}\right] \\
& \geq\left(1-\frac{\kappa^{+}}{m-\kappa^{+}-\kappa^{-}}\right) \sum_{g \in N_{h(f)}^{-}} \hat{w}(g) \\
& =(1+o(1)) \hat{\Gamma}_{h}^{T}(f)
\end{aligned}
$$

Using (4.19), an application of Azuma's inequality (see [21, Chapter 11]) to $B_{e, f}$ determined by the random vector $\left(\hat{w}(g) \mathbf{1}_{g \in \mathcal{G}}\right)_{g \in N_{h(f)}^{-}}$implies that

$$
\begin{aligned}
\left.\mathbb{P}\left\{B_{e, f}<\frac{\hat{\Gamma}_{h}^{T}(f)}{2} \mid \sigma^{-}\right\}\right) & \left.\leq \exp \left\{-2\left(1+o(1) \frac{\hat{\Gamma}_{h}^{T}(f)^{2}}{\sum_{g \in N_{h(f)}^{-}} \hat{w}(g)^{2}}\right\} \right) \\
& \leq \exp \left\{-2 \gamma^{-1} n^{2 \varepsilon} \hat{\Gamma}_{h}^{T}(f)\right\}
\end{aligned}
$$

Since the event $\left[\hat{\Gamma}_{h}^{T}(f)>\frac{\omega^{\gamma}}{2}\right]$ is measurable with respect to $\sigma^{-}$, we have

$$
\mathbb{P}\left\{B_{e, f}<\frac{\omega^{\gamma}}{4}\right\} \cap\left\{\hat{\Gamma}_{h}^{T}(f)>\frac{\omega^{\gamma}}{2}\right\} \leq \exp \left\{-2 \gamma^{-1} n^{2 \varepsilon}\left(\frac{\omega^{\gamma}}{2}\right)\right\} \leq \exp \left\{-\omega n^{2 \varepsilon}\right\}
$$

Putting this into (4.36) finishes the proof.

# 4.5 Upper bound for $\pi_{0}^{e}$ 

Let $\varepsilon>0$ be small enough. Let $a_{0}$ be the minimiser of $\varphi(a)$, defined as in (1.10). Let

$$
h_{1}:=\frac{1-\varepsilon}{a_{0} \varphi\left(a_{0}\right)} \log n, \quad h_{2}:=\log _{\nu} \omega=O(\log \log n)
$$

In this subsection we set $\gamma:=n^{-(1-\varepsilon)} \hat{H}^{-} / \varphi\left(a_{0}\right)$. Let $\Gamma^{t}(f)$ be a graph analogue of $\Gamma_{t}$ (see Section 2) defined by

$$
\Gamma^{t}(f):=\sum_{g \in N_{t}^{-}(f)} P^{t}(g, f)
$$

Recall the definition of $w(g)$ in (4.16). Define

$$
\Gamma_{t}^{T}(f):=\sum_{g \in N_{t}^{-}(f)} w(g)
$$



For any subgraph $G$ with vertex set a subset of $[n]$, let $\operatorname{TX}(G):=|E(G)|-(|V(G)|-1)$ denote the excess of edges of $G$ comparing to the case when it induces a tree.

For $f \in \mathcal{E}_{-}$, consider the following events:

$$
\begin{aligned}
\mathcal{E}_{0}(f) & =\left[t_{\omega}^{-}(f) \leq h_{1}+h_{2}\right] \\
\mathcal{E}_{1}(f) & =\left[0<\Gamma_{\mathbb{T}_{\mathrm{h}_{1}(f)}^{-}}<\gamma\right] \\
\mathcal{E}_{2}(f) & =\left[\operatorname{TX}\left(\mathcal{N}_{\leq \mathrm{h}_{1}(f)}^{-}\right)=0\right] \\
\mathcal{F}(f) & =\bigcap_{r=1}^{\mathrm{h}_{1}}\left[0<\left|\mathcal{N}_{r}^{-}(f)\right|<\omega\right]
\end{aligned}
$$

Write $\mathcal{A}(f)=\mathcal{E}_{0}(f) \cap \mathcal{E}_{1}(f) \cap \mathcal{E}_{2}(f) \cap \mathcal{F}(f)$.
The proof in this section uses $\mathbb{T}_{f}^{-}$, the tree constructed in Subsection 3.1, stopping once all heads at distance $h_{1}+h_{2}$ to $f$ have become active. During the construction of $\mathbb{T}_{f}^{-}$we deterministically expose at most $\kappa_{1}^{-}:=\omega h_{1}+\omega\left(\Delta^{-}\right)^{h_{2}+1}=O\left(\log ^{C} n\right)$ pairings for some constant $C$.

Proposition 4.8. For all $f \in \mathcal{E}_{-}$, we have

$$
\mathbb{P}\{\mathcal{A}(f)\}=n^{-1+\varepsilon+o(1)}
$$

Proof. Consider $\mathcal{E}_{0}=\left[t_{\omega} \leq h_{1}+h_{2}\right], \mathcal{E}_{1}=\left[0<\Gamma_{\mathrm{h}_{1}}<\gamma\right]$ and $\mathcal{F}=\bigcap_{r=1}^{\mathrm{h}_{1}}\left[0<X_{r}<\omega\right]$ to be the corresponding events on the marked branching process $\left(X_{r}\right)_{r \geq 0}$ with offspring distribution $\eta^{\downarrow}(\beta)$ with $\eta=\mathbf{D}^{\text {out }}$ and an arbitrary $\beta \in(0,1 / 4)$ to be determined later. With $a=a_{0}$ and $t=h_{1}$, Theorem 2.5 implies that

$$
\mathbb{P}\left\{\mathcal{E}_{1} \cap \mathcal{F}\right\} \geq n^{-1+\varepsilon+o(1)}
$$

When $\mathcal{F}$ happens, there is at least one individual in generation $h_{1}$. Thus, for some constant $c$,

$$
\mathbb{P}\left\{\mathcal{E}_{0} \mid \mathcal{E}_{1} \cap \mathcal{F}\right\} \geq \mathbb{P}\left\{X_{\mathrm{h}_{2}+\mathrm{h}_{1}} \geq \omega \mid X_{\mathrm{h}_{1}}>0\right\} \geq \mathbb{P}\left\{X_{\mathrm{h}_{2}} \geq \omega\right\}>c
$$

where in the last inequality we used that $v_{-}{ }_{\mathrm{h}_{X} \mathrm{X}}$ converges to an absolutely continuous random variable, and $h_{2}=\log _{\nu} \omega$. Therefore, $\mathbb{P}\left\{\mathcal{E}_{0} \cap \mathcal{E}_{1} \cap \mathcal{F}\right\} \geq n^{-1+\varepsilon+o(1)}$.

Conditional on $\left[\mathbb{T}_{f}^{-}(p(\mathbb{T}))=\mathbb{T}\right]$, the event $\mathcal{E}_{0}(f) \cap \mathcal{E}_{1}(f) \cap \mathcal{F}(f)$ is measurable. Let $\mathcal{T}$ be the set of incomplete trees $\mathbb{T}$ that satisfy $\mathcal{E}_{0}(f) \cap \mathcal{E}_{1}(f) \cap \mathcal{F}(f)$. Using Lemma 3.1 we have

$$
\mathbb{P}\left\{\mathcal{E}_{0}(f) \cap \mathcal{E}_{1}(f) \cap \mathcal{F}(f)\right\} \geq(1+o(1)) \mathbb{P}\left\{\mathcal{G}_{\mathbb{W}_{\eta}} \eta^{\downarrow} \in \mathcal{T}\right\} \geq n^{-1+\varepsilon+o(1)}
$$

where we used that the parameters $\hat{v}^{\downarrow}$, $\hat{H}^{-}, \varphi^{\downarrow}\left(a_{0}^{\downarrow}\right)$ are well-approximated by $\hat{v}, \hat{H}, \varphi\left(a_{0}\right)$, as in the proof of Lemma 4.7.

We next bound the probability of $\left(\mathcal{E}_{2}(f)\right)^{c} \cap \mathcal{F}(f)$. Recall that $\mathrm{TX}\left(\mathcal{N}_{\leq \mathrm{h}_{1}(f)}^{-}\right)$is the number of collisions; that is, steps in the exploration process where $e_{i}^{+} \in \mathcal{A}_{i-1}^{+}$. At step $i$, the probability of a collision is at most $\frac{\Delta^{+} i}{m-i+1}$. So, provided $\ell \leq m / 2$, the number of collisions in the first $\ell$ steps is dominated by a binomial random variable with parameters ( $\left.\ell, 2 \ell M / m\right)$. Note that the probability of $\left(\mathcal{E}_{2}(f)\right)^{c} \cap \mathcal{F}(f)$ is bounded from above by the probability of at least one collision happens when running the exploration process for at most $\ell=(\omega-1) h_{1}=O\left(\log ^{7} n\right)$ steps. Thus we have

$$
\mathbb{P}\left\{\left(\mathcal{E}_{2}(f)\right)^{c} \cap \mathcal{F}(f)\right\}=O\left(\ell^{2} / m\right)=n^{-1+o(1)}
$$

Thus, we obtain the lower bound in (4.44):

$$
\mathbb{P}\{\mathcal{A}(f)\}=\mathbb{P}\left\{\mathcal{E}_{0}(f) \cap \mathcal{E}_{1}(f) \cap \mathcal{E}_{2}(f) \cap \mathcal{F}(f)\right\}
$$



$$
\geq \mathbb{P}\left\{\mathcal{E}_{0}(f) \cap \mathcal{E}_{1}(f) \cap \mathcal{F}(f)\right\}-\mathbb{P}\left\{\left(\mathcal{E}_{2}(f)\right)^{c} \cap \mathcal{F}(f)\right\} \geq n^{-1+\varepsilon+o(1)}
$$

For the upper bound in (4.44), an application of Theorem 2.6 with offspring $\eta^{\uparrow}(\beta), a=a_{0}^{\uparrow}$ and $t=h_{1}$, and Lemma 3.1, as in the proof of Lemma 4.7, gives that

$$
\begin{aligned}
\mathbb{P}\{\mathcal{A}(f)\} & \leq \mathbb{P}\left\{\mathcal{E}_{1}(f) \cap \mathcal{F}(f)\right\} \\
& \leq(1+o(1)) \mathbb{P}\left\{\mathcal{E}_{1} \cap \mathcal{F}\right\} \\
& \leq \mathbb{P}\left\{\left(\mathbb{G}_{t}\left(e^{-a} \hat{H}_{t}\right)\right)^{c} \cap\left[0<X_{t}<\omega\right]\right\} \\
& \leq n^{-1+\varepsilon+o(1)}
\end{aligned}
$$

We will show that $Z:=\sum_{f \in \varepsilon_{-1}} \mathcal{A}(f)$ satisfies $Z>0$ whp, using a second moment calculation. As $m \geq n$, Proposition 4.8 implies that $\mathbb{E}[Z] \geq n^{\varepsilon / 2}$. For a partial pairing $\sigma_{0}$, we write $f \in \operatorname{Im}\left(\sigma_{0}\right)$ if $f$ has been paired by $\sigma_{0}$. For $f_{1}, f_{2} \in \mathcal{E}_{-}$with $f_{1} \neq f_{2}$, we have

$$
\begin{aligned}
\mathbb{P}\left\{\mathcal{A}\left(f_{1}\right) \cap \mathcal{A}\left(f_{2}\right)\right\} & \leq \sum_{\sigma_{0} \in \mathcal{A}\left(f_{1}\right)} \mathbb{P}\left\{\sigma_{0}\right\}\left(\mathbb{1}_{f_{2} \in \operatorname{Im}\left(\sigma_{0}\right)}+\mathbb{1}_{f_{2} \notin \operatorname{Im}\left(\sigma_{0}\right)} \mathbb{P}\left\{\mathcal{A}\left(f_{2}\right) \mid \sigma_{0}\right\}\right) \\
& \leq \mathbb{P}\left\{\mathcal{A}\left(f_{1}\right)\right\}\left(\mathcal{O}\left(\log ^{C} \frac{n}{n}\right)+\max _{\substack{\sigma_{0} \in \mathcal{A}\left(f_{1}\right) \\ f_{2} \notin \operatorname{Im}\left(\sigma_{0}\right)}} \mathbb{P}\left\{\mathcal{A}\left(f_{2}\right) \mid \sigma_{0}\right\}\right)
\end{aligned}
$$

where we used that at most $\kappa_{1}^{-}=\mathcal{O}\left(\log ^{C} n\right)$ edges need to be exposed to determine $\mathcal{A}\left(f_{1}\right)$. We briefly describe how to compute $\mathbb{P}\left\{\mathcal{A}\left(f_{2}\right) \mid \sigma_{0}\right\}$ for $f_{2} \notin \operatorname{Im}\left(\sigma_{0}\right)$. Start the exploration process in Subsection 3.1 from $f_{2}$ with one modification: if $e_{i}^{-}$has already been paired in $\sigma_{0}$, then we choose $e_{i}^{+}$according to $\sigma_{0}$ instead of uniformly at random. Since at most $\kappa_{1}^{-}$half-edges need to be paired, the probability of [ $\left.N_{\leq h_{1}+h_{2}\left(f_{2}\right)} \cap \operatorname{Im}\left(\sigma_{0}\right) \neq \emptyset\right]$ is at most $\mathcal{O}\left(\left(\kappa_{1}^{-}\right)^{2} / n\right)$. This implies that

$$
\mathbb{P}\left\{\mathcal{A}\left(f_{2}\right) \mid \sigma_{0}\right\} \leq \mathbb{P}\left\{\mathcal{A}\left(f_{2}\right)\right\}+\mathcal{O}\left(\frac{\left(\kappa_{1}^{-}\right)^{2}}{n}\right)
$$

By Proposition 4.8, we obtain

$$
\mathbb{P}\left\{\mathcal{A}\left(f_{1}\right) \cap \mathcal{A}\left(f_{2}\right)\right\} \leq \mathbb{P}\left\{\mathcal{A}\left(f_{1}\right)\right\}\left(\mathcal{O}\left(\frac{\left(\kappa_{1}^{-}\right)^{2}}{n}\right)+\mathbb{P}\left\{\mathcal{A}\left(f_{2}\right)\right\}\right) \leq n^{-2+3 \varepsilon}
$$

So $\mathbb{E}\left[Z^{2}\right]=(1+o(1)) \mathbb{E}[Z]^{2}$, and Chebyshev's inequality implies that whp $[Z>0]$.
Under [ $\left.Z>0\right]$, let $f_{0}$ be such that $\mathcal{A}\left(f_{0}\right)$ holds. As $\mathcal{E}_{0}\left(f_{0}\right)$ holds, we have $f_{0} \in \mathcal{E}_{0}^{-}$. Moreover, $\mathcal{E}_{1}\left(f_{0}\right) \cap \mathcal{E}_{2}\left(f_{0}\right)$ implies that $\sum_{g \in \varepsilon^{-}} \mathbf{h}_{1}\left(g, f_{0}\right)=\Gamma_{\mathbf{h}_{1}}\left(f_{0}\right)=\Gamma_{\mathbf{h}_{1}}^{T}\left(f_{0}\right)<\gamma$. Moreover, Remark 1.5 implies that $\pi_{e}(g) \leq n^{-1+o(1)}$ for any $g \in \mathcal{E}_{-}$. By stationarity at time $h_{1}$, we obtain

$$
\pi_{e}^{0} \leq \pi_{e}\left(f_{0}\right)=\sum_{g \in \mathcal{E}_{-}} \mathbf{h}_{1}\left(g, f_{0}\right) \pi_{e}(g) \leq n^{-1-(1-\varepsilon)} \hat{H}_{\phi\left(a_{0}\right)}^{-}+o(1)
$$

for any $\varepsilon>0$ and the upper bound in Theorem 1.1 follows.



# 4.6 The proof of Remark 1.8 

To show the upper bound in (1.16) in Remark 1.8, we can follow the line of arguments in Subsection 4.3 and Subsection 4.4. Here we briefly sketch the changes needed for it to work. For $\alpha \in\left[0, \hat{H}_{-} / \phi\left(a_{0}\right)\right]$, let $\gamma_{\alpha}:=n^{-\alpha}$ and $\beta=\frac{\alpha \phi\left(a_{0}\right)}{\hat{H}_{-}} \leq 1$. By Corollary 2.9 with $p=n^{-\beta}$, we have

$$
\mathbb{P}\left\{\left(\mathscr{G}_{t_{\omega}\left(\gamma_{\alpha}\right)}\right)^{c} \cap\left[t_{\omega}<\infty\right]\right\} \leq n^{-\beta+o(1)}
$$

Let

$$
Y_{f}^{\alpha}:=\bigcap_{e \in E_{-}}\left[\left[A_{e, f} \geq \frac{1}{2}\right] \cap\left[B_{e, f} \geq \frac{\omega \gamma_{\alpha}}{4}\right]\right]
$$

Then by the same argument of Lemma 4.6 using (4.54), we have

$$
\mathbb{P}\left\{\left(Y_{f}^{\alpha}\right)^{c} \cap\left[t_{\omega}^{-}(f)<\infty\right]\right\} \leq n^{-\beta+o(1)}
$$

Let $B(f):=\left[0<\pi_{e}(f)<\frac{\gamma_{\alpha}}{2 n}\right]$. It follows from Proposition 4.1 and Lemma 4.5 that

$$
\begin{aligned}
& \mathbb{P}\{B(f)\}=(1+o(1)) \mathbb{P}\left\{\left[\pi_{e}(f)<\frac{\gamma_{\alpha}}{2 n}\right] \cap\left[t_{\omega}^{-}(f)<\infty\right]\right\} \\
& \leq(1+o(1))\left(\mathbb{P}\left\{\left[\pi_{e}(f)<\frac{\gamma_{\alpha}}{2 n}\right] \cap Y_{f}^{\alpha}\right\}+\mathbb{P}\left\{\left(Y_{f}^{\alpha}\right)^{c} \cap\left[t_{\omega}^{-}(f)<\infty\right]\right\}\right) \\
& =(1+o(1)) \mathbb{P}\left\{\cup_{e \in E_{-}}\left[\hat{\mathbb{P}}_{\tau(f)}(e, f)<\frac{\gamma_{\alpha}}{2 n}\right] \cap Y_{f}^{\alpha}\right\}+n^{-\beta+o(1)} \\
& =n^{-\beta+o(1)}
\end{aligned}
$$

where in the last step we have used Lemma 4.5. Recall the definition of $A(f)$ below (4.43). Define $A^{\alpha}(f)$ analogously with $h_{1}$ replaced by $h_{1, \alpha}=\beta h_{1}$ and $\gamma$ replaced by $\gamma_{\alpha}$. Then by the same argument of Proposition 4.8 , we have

$$
\mathbb{P}\{B(f)\} \geq(1+o(1)) \mathbb{P}\left\{A^{\alpha}(f)\right\} \geq n^{-\beta+o(1)}
$$

and Remark 1.8 follows.

## 5 Applications

### 5.1 Hitting and cover times

We now prove Theorem 1.9, i.e., whp the maximal hitting and the cover time of $\overrightarrow{G_{n}}$ are both $n^{1+\hat{H}_{-} / \phi\left(a_{0}\right)+o(1)}$. Clearly, $\tau_{\text {hit }} \leq \tau_{\text {cov }}$, so it suffices to lower bound $\tau_{\text {hit }}$ and upper bound $\tau_{\text {cov }}$.

For the lower bound, let $C<\left(1+\hat{H}_{-} / \phi\left(a_{0}\right)\right)$ be a constant. Then by Theorem 1.1, we have $\pi_{\min } \leq n^{-C}$ whp. Recall the definition of $\tau_{x}(y)$ in Subsection 1.2. The returning time to $x \in[n]$ is $\tau_{x}^{+}:=\inf \left\{t \geq 1: Z_{t}=x, Z_{0}=x\right\}$. By the well-known relation between the expected returning time and the stationary distribution, we have whp

$$
\max _{x \in[n]} \mathbb{E}\left[\tau_{x}^{+} \mid \overrightarrow{G_{n}}\right]=\max _{x \in[n]} \frac{1}{\pi(x)}=\frac{1}{\pi_{\min }} \geq n^{C}
$$



Thus, whp there exists a vertex $x_{0} \in[n]$ such that $\mathbb{E}\left[\tau_{x_{0}}^{+} \mid \vec{G}_{n}\right] \geq n^{C}$, which implies that

$$
n^{C} \leq \mathbb{E}\left[\tau_{x_{0}}^{+} \mid \vec{G}_{n}\right]=1+\frac{1}{d_{x_{0}}^{+}} \sum_{y \in N_{x_{0}}^{+}} m(x, y) \mathbb{E}\left[\tau_{y}\left(x_{0}\right) \mid \vec{G}_{n}\right]
$$

where $m(x, y)$ is the multiplicity of the directed edge $(x, y)$ in $\vec{G}_{n}$. Thus, whp there exists two vertices $x_{0}$ and $y_{0}$ such that

$$
\mathbb{E}\left[\tau_{y_{0}}\left(x_{0}\right) \mid \vec{G}_{n}\right] \geq n^{C}-1
$$

It follows that $\tau^{\text {hit }} \geq n^{C}-1$ whp.
For the upper bound, let $C>\left(1+\hat{H}_{-} / \phi\left(a_{0}\right)\right)$ be a fixed constant and let $\tau=\omega^{2}=\log ^{12} n$. Recall the definition of $V_{0}$ in Proposition 4.1. Lemma 4.4 implies that whp for all $x \in[n]$ and $y \in V_{0}$, there exists $\tau(y) \leq \tau$ (by $(4.12)$ ) such that $P^{\tau(y)}(x, y) \geq n^{-C}$,. So the probability to hit $y$ in at most $\tau$ steps, which we call a try, is at least $n^{-C}$ uniformly for any starting point $x$. Thus, the number of tries needed to hit $y$ is stochastically dominated by a geometric random variable with success probability $n^{-C}$. It follows that whp

$$
\tau^{\text {hit }}=\max _{\substack{x \in[n] \\ y \in V_{0}}} \mathbb{E}\left[\tau_{x}(y)\right] \leq n^{C} \tau
$$

Therefore, by Matthews' bound [20, Theorem 2.6], we have

$$
\tau^{\text {cov }} \leq H_{n} \tau^{\text {hit }}=n^{C+o(1)}
$$

where $H_{n}$ is the $n$-th harmonic number.

# 5.2 Explicit constants for particular degree sequences 

In this section we discuss two particular examples where the polynomial exponent can be made explicit.

### 5.2.1 $r$-out digraph

For any integer $r \geq 2$, an $r$-out digraph $D_{n, r}$ is a random directed graph with $n$ vertices in which each vertex chooses $r$ out-neighbours uniformly at random. It is used as a model for studying uniform random Deterministic Finite Automata [7]. For $r \geq 2$, Addario-Berry, Balle, Perarnau [1] showed that in $D_{n, r}$, whp,

$$
\pi_{\min }=n^{-(1+\log (r) /(s r-\log r)+o(1))}
$$

where $s$ is the largest solution of $1-s=e^{-s r}$.
Although in $D_{n, r}$ the in-degrees are random, events that hold whp in $\vec{G}_{n}$ also holds whp in $D_{n, r}$, assuming that $D^{-}$(the in-degree of a uniform random vertex in $\vec{G}_{n}$ ) converges to a Poisson distribution with mean $r$ whereas the $D^{+}=r \geq 2$ almost surely. It is an exercise to check that whp the maximum in-degree of $D_{n, p}$ has order $\frac{\log n}{\log \log n}$. A careful inspection of the proof of Theorem 1.1 shows that the bounded maximum degree condition can be relaxed to $\Delta_{ \pm}=o(\log n)$. Therefore, the conclusion of Theorem 1.1 holds in this setting. As $\mathbb{P}\left\{D^{+}=r\right\}=1$, we have $I\left(a \hat{H}_{-}\right)=\infty$ for any $a \neq 1$, so $a_{0}=1$ and $\pi_{\min }=n^{-(1+\left|\log \hat{\nu}_{-}\right|+o(1))}$, which coincides with (5.6).



# 5.2.2 A toy example 

Here we show how the explicit value for $\pi_{\min }$ can be computed for a simple distribution, providing an example where $a_{0} \neq 1$. Consider a degree distribution $\mathcal{D}$ defined by

$$
\mathbb{P}\{D=(0,2)\}=\mathbb{P}\{D=(0,3)\}=\mathbb{P}\{D=(5,0)\}=\mathbb{P}\{D=(5,2)\}=\frac{1}{4}
$$

As $D^{+}$is uniform on $\{2,3\}$ and is independent from $D^{-}$, we have $\tilde{D}_{\text {out }}^{+}=\log 2+\log (3 / 2) X$, where $X$ is a Bernoulli random variable with probability $p=3 / 5$. The large deviation rate function for a Bernoulli random variable with probability $p$ is $I_{B e}(z)=z \log \left(\frac{z}{p}\right)+(1-z) \log \left(\frac{1-z}{1-p}\right)$ for $z \in[0,1]$ (see, e.g., [16, Exercise 2.2.23]). Thus, we have $I(z)=I_{B e}\left((\log (3 / 2))^{-1}(z-\log 2)\right)$.

With the help of interval arithmetic libraries [24], we get

$$
\begin{aligned}
\hat{H}^{-} & \doteq 0.936426 \\
\hat{\nu} & \doteq 0.181095 \\
a_{0} & \doteq 1.06671 \\
\phi\left(a_{0}\right) & \doteq 1.65129 \\
1+\frac{\hat{H}^{-}}{\phi\left(a_{0}\right)} & \doteq 1.56708
\end{aligned}
$$

with errors guaranteed to be at most $10^{-6}$ by the algorithm. As shown in Figure 2, the function $\phi(a)$ attains minimum at $a_{0}>1$. In particular, the vertex that is hardest to hit has a "thin" in-neighbourhood which is of $97.1 \%$ of the length of the longest such "thin" in-neighbourhoods. In other words, it is not the vertex that is furthest away from others that is hardest to find.



Figure 2: Plot of the function $\phi(a)$

Acknowledgements. We would like to thanks Pietro Caputo and Matteo Quattropani for insightful discussions on the topic.

## References

[1] L. Addario-Berry, B. Balle, and G. Perarnau. Diameter and Stationary Distribution of Random r-Out Digraphs. The Electronic Journal of Combinatorics, pages P3.28-P3.28, Aug. 2020. ISSN 1077-8926. doi: $10 /$ ghd74q.



[2] H. Amini. Bootstrap Percolation in Living Neural Networks. J Stat Phys, 141(3):459-475, Nov. 2010. ISSN 1572-9613. doi: 10/c53hx4.
[3] K. B. Athreya and P. E. Ney. Branching Processes. Grundlehren Der Mathematischen Wissenschaften. Springer-Verlag, Berlin Heidelberg, 1972. doi: $10 /$ dft4.
[4] J. Blanchet and A. Slynder. Characterizing optimal sampling of binary contingency tables via the configuration model. Random Structuresability Alorithms, 42(2):159-184, 2013. doi $: 10 / \mathrm{f} 4 \mathrm{mtxh}$.
[5] C. Bordenave, P. Caputo, and J. Salez. Random walk on sparse random digraphs. Probab. Theory Relat. Fields, 170(3):933-960, Apr. 2018. ISSN 1432-2064. doi: $10 /$ gc8nxk.
[6] C. Bordenave, P. Caputo, and J. Salez. Cutoff at the "entropic time" for sparse Markov chains. Probab. Theory Relat. Fields, 173(1):261-292, Feb. 2019. ISSN 1432-2064. doi: 10/ghcrhr.
[7] X. S. Cai and L. Devroye. The graph structure of a deterministic automaton chosen at random. Random Structures \& Algorithms, 51(3):428-458, 2017. ISSN 1098-2418. doi: 10/gbtqgb.
[8] X. S. Cai and G. Perarnau. The giant component of the directed configuration model revisited. arXiv:2004.04998 [cs, math], Apr. 2020. URL http://arxiv.org/abs/2004.04998.
[9] X. S. Cai and G. Perarnau. The diameter of the directed configuration model. arXiv:2003.04965 [cs, math], Mar. 2020. URL http://arxiv.org/abs/2003.04965.
[10] P. Caputo and M. Quattropani. Mixing time of PageRank surfers on sparse random digraphs. arXiv:1905.04993 [math], July 2020. URL http://arxiv.org/abs/1905.04993.
[11] P. Caputo and M. Quattropani. Stationary distribution and cover time of sparse directed configuration models. Probab. Theory Relat. Fields, Aug. 2020. ISSN 1432-2064. doi: $10 / \mathrm{ghd} 74 \mathrm{~v}$.
[12] S. Chatterjee. Stein's method for concentration inequalities. Probab. Theory ability Fields, 138 (1-2):305-321, Feb. 2007. ISSN 0178-8051, 1432-2064. doi: 10/fm2x4r.
[13] N. Chen, N. Litvak, and M. Olvera-Cravioto. Generalized PageRank on directed configuration networks. Random Structures \& Algorithms, 51(2):237-274, 2017. ISSN 1098-2418. doi: $10 /$ gbrth 6 .
[14] C. Cooper and A. Frieze. The Size of the Largest Strongly Connected Component of a Random Digraph with a Given Degree Sequence. Combinatorics, Probability and Computing, 13(3): 319-337, May 2004. doi: $10 / \mathrm{cn} 8 \mathrm{q} 5 \mathrm{j}$.
[15] C. Cooper and A. Frieze. Stationary distribution and cover time of random walks on random digraphs. J. Comb. Theory Ser. B, 102(2):329-362, Mar. 2012. ISSN 0095-8956. doi: 10/cv9wbh.
[16] A. Dembo and O. Zeitouni. Large Deviations Techniques and Applications. Stochastic Modelling and Applied Probability. Springer-Verlag, Berlin Heidelberg, second edition, 2010. doi: $10.1007 / 978-3-642-03311-7$.
[17] A. Graf. On the Strongly Connected Components of Random Directed Graphs with given Degree Sequences. PhD thesis, University of Waterloo, 2016. URL http://hdl.handle.net/10012/ 10681 .



[18] S. Janson. The probability that a random multigraph is simple. Combinatorics, Probability and Computing, 18(1-2):205-225, 2009. doi: $10 / \mathrm{bg} 4 \mathrm{~m} 2 \mathrm{c}$.
[19] H. Li. Attack Vulnerability of Online Social Networks. In 2018 37th Chinese Control Conference (CCC), pages 1051-1056, July 2018. doi: $10 / \mathrm{gg} \mathrm{kg} 2 \mathrm{~kg}$.
[20] P. Matthews. Covering Problems for Brownian Motion on Spheres. Ann. Probab., 16(1): 189-199, Jan. 1988. ISSN 0091-1798, 2168-894X. doi: $10 / \mathrm{cy} 882 \mathrm{r} 8$.
[21] M. Molloy and B. Reed. Graph Colouring and the Probabilistic Method. Algorithms and Combinatorics. Springer-Verlag, Berlin Heidelberg, 2002. doi: $10.1007 / 978-3-642-04016-0$.
[22] V. Petrov. Sums of Independent Random Variables. Ergebnisse Der Mathematik Und Ihrer Grenzgebiete. 2. Folge. Springer-Verlag, Berlin Heidelberg, 1975. doi: 10.1007/978-3-642-65809-9.
[23] O. Riordan and N. Wormald. The diameter of sparse random graphs. Combin. Probab. Comput., 19(5-6):835-926, 2010. ISSN 0963-5483. doi: $10 / \mathrm{dg} \mathrm{g} 66 \mathrm{hh}$.
[24] D. P. Sanders, L. Benet, K. Agarwal, E. Gupta, B. Richard, M. Forets, yashrajgupta, E. Hanson, B. van Dyk, C. Rackauckas, S. Miclua-Cämpeanu, T. Koolen, C. Wormell, F. A. Vázquez, J. Grawitter, J. TagBot, K. O’Bryant, K. Carlsson, M. Piibeleht, Reno, R. Deits, S. Olver, T. Holy, kalmarek, and matsueushi. JuliaIntervals/IntervalArithmetic.jl: V0.17.5. Zenodo, June 2020. URL https://github.com/JuliaIntervals/IntervalArithmetic{j.l.
[25] R. van der Hofstad. Random Graphs and Complex Networks, volume 1 of Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, Cambridge, England, 2016. doi: $10.1017 / 9781316779422$.
[26] P. van der Hoorn and M. Olvera-Cravioto. Typical distances in the directed configuration model. Ann. Appl. Probab., 28(3):1739-1792, June 2018. ISSN 1050-5164, 2168-8737. doi: $10 / \mathrm{gg} \mathrm{g} 2 \mathrm{ch}$.



