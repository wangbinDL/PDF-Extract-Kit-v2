
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Quickstart &#8212; PDF-Extract-Kit 0.1.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=a1637f0b"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=a5fa425f"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'get_started/quickstart';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Pretrained Model" href="../preparation/pretrained_model.html" />
    <link rel="prev" title="Installation" href="installation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="PDF-Extract-Kit 0.1.0 documentation - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="PDF-Extract-Kit 0.1.0 documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>


<li class="toctree-l1 current active"><a class="current reference internal" href="#">Quickstart</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Preparation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../preparation/pretrained_model.html">Pretrained Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../preparation/prompt_template.html">Prompt Template</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Training</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../training/modify_settings.html">Modify Settings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/custom_sft_dataset.html">Custom SFT Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/custom_pretrain_dataset.html">Custom Pretrain Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/custom_agent_dataset.html">Custom Agent Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/multi_modal_dataset.html">Multi-modal Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/open_source_dataset.html">Open Source Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/visualization.html">Visualization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DPO</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../dpo/overview.html">Introduction to DPO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dpo/quick_start.html">Quick Start with DPO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dpo/modify_settings.html">Modify DPO Training Configuration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reward Model</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../reward_model/overview.html">Introduction to Reward Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reward_model/quick_start.html">Quick Start Guide for Reward Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reward_model/modify_settings.html">Modify Reward Model Training Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reward_model/preference_data.html">Preference Dataset</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Acceleration</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../acceleration/deepspeed.html">DeepSpeed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../acceleration/pack_to_max_length.html">Pack to Max Length</a></li>
<li class="toctree-l1"><a class="reference internal" href="../acceleration/flash_attn.html">Flash Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../acceleration/varlen_flash_attn.html">Varlen Flash Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../acceleration/hyper_parameters.html">HyperParameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../acceleration/length_grouped_sampler.html">Length Grouped Sampler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../acceleration/train_large_scale_dataset.html">Train Large-scale Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../acceleration/train_extreme_long_sequence.html">Train Extreme Long Sequence</a></li>
<li class="toctree-l1"><a class="reference internal" href="../acceleration/benchmark.html">Benchmark</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chat</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chat/llm.html">Chat with LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chat/agent.html">Chat with Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chat/vlm.html">Chat with VLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chat/lmdeploy.html">Accelerate chat by LMDeploy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Evaluation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../evaluation/hook.html">Evaluation during training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../evaluation/mmlu.html">MMLU (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../evaluation/mmbench.html">MMBench (VLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../evaluation/opencompass.html">Evaluate with OpenCompass</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../models/supported.html">Supported Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">InternEvo Migration</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../internevo_migration/internevo_migration.html">InternEVO Migration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internevo_migration/ftdp_dataset/ftdp.html">ftdp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internevo_migration/ftdp_dataset/Case1.html">Case 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internevo_migration/ftdp_dataset/Case2.html">Case 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internevo_migration/ftdp_dataset/Case3.html">Case 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="../internevo_migration/ftdp_dataset/Case4.html">Case 4</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/opendatalab/PDF-Extract-Kit" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/get_started/quickstart.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Quickstart</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Quickstart</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-the-model-weights">Prepare the model weights</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#download-from-huggingface">Download from HuggingFace</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#download-from-modelscope">Download from ModelScope</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-the-fine-tuning-dataset">Prepare the fine-tuning dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Download from HuggingFace</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Download from ModelScope</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-the-config">Prepare the config</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modify-the-config">Modify the config</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#start-fine-tuning">Start fine-tuning</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-convert-lora-merge">Model Convert + LoRA Merge</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-convert">Model Convert</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lora-merge">LoRA Merge</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chat-with-the-model">Chat with the model</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="quickstart">
<h1>Quickstart<a class="headerlink" href="#quickstart" title="Link to this heading">#</a></h1>
<p>In this section, we will show you how to use XTuner to fine-tune a model to help you get started quickly.</p>
<p>After installing XTuner successfully, we can start fine-tuning the model. In this section, we will demonstrate how to use XTuner to apply the QLoRA algorithm to fine-tune InternLM2-Chat-7B on the Colorist dataset.</p>
<p>The Colorist dataset (<a class="reference external" href="https://huggingface.co/datasets/burkelibbey/colors">HuggingFace link</a>; <a class="reference external" href="https://www.modelscope.cn/datasets/fanqiNO1/colors/summary">ModelScope link</a>) is a dataset that provides color choices and suggestions based on color descriptions. A model fine-tuned on this dataset can be used to give a hexadecimal color code based on the user’s description of the color. For example, when the user enters “a calming but fairly bright light sky blue, between sky blue and baby blue, with a hint of fluorescence due to its brightness”, the model will output <img alt="#66ccff" src="https://img.shields.io/badge/%2366ccff-66CCFF" />, which matches the user’s description. There are a few sample data from this dataset:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Enligsh Description</p></th>
<th class="head"><p>Chinese Description</p></th>
<th class="head"><p>Color</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Light Sky Blue: A calming, fairly bright color that falls between sky blue and baby blue, with a hint of slight fluorescence due to its brightness.</p></td>
<td><p>浅天蓝色：一种介于天蓝和婴儿蓝之间的平和、相当明亮的颜色，由于明亮而带有一丝轻微的荧光。</p></td>
<td><p>#66ccff: <img alt="#66ccff" src="https://img.shields.io/badge/%2366ccff-66CCFF" /></p></td>
</tr>
<tr class="row-odd"><td><p>Bright red: This is a very vibrant, saturated and vivid shade of red, resembling the color of ripe apples or fresh blood. It is as red as you can get on a standard RGB color palette, with no elements of either blue or green.</p></td>
<td><p>鲜红色： 这是一种非常鲜艳、饱和、生动的红色，类似成熟苹果或新鲜血液的颜色。它是标准 RGB 调色板上的红色，不含任何蓝色或绿色元素。</p></td>
<td><p>#ee0000: <img alt="#ee0000" src="https://img.shields.io/badge/%23ee0000-EE0000" /></p></td>
</tr>
<tr class="row-even"><td><p>Bright Turquoise: This color mixes the freshness of bright green with the tranquility of light blue, leading to a vibrant shade of turquoise. It is reminiscent of tropical waters.</p></td>
<td><p>明亮的绿松石色：这种颜色融合了鲜绿色的清新和淡蓝色的宁静，呈现出一种充满活力的绿松石色调。它让人联想到热带水域。</p></td>
<td><p>#00ffcc: <img alt="#00ffcc" src="https://img.shields.io/badge/%2300ffcc-00FFCC" /></p></td>
</tr>
</tbody>
</table>
</div>
<section id="prepare-the-model-weights">
<h2>Prepare the model weights<a class="headerlink" href="#prepare-the-model-weights" title="Link to this heading">#</a></h2>
<p>Before fine-tuning the model, we first need to prepare the weights of the model.</p>
<section id="download-from-huggingface">
<h3>Download from HuggingFace<a class="headerlink" href="#download-from-huggingface" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>huggingface_hub

<span class="c1"># Download the model weights to Shanghai_AI_Laboratory/internlm2-chat-7b</span>
huggingface-cli<span class="w"> </span>download<span class="w"> </span>internlm/internlm2-chat-7b<span class="w"> </span><span class="se">\</span>
<span class="w">                            </span>--local-dir<span class="w"> </span>Shanghai_AI_Laboratory/internlm2-chat-7b<span class="w"> </span><span class="se">\</span>
<span class="w">                            </span>--local-dir-use-symlinks<span class="w"> </span>False<span class="w"> </span><span class="se">\</span>
<span class="w">                            </span>--resume-download
</pre></div>
</div>
</section>
<section id="download-from-modelscope">
<h3>Download from ModelScope<a class="headerlink" href="#download-from-modelscope" title="Link to this heading">#</a></h3>
<p>Since pulling model weights from HuggingFace may lead to an unstable download process, slow download speed and other problems, we can choose to download the weights of InternLM2-Chat-7B from ModelScope when experiencing network issues.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>modelscope

<span class="c1"># Download the model weights to the current directory</span>
python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from modelscope import snapshot_download; snapshot_download(&#39;Shanghai_AI_Laboratory/internlm2-chat-7b&#39;, cache_dir=&#39;.&#39;)&quot;</span>
</pre></div>
</div>
<p>After completing the download, we can start to prepare the dataset for fine-tuning.</p>
<p>The HuggingFace link and ModelScope link are attached here:</p>
<ul class="simple">
<li><p>The HuggingFace link is located at: https://huggingface.co/internlm/internlm2-chat-7b</p></li>
<li><p>The ModelScope link is located at: https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b/summary</p></li>
</ul>
</section>
</section>
<section id="prepare-the-fine-tuning-dataset">
<h2>Prepare the fine-tuning dataset<a class="headerlink" href="#prepare-the-fine-tuning-dataset" title="Link to this heading">#</a></h2>
<section id="id1">
<h3>Download from HuggingFace<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/datasets/burkelibbey/colors
</pre></div>
</div>
</section>
<section id="id2">
<h3>Download from ModelScope<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>Due to the same reason, we can choose to download the dataset from ModelScope.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://www.modelscope.cn/datasets/fanqiNO1/colors.git
</pre></div>
</div>
<p>The HuggingFace link and ModelScope link are attached here:</p>
<ul class="simple">
<li><p>The HuggingFace link is located at: https://huggingface.co/datasets/burkelibbey/colors</p></li>
<li><p>The ModelScope link is located at: https://modelscope.cn/datasets/fanqiNO1/colors</p></li>
</ul>
</section>
</section>
<section id="prepare-the-config">
<h2>Prepare the config<a class="headerlink" href="#prepare-the-config" title="Link to this heading">#</a></h2>
<p>XTuner provides several configs out-of-the-box, which can be viewed via <code class="docutils literal notranslate"><span class="pre">xtuner</span> <span class="pre">list-cfg</span></code>. We can use the following command to copy a config to the current directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>xtuner<span class="w"> </span>copy-cfg<span class="w"> </span>internlm2_7b_qlora_colorist_e5<span class="w"> </span>.
</pre></div>
</div>
<p>Explanation of the config name:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Config Name</p></th>
<th class="head"><p>internlm2_7b_qlora_colorist_e5</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Model Name</p></td>
<td><p>internlm2_7b</p></td>
</tr>
<tr class="row-odd"><td><p>Algorithm</p></td>
<td><p>qlora</p></td>
</tr>
<tr class="row-even"><td><p>Dataset</p></td>
<td><p>colorist</p></td>
</tr>
<tr class="row-odd"><td><p>Epochs</p></td>
<td><p>5</p></td>
</tr>
</tbody>
</table>
</div>
<p>The directory structure at this point should look like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>.
├──<span class="w"> </span>colors
│<span class="w">   </span>├──<span class="w"> </span>colors.json
│<span class="w">   </span>├──<span class="w"> </span>dataset_infos.json
│<span class="w">   </span>├──<span class="w"> </span>README.md
│<span class="w">   </span>└──<span class="w"> </span>train.jsonl
├──<span class="w"> </span>internlm2_7b_qlora_colorist_e5_copy.py
└──<span class="w"> </span>Shanghai_AI_Laboratory
<span class="w">    </span>└──<span class="w"> </span>internlm2-chat-7b
<span class="w">        </span>├──<span class="w"> </span>config.json
<span class="w">        </span>├──<span class="w"> </span>configuration_internlm2.py
<span class="w">        </span>├──<span class="w"> </span>configuration.json
<span class="w">        </span>├──<span class="w"> </span>generation_config.json
<span class="w">        </span>├──<span class="w"> </span>modeling_internlm2.py
<span class="w">        </span>├──<span class="w"> </span>pytorch_model-00001-of-00008.bin
<span class="w">        </span>├──<span class="w"> </span>pytorch_model-00002-of-00008.bin
<span class="w">        </span>├──<span class="w"> </span>pytorch_model-00003-of-00008.bin
<span class="w">        </span>├──<span class="w"> </span>pytorch_model-00004-of-00008.bin
<span class="w">        </span>├──<span class="w"> </span>pytorch_model-00005-of-00008.bin
<span class="w">        </span>├──<span class="w"> </span>pytorch_model-00006-of-00008.bin
<span class="w">        </span>├──<span class="w"> </span>pytorch_model-00007-of-00008.bin
<span class="w">        </span>├──<span class="w"> </span>pytorch_model-00008-of-00008.bin
<span class="w">        </span>├──<span class="w"> </span>pytorch_model.bin.index.json
<span class="w">        </span>├──<span class="w"> </span>README.md
<span class="w">        </span>├──<span class="w"> </span>special_tokens_map.json
<span class="w">        </span>├──<span class="w"> </span>tokenization_internlm2_fast.py
<span class="w">        </span>├──<span class="w"> </span>tokenization_internlm2.py
<span class="w">        </span>├──<span class="w"> </span>tokenizer_config.json
<span class="w">        </span>└──<span class="w"> </span>tokenizer.model
</pre></div>
</div>
</section>
<section id="modify-the-config">
<h2>Modify the config<a class="headerlink" href="#modify-the-config" title="Link to this heading">#</a></h2>
<p>In this step, we need to modify the model path and dataset path to local paths and modify the dataset loading method.
In addition, since the copied config is based on the Base model, we also need to modify the <code class="docutils literal notranslate"><span class="pre">prompt_template</span></code> to adapt to the Chat model.</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span>#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
<span class="gd">- pretrained_model_name_or_path = &#39;internlm/internlm2-7b&#39;</span>
<span class="gi">+ pretrained_model_name_or_path = &#39;./Shanghai_AI_Laboratory/internlm2-chat-7b&#39;</span>

# Data
<span class="gd">- data_path = &#39;burkelibbey/colors&#39;</span>
<span class="gi">+ data_path = &#39;./colors/train.jsonl&#39;</span>
<span class="gd">- prompt_template = PROMPT_TEMPLATE.default</span>
<span class="gi">+ prompt_template = PROMPT_TEMPLATE.internlm2_chat</span>

...
#######################################################################
#                      PART 3  Dataset &amp; Dataloader                   #
#######################################################################
train_dataset = dict(
<span class="w"> </span>   type=process_hf_dataset,
<span class="gd">-   dataset=dict(type=load_dataset, path=data_path),</span>
<span class="gi">+   dataset=dict(type=load_dataset, path=&#39;json&#39;, data_files=dict(train=data_path)),</span>
<span class="w"> </span>   tokenizer=tokenizer,
<span class="w"> </span>   max_length=max_length,
<span class="w"> </span>   dataset_map_fn=colors_map_fn,
<span class="w"> </span>   template_map_fn=dict(
<span class="w"> </span>       type=template_map_fn_factory, template=prompt_template),
<span class="w"> </span>   remove_unused_columns=True,
<span class="w"> </span>   shuffle_before_pack=True,
<span class="w"> </span>   pack_to_max_length=pack_to_max_length)
</pre></div>
</div>
<p>Therefore, <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code>, <code class="docutils literal notranslate"><span class="pre">data_path</span></code>, <code class="docutils literal notranslate"><span class="pre">prompt_template</span></code>, and the <code class="docutils literal notranslate"><span class="pre">dataset</span></code> fields in <code class="docutils literal notranslate"><span class="pre">train_dataset</span></code> are modified.</p>
</section>
<section id="start-fine-tuning">
<h2>Start fine-tuning<a class="headerlink" href="#start-fine-tuning" title="Link to this heading">#</a></h2>
<p>Once having done the above steps, we can start fine-tuning using the following command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Single GPU</span>
xtuner<span class="w"> </span>train<span class="w"> </span>./internlm2_7b_qlora_colorist_e5_copy.py
<span class="c1"># Multiple GPUs</span>
<span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="si">${</span><span class="nv">GPU_NUM</span><span class="si">}</span><span class="w"> </span>xtuner<span class="w"> </span>train<span class="w"> </span>./internlm2_7b_qlora_colorist_e5_copy.py
<span class="c1"># Slurm</span>
srun<span class="w"> </span><span class="si">${</span><span class="nv">SRUN_ARGS</span><span class="si">}</span><span class="w"> </span>xtuner<span class="w"> </span>train<span class="w"> </span>./internlm2_7b_qlora_colorist_e5_copy.py<span class="w"> </span>--launcher<span class="w"> </span>slurm
</pre></div>
</div>
<p>The correct training log may look similar to the one shown below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>01/29 21:35:34 - mmengine - INFO - Iter(train) [ 10/720]  lr: 9.0001e-05  eta: 0:31:46  time: 2.6851  data_time: 0.0077  memory: 12762  loss: 2.6900
01/29 21:36:02 - mmengine - INFO - Iter(train) [ 20/720]  lr: 1.9000e-04  eta: 0:32:01  time: 2.8037  data_time: 0.0071  memory: 13969  loss: 2.6049  grad_norm: 0.9361
01/29 21:36:29 - mmengine - INFO - Iter(train) [ 30/720]  lr: 1.9994e-04  eta: 0:31:24  time: 2.7031  data_time: 0.0070  memory: 13969  loss: 2.5795  grad_norm: 0.9361
01/29 21:36:57 - mmengine - INFO - Iter(train) [ 40/720]  lr: 1.9969e-04  eta: 0:30:55  time: 2.7247  data_time: 0.0069  memory: 13969  loss: 2.3352  grad_norm: 0.8482
01/29 21:37:24 - mmengine - INFO - Iter(train) [ 50/720]  lr: 1.9925e-04  eta: 0:30:28  time: 2.7286  data_time: 0.0068  memory: 13969  loss: 2.2816  grad_norm: 0.8184
01/29 21:37:51 - mmengine - INFO - Iter(train) [ 60/720]  lr: 1.9863e-04  eta: 0:29:58  time: 2.7048  data_time: 0.0069  memory: 13969  loss: 2.2040  grad_norm: 0.8184
01/29 21:38:18 - mmengine - INFO - Iter(train) [ 70/720]  lr: 1.9781e-04  eta: 0:29:31  time: 2.7302  data_time: 0.0068  memory: 13969  loss: 2.1912  grad_norm: 0.8460
01/29 21:38:46 - mmengine - INFO - Iter(train) [ 80/720]  lr: 1.9681e-04  eta: 0:29:05  time: 2.7338  data_time: 0.0069  memory: 13969  loss: 2.1512  grad_norm: 0.8686
01/29 21:39:13 - mmengine - INFO - Iter(train) [ 90/720]  lr: 1.9563e-04  eta: 0:28:36  time: 2.7047  data_time: 0.0068  memory: 13969  loss: 2.0653  grad_norm: 0.8686
01/29 21:39:40 - mmengine - INFO - Iter(train) [100/720]  lr: 1.9426e-04  eta: 0:28:09  time: 2.7383  data_time: 0.0070  memory: 13969  loss: 1.9819  grad_norm: 0.9127
</pre></div>
</div>
<p>Before training begins, the output of the model is as shown below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>2024/01/29 21:34:58 - mmengine - INFO - before_train in EvaluateChatHook.
2024/01/29 21:35:03 - mmengine - INFO - Sample output:
&lt;s&gt;&lt;|im_start|&gt;system
You are a professional color designer. Please provide the corresponding colors based on the description of Human.
&lt;|im_end|&gt;
&lt;|im_start|&gt;user
请给我一个像天空一样清澈透明的蓝色。&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
为了匹配您所描述的“像天空一样清澈透明的蓝色”，我建议您选择一种名为“天蓝”（Cerulean）的颜色。这种颜色通常被用来代表天空、海洋和清澈的水域，它具有明亮

2024/01/29 21:35:07 - mmengine - INFO - Sample output:
&lt;s&gt;&lt;|im_start|&gt;system
You are a professional color designer. Please provide the corresponding colors based on the description of Human.
&lt;|im_end|&gt;
&lt;|im_start|&gt;user
Please give me a clear blue like the sky.&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
The color you&#39;re describing is a clear, bright blue that resembles the color of the sky on a sunny day. This color is often referred to as &quot;sky blue&quot; or &quot;cerulean blue.&quot; Here are some specific color codes and names that
</pre></div>
</div>
<p>After training, the output of the model is as shown below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>2024/01/29 22:08:07 - mmengine - INFO - Sample output:
&lt;s&gt;&lt;|im_start|&gt;system
You are a professional color designer. Please provide the corresponding colors based on the description of Human.
&lt;|im_end|&gt;
&lt;|im_start|&gt;user
请给我一个像天空一样清澈透明的蓝色。&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
#1099ee&lt;|im_end|&gt;

2024/01/29 22:08:08 - mmengine - INFO - Sample output:
&lt;s&gt;&lt;|im_start|&gt;system
You are a professional color designer. Please provide the corresponding colors based on the description of Human.
&lt;|im_end|&gt;
&lt;|im_start|&gt;user
Please give me a clear blue like the sky.&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
#0066dd&lt;|im_end|&gt;
</pre></div>
</div>
<p>The color of the model output is shown below:</p>
<ul class="simple">
<li><p>天空一样清澈透明的蓝色：<img alt="天空一样清澈透明的蓝色" src="https://img.shields.io/badge/%E5%A4%A9%E7%A9%BA%E4%B8%80%E6%A0%B7%E6%B8%85%E6%BE%88%E9%80%8F%E6%98%8E%E7%9A%84%E8%93%9D%E8%89%B2-1099EE" /></p></li>
<li><p>A clear blue like the sky: <img alt="A clear blue like the sky" src="https://img.shields.io/badge/A_clear_blue_like_the_sky-0066DD" /></p></li>
</ul>
<p>It is clear that the output of the model after training has been fully aligned with the content of the dataset.</p>
</section>
</section>
<section id="model-convert-lora-merge">
<h1>Model Convert + LoRA Merge<a class="headerlink" href="#model-convert-lora-merge" title="Link to this heading">#</a></h1>
<p>After training, we will get several <code class="docutils literal notranslate"><span class="pre">.pth</span></code> files that do <strong>NOT</strong> contain all the parameters of the model, but store the parameters updated by the training process of the QLoRA algorithm. Therefore, we need to convert these <code class="docutils literal notranslate"><span class="pre">.pth</span></code> files to HuggingFace format and merge them into the original LLM weights.</p>
<section id="model-convert">
<h2>Model Convert<a class="headerlink" href="#model-convert" title="Link to this heading">#</a></h2>
<p>XTuner has already integrated the tool of converting the model to HuggingFace format. We can use the following command to convert the model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the directory to store parameters in hf format</span>
mkdir<span class="w"> </span>work_dirs/internlm2_7b_qlora_colorist_e5_copy/iter_720_hf

<span class="c1"># Convert the model to hf format</span>
xtuner<span class="w"> </span>convert<span class="w"> </span>pth_to_hf<span class="w"> </span>internlm2_7b_qlora_colorist_e5_copy.py<span class="w"> </span><span class="se">\</span>
<span class="w">                            </span>work_dirs/internlm2_7b_qlora_colorist_e5_copy/iter_720.pth<span class="w"> </span><span class="se">\</span>
<span class="w">                            </span>work_dirs/internlm2_7b_qlora_colorist_e5_copy/iter_720_hf
</pre></div>
</div>
<p>This command will convert <code class="docutils literal notranslate"><span class="pre">work_dirs/internlm2_7b_qlora_colorist_e5_copy/iter_720.pth</span></code> to hf format based on the contents of the config <code class="docutils literal notranslate"><span class="pre">internlm2_7b_qlora_colorist_e5_copy.py</span></code> and will save it in <code class="docutils literal notranslate"><span class="pre">work_dirs/internlm2_7b_qlora_colorist_e5_copy/iter_720_hf</span></code>.</p>
</section>
<section id="lora-merge">
<h2>LoRA Merge<a class="headerlink" href="#lora-merge" title="Link to this heading">#</a></h2>
<p>XTuner has also integrated the tool of merging LoRA weights, we just need to execute the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the directory to store the merged weights</span>
mkdir<span class="w"> </span>work_dirs/internlm2_7b_qlora_colorist_e5_copy/merged

<span class="c1"># Merge the weights</span>
xtuner<span class="w"> </span>convert<span class="w"> </span>merge<span class="w"> </span>Shanghai_AI_Laboratory/internlm2-chat-7b<span class="w"> </span><span class="se">\</span>
<span class="w">                        </span>work_dirs/internlm2_7b_qlora_colorist_e5_copy/iter_720_hf<span class="w"> </span><span class="se">\</span>
<span class="w">                        </span>work_dirs/internlm2_7b_qlora_colorist_e5_copy/merged<span class="w"> </span><span class="se">\</span>
<span class="w">                        </span>--max-shard-size<span class="w"> </span>2GB
</pre></div>
</div>
<p>Similar to the command above, this command will read the original parameter path <code class="docutils literal notranslate"><span class="pre">Shanghai_AI_Laboratory/internlm2-chat-7b</span></code> and the path of parameter which has been converted to hf format <code class="docutils literal notranslate"><span class="pre">work_dirs/internlm2_7b_qlora_colorist_e5_copy/iter_720_hf</span></code> and merge the two parts of the parameters and save them in <code class="docutils literal notranslate"><span class="pre">work_dirs/internlm2_7b_qlora_colorist_e5_copy/merged</span></code>, where the maximum file size for each parameter slice is 2GB.</p>
</section>
<section id="chat-with-the-model">
<h2>Chat with the model<a class="headerlink" href="#chat-with-the-model" title="Link to this heading">#</a></h2>
<p>To better appreciate the model’s capabilities after merging the weights, we can chat with the model. XTuner also integrates the tool of chatting with models. We can start a simple demo to chat with the model with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>xtuner<span class="w"> </span>chat<span class="w"> </span>work_dirs/internlm2_7b_qlora_colorist_e5_copy/merged<span class="w"> </span><span class="se">\</span>
<span class="w">                </span>--prompt-template<span class="w"> </span>internlm2_chat<span class="w"> </span><span class="se">\</span>
<span class="w">                </span>--system-template<span class="w"> </span>colorist
</pre></div>
</div>
<p>Of course, we can also choose not to merge the weights and instead chat directly with the LLM + LoRA Adapter, we just need to execute the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>xtuner<span class="w"> </span>chat<span class="w"> </span>Shanghai_AI_Laboratory/internlm2-chat-7b
<span class="w">                </span>--adapter<span class="w"> </span>work_dirs/internlm2_7b_qlora_colorist_e5_copy/iter_720_hf<span class="w"> </span><span class="se">\</span>
<span class="w">                </span>--prompt-template<span class="w"> </span>internlm2_chat<span class="w"> </span><span class="se">\</span>
<span class="w">                </span>--system-template<span class="w"> </span>colorist
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">work_dirs/internlm2_7b_qlora_colorist_e5_copy/merged</span></code> is the path to the merged weights, <code class="docutils literal notranslate"><span class="pre">--prompt-template</span> <span class="pre">internlm2_chat</span></code> specifies that the chat template is InternLM2-Chat, and <code class="docutils literal notranslate"><span class="pre">--</span> <span class="pre">system-template</span> <span class="pre">colorist</span></code> specifies that the System Prompt for conversations with models is the template required by the Colorist dataset.</p>
<p>There is an example below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>double enter to end input (EXIT: exit chat, RESET: reset history) &gt;&gt;&gt; A calming but fairly bright light sky blue, between sky blue and baby blue, with a hint of fluorescence due to its brightness.

#66ccff&lt;|im_end|&gt;
</pre></div>
</div>
<p>The color of the model output is shown below:</p>
<p>A calming but fairly bright light sky blue, between sky blue and baby blue, with a hint of fluorescence due to its brightness: <img alt="#66ccff" src="https://img.shields.io/badge/A_calming_but_fairly_bright_light_sky_blue_between_sky_blue_and_baby_blue_with_a_hint_of_fluorescence_due_to_its_brightness-66CCFF" />.</p>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="installation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Installation</p>
      </div>
    </a>
    <a class="right-next"
       href="../preparation/pretrained_model.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Pretrained Model</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Quickstart</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-the-model-weights">Prepare the model weights</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#download-from-huggingface">Download from HuggingFace</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#download-from-modelscope">Download from ModelScope</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-the-fine-tuning-dataset">Prepare the fine-tuning dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Download from HuggingFace</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Download from ModelScope</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-the-config">Prepare the config</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modify-the-config">Modify the config</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#start-fine-tuning">Start fine-tuning</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-convert-lora-merge">Model Convert + LoRA Merge</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-convert">Model Convert</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lora-merge">LoRA Merge</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chat-with-the-model">Chat with the model</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By PDF-Extract-Kit Contributors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, PDF-Extract-Kit Contributors.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>